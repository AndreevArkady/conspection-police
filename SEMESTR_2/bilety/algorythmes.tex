\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[unicode, pdftex]{hyperref}
\usepackage{cmap}
\usepackage{mathtext}
\usepackage{multicol}
\setlength{\columnsep}{1cm}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{icomma}
\usepackage{euscript}
\usepackage{mathrsfs}
\usepackage{geometry}
\usepackage{graphicx}
\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[usenames]{color}
\hypersetup{
     colorlinks=true,
     linkcolor=magenta,
     filecolor=magenta,
     citecolor=black,      
     urlcolor=magenta,
     }
\usepackage{fancyhdr}
\pagestyle{fancy} 
\fancyhead{} 
\fancyhead[LE,RO]{\thepage} 
\fancyhead[CO]{\hyperlink{t2}{к списку объектов}}
\fancyhead[LO]{\hyperlink{t1}{к содержанию}} 
\fancyfoot{}
\newtheoremstyle{indented}{0 pt}{0 pt}{\itshape}{}{\bfseries}{. }{0 em}{ }

\renewcommand\thesection{}
\renewcommand\thesubsection{}

%\geometry{verbose,a4paper,tmargin=2cm,bmargin=2cm,lmargin=2.5cm,rmargin=1.5cm}

\title{Матосновы алгоритмов}
\author{Кабашный Иван (@keba4ok) \\ 
    на основе лекций А. С. Охотина, А. В. Тискина, \\ 
    Э. А. Гирша и других материалов}
\date{22 января 2021 г.}


%envirnoments
    \theoremstyle{indented}
    \newtheorem{theorem}{Теорема}
    \newtheorem{lemma}{Лемма}
    \newtheorem{alg}{Алгоритм}

    \theoremstyle{definition} 
    \newtheorem{defn}{Определение}
    \newtheorem{exl}{Пример(ы)}
    \newtheorem{prob}{Задача}

    \theoremstyle{remark} 
    \newtheorem{remark}{Примечание}
    \newtheorem{cons}{Следствие}
    \newtheorem{exer}{Упражнение}
    \newtheorem{stat}{Утверждение}

%declarations
        %arrows_shorten
            \DeclareMathOperator{\la}{\leftarrow}
            \DeclareMathOperator{\ra}{\rightarrow}
            \DeclareMathOperator{\lra}{\leftrightarrow}
            \DeclareMathOperator{\llra}{\longleftrightarrow}
            \DeclareMathOperator{\La}{\Leftarrow}
            \DeclareMathOperator{\Ra}{\Rightarrow}
            \DeclareMathOperator{\Lra}{\Leftrightarrow}
            \DeclareMathOperator{\Llra}{\Longleftrightarrow}

        %letters_different
            \DeclareMathOperator{\CC}{\mathbb{C}}
            \DeclareMathOperator{\ZZ}{\mathbb{Z}}
            \DeclareMathOperator{\RR}{\mathbb{R}}
            \DeclareMathOperator{\NN}{\mathbb{N}}
            \DeclareMathOperator{\HH}{\mathbb{H}}
            \DeclareMathOperator{\LL}{\mathscr{L}}
            \DeclareMathOperator{\KK}{\mathscr{K}}
            \DeclareMathOperator{\GA}{\mathfrak{A}}
            \DeclareMathOperator{\GB}{\mathfrak{B}}
            \DeclareMathOperator{\GC}{\mathfrak{C}}
            \DeclareMathOperator{\GD}{\mathfrak{D}}
            \DeclareMathOperator{\GN}{\mathfrak{N}}
            \DeclareMathOperator{\Rho}{\mathcal{P}}
            \DeclareMathOperator{\FF}{\mathcal{F}}

        %common_shit
            \DeclareMathOperator{\Ker}{Ker}
            \DeclareMathOperator{\Frac}{Frac}
            \DeclareMathOperator{\Imf}{Im}
            \DeclareMathOperator{\cont}{cont}
            \DeclareMathOperator{\id}{id}
            \DeclareMathOperator{\ev}{ev}
            \DeclareMathOperator{\lcm}{lcm}
            \DeclareMathOperator{\chard}{char}
            \DeclareMathOperator{\codim}{codim}
            \DeclareMathOperator{\rank}{rank}
            \DeclareMathOperator{\ord}{ord}
            \DeclareMathOperator{\End}{End}
            \DeclareMathOperator{\Ann}{Ann}
            \DeclareMathOperator{\Real}{Re}
            \DeclareMathOperator{\Res}{Res}
            \DeclareMathOperator{\Rad}{Rad}
            \DeclareMathOperator{\disc}{disc}
            \DeclareMathOperator{\rk}{rk}
            \DeclareMathOperator{\const}{const}
            \DeclareMathOperator{\grad}{grad}
            \DeclareMathOperator{\Aff}{Aff}
            \DeclareMathOperator{\Lin}{Lin}
            \DeclareMathOperator{\Prf}{Pr}
            \DeclareMathOperator{\Iso}{Iso}

        %specific_shit
            \DeclareMathOperator{\Tors}{Tors}
            \DeclareMathOperator{\form}{Form}
            \DeclareMathOperator{\Pred}{Pred}
            \DeclareMathOperator{\Func}{Func}
            \DeclareMathOperator{\Const}{Const}
            \DeclareMathOperator{\arity}{arity}
            \DeclareMathOperator{\Aut}{Aut}
            \DeclareMathOperator{\Var}{Var}
            \DeclareMathOperator{\Term}{Term}
            \DeclareMathOperator{\sub}{sub}
            \DeclareMathOperator{\Sub}{Sub}
            \DeclareMathOperator{\Atom}{Atom}
            \DeclareMathOperator{\FV}{FV}
            \DeclareMathOperator{\Sent}{Sent}
            \DeclareMathOperator{\Th}{Th}
            \DeclareMathOperator{\supp}{supp}
            \DeclareMathOperator{\Eq}{Eq}
            \DeclareMathOperator{\Prop}{Prop}


%env_shortens_from_hirsh            
    \newcommand{\bex}{\begin{example}\rm}
    \newcommand{\eex}{\end{example}}
    \newcommand{\ba}{\begin{algorithm}\rm}
    \newcommand{\ea}{\end{algorithm}}
    \newcommand{\bea}{\begin{eqnarray*}}
    \newcommand{\eea}{\end{eqnarray*}}
    \newcommand{\be}{\begin{eqnarray}}
    \newcommand{\ee}{\end{eqnarray}}
    \newcommand{\abs}[1]{\lvert#1\rvert}
    

\begin{document}

\newcommand{\resetexlcounters}{%
  \setcounter{exl}{0}%
} 

\newcommand{\resetremarkcounters}{%
  \setcounter{remark}{0}%
} 

\newcommand{\reseconscounters}{%
  \setcounter{cons}{0}%
} 

\newcommand{\resetall}{%
    \resetexlcounters
    \resetremarkcounters
    \reseconscounters%
}



\maketitle 

\newpage

\url{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/} - источник с конспектами А. С. Охотина, где всё в тысячу раз подробнее и грамотнее. \\

\hypertarget{t1}{Основные моменты}. 
\tableofcontents

\newpage

\section{Билеты.}

На каждом билте можно нажимать на то, куда вам надо (кроме того, что с первой и восьмой лекции, пришлите мне, пожалуйста, кто-нибудь, записи с них). К билету относится то, что от метки, по которой вы перешли, и до слова \textbf{подробнее}, ссылка рядом с которым перекинет вас на подробный конспект Охотина, который этот билет содержит (иногда билет на соседних лекциях, тогда два \textbf{подробнее}, соответственно). \\

\textbf{Part 1.} \ 

1. \hyperlink{d-1}{Понятие алгоритма. Псевдокод. Хранение переменных при рекурсии. Машина с произвольным доступом к памяти.} \\

2. \hyperlink{t-1}{Метод «разделяй и властвуй». Быстрое умножение Карацубы, анализ времени работы.} \\ 

3. \hyperlink{t-3}{Сортировка вставкой, сортировка слиянием.} \hyperlink{d1.5}{Куча, действия над нею, сортировка кучей.} \\ 

4. \hyperlink{r1}{«Быстрая сортировка», среднее время работы.} \\ 

5. \hyperlink{t2.5}{Нижняя оценка числа сравнений при сортировке. Сортировка подсчётом. Поразрядная сортировка.} \\ 

6. \hyperlink{t4}{Нахождение i-го по величине элемента массива.} \\ 

7. \hyperlink{e1}{Метод динамического программирования. Задача о разделении стержня. Задача о порядке умножения матриц.} \\ 

8. \hyperlink{d2}{Нахождение наибольшей общей подпоследовательности: «народный» алгоритм, алгоритм Хиршберга.} \\ 

9. \hyperlink{t10}{Поиск в ориентированном графе: поиск в ширину, поиск в глубину, топологическая сортировка, нахождение компонентов сильной связности.} \\ 

10. \hyperlink{t14}{Кратчайшие пути в графе с весами: алгоритм Беллмана–Форда, алгоритм Дейкстры. Очередь с приоритетами и её реализация.} \\ 

11. \hyperlink{t17}{Нахождение минимального остовного дерева графа: алгоритм Прима, алгоритм Крускала.} \\

12. \hyperlink{t20}{Лес непересекающихся множеств, нахождение компонентов связности в неориентированном графе.} \\ 

13. \hyperlink{t22}{Кратчайшие пути в графе между всеми парами вершин. Транзитивное замыкание матриц. Алгоритм Варшалла. Кратчайшие пути с весами, алгоритм Флойда–Варшалла. Использование умножения матриц.}  \\ 

14. Быстрое умножение матриц, алгоритм Штрассена. (это я не стал переписывать, сразу смотрите в \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекции на странице 5})\\ 

15. \hyperlink{t25}{Основная теорема о времени работы рекурсивных алгоритмов.} \\ 

16. \hyperlink{e2}{Быстрое умножение булевых матриц через числовые. Метод четырёх русских.} \\ 

17. \hyperlink{d13}{Структуры данных для представления множеств: вектор, список, двоичное дерево поиска. Основные операции (поиск, вставка, удаление, следующий, предыдущий, наибольший, наименьший), сложность их реализации. АВЛ-деревья. Реализация операций над ними, их сложность.} \\ 

18. \hyperlink{t29}{B-деревья. Реализация операций над ними, их сложность.} \\ 

19. \hyperlink{t30}{Полиномиальное хэширование строк. Алгоритм Рабина–Карпа. Нахождение наибольшей общей подстроки. Нахождение самого длинного палиндрома.}  \\ 

20. \hyperlink{e3}{Алгоритм Кнута–Морриса–Пратта. Конечные автоматы, реализация алгоритма Кнута–Морриса–Пратта на конечном автомате.} \\ 

21. \hyperlink{t32}{Префиксное дерево. Алгоритм Ахо–Корасик. Его реализация на конечном автомате.} \\ 

22. Префиксное дерево для множества строк. Построение упрощённого суффиксного дерева для строки. Суффиксное дерево и его применение. \\ 

23. Суффиксное дерево для строки, алгоритм Укконена.  \\ 

24. Сжатие данных методом Хаффмана. Арифметическое кодирование. \\ 

\texttt{Билеты 22-24 см. в \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l8.pdf}{8 лекции}} \\

25. \hyperlink{t35}{Сжатие данных со словарём, метод Лемпеля–Зива (LZ77, LZ78).} \\ 

26. \hyperlink{t37}{Преобразование Берроуза–Вилера, его реализация.}

\ 

\textbf{Part 2.}

\ 

1. Первообразные корни из единицы. Дискретное преобразование Фурье (DFT), обратное DFT. Линейная, циклическая и косоциклическая свертки, их вычисление при помощи DFT. Умножение полиномов при помощи алгоритма Карацубы и DFT. Умножение целых чисел при помощи алгоритма Карацубы.

\ 

2. Быстрое преобразование Фурье (FFT). Общая шестиэтапная схема, схемы с прореживанием по времени и по частоте.

\ 

3. Преобразование DFT в кольце классов вычетов $Z_N$. Формулировка теоремы о выборе модуля $N$ по заданной степени преобразования $n$ и заданному первообразному корню $\omega$ из единицы; доказательство, что при таком выборе модуля значение $n$ обратимо. Трудоемкость битовых операций FFT по модулю $N$, заданному теоремой. 

\ 

4. Формулировка теоремы о выборе модуля $N$ по заданной степени преобразования $n$ и заданному первообразному корню $\omega$ из единицы; доказательство, что при таком выборе модуля значение $\omega$ --- первообразный корень из единицы.

\ 

5. Алгоритм Шенхаге-Штрассена для быстрого умножения двоичных чисел: сведение задачи умножения чисел к задаче вычисления косоциклической свертки в кольце классов вычетов.

\ 

6. Алгоритм Шенхаге-Штрассена для быстрого умножения двоичных чисел: вычисление косоциклической свертки по нечетному сомножителю модуля.

\ 

7. Алгоритм Шенхаге-Штрассена для быстрого умножения двоичных чисел: вычисление косоциклической свертки по четному сомножителю модуля; восстановление значения свертки по китайской теореме об остатках.

\ 

8. Вычисления схемами как модель параллельных вычислений. Схемы сравнения, слияния, сортировки. Принцип нулей-единиц. Эффективные схемы параллельной сортировки: сортировка нечетно-четным слиянием; сортировка битонным слиянием.

\ 

9. Задача префиксного накопления. Эффективная схема для параллельного решения этой задачи. Трудоемкость вычислений, коммуникации и синхронизации при решении этой задачи в модели барьерно-синхронного параллелизма (BSP). Параллельное вычисление обобщенных линейных рекуррентных соотношений и параллельное сложение двоичных чисел при помощи задачи префиксного накопления.

\ 

10. Задача линейного программирования (LP). Допустимые и оптимальные решения. Стандартная и каноническая форма задачи LP. Двойственная задача LP. Основная теорема линейного программирования (без доказательства). Общая идея симплекс-метода. Задача о кратчайших путях с одним источником как задача LP.

\ 

11. Задача о максимальном потоке в сети как задача LP. Алгоритм Форда-Фалкерсона.

\ 

12. Задача о минимальном вершинном покрытии. Ее приближенное решение при помощи LP-релаксации.
 
\ 

\textbf{Part 3.}  
 
\ 

1. Алгоритм Фрейвальдса для проверки умножения матриц.

\ 

2. Проверка равенства полиномов. Лемма Шварца-Циппеля.

\ 

3. Randomized QuickSort.

\ 

4. Сравнение строк на расстоянии и алгоритм Рабина-Карпа.

\ 

5. Вероятностная проверка простоты: алгоритм Соловея-Штрассена.

\ 

6. Слабоэкспоненциальный детерминированный алгоритм для 3-SAT.

\ 

7. Слабоэкспоненциальный вероятностный алгоритм для 3-SAT, основанный на случайном блуждании.

\ 

8. Универсальные семейства хеш-функций, пример семейства (с доказательством). Хеш-таблица, основанная на универсальном семействе, оценка сложности операций.

\ 

9. Совершенное хеширование. 

\ 

10. Линейный вероятностный алгоритм для минимального остовного дерева (без алгоритма верификации).

\ 

11. Оценки Чернова (Chernoff), без доказательства (достаточно самой простой формулировки). Уменьшение вероятности двусторонней ошибки.

\ 

12. Передача пакетов в кубической сети.

\ 

13. Онлайн-алгоритм MARKER для задачи кеширования (paging).

\ 

14. Алгоритм для онлайн-варианта задачи о покрытии множествами (случай единичных весов).

\ 

15. Алгоритм для онлайн-варианта задачи о покрытии множествами (общий случай).


\newpage

\section{Лекция 1.} 

\subsection{Билет 1.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d-1}{\textit{Алгоритм}}} - понятие, которое мы формально не определили, но согласно википедии, это - конечная совокупность точно заданных правил решения некоторого класса задач или набор инструкций, описывающих порядок действий исполнителя для решения определённой задачи. 
\end{defn}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d-2}{\textit{Псевдокод}}} - удобный способ записи алгоритма, который, конечно, невполне формален.
\end{defn}

\begin{stat}
    Рассмотрим машину с произвольным доступом памяти (\textcolor{magenta}{RAM}). Это - абстрактная модель вычисления, которая состоит из ячеек с памятью (рисуем их в столбик и нумеруем целыми числами). Программа состоит из команд, и каждая команда имеет номер (скажем вообще, что они называются $x_k$). Пусть первая команда у нас пересылать что-то куда-то (\textit{оператор присваивания} $A=B$, где $B$ может быть константой, прямой адресацией (какая-то ячейка), или косвенной адресацией $x_{x_n}$, а $A$ может быть всем этим же, кроме константы). Следующее, что нам нужно арифметические действия $A=B*C$, где $*\in\{+, -, \cdot, /, \div\}$. \ 

    Добавим также безусловный переход GOTO $n$ - переход к строке программы, аналогично можно задать GOTO $x_n$, которая может пересылать к строчке команды, которая имеет номер, хранящийся в этой ячейке. Также нам нужен условный оператор {IF; THEN}, и в конец добавим оператор HALT, которая будет говорить остановиться, и в целом, всё, что можно, мы уже умеем делать с помощью такой тривиальной вещи. \ 
\end{stat}

\begin{alg}
    Рассмотрим \textcolor{magenta}{\hypertarget{d-3}{\textit{рекурсию}}}, а конкретнее, как мы храним значения. \textcolor{magenta}{\hypertarget{d-4}{\textit{Стэк}}} - абстрактная структура данных, у которой есть дно, вершина, и мы всегда можем положить элемент наверх, или вынести его. Для рекурсии стэк состоит из нескольких stack-frame-ов, которые соответствуют каждому вызову рекурсивной программы, которые могут содержать ячейки со значениями, номерами строк или номерами вызова.
\end{alg}

\subsection{Билет 2.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-1}{\textbf{Быстрое умножение Карацубы}}}. Система идей, которая позволяет значительно ускорить умножение (скажем, двоичных) чисел. \ 

    Идея первая: при умножени двузначных чисел, вместо того, чтобы совершать 4 умножения и нескольких сумм, совершим 3 умножения и несколько сумм и разностей. Пусть у нас есть числа $a_1a_0$ и $b_1b_0$. Тогда рассмотрим $(a_1+a_0)(b_1+b_0)$ и вычтем $(a_1b_1+a_0b_0)$, в итоге получим, что из результатов у нас выйдет собрать искомую сумму четырёх изначальных произведений. \ 
     
    Идея вторая: вообще, как мы могли заметить, нигде сильно нас не волновало, в какой системе счисления мы работали. Расмотрим большие числа $a_{n-1}\ldots a_{\frac{n}{2}}a_{\frac{n}{2}-1}\ldots a_0$ и $b_{n-1}\ldots b_{\frac{n}{2}}b_{\frac{n}{2}-1}\ldots b_0$ (двоичные), и разделим каждое из них пополам. Будем считать половины цифрами, тогда вот мы и свели к первой идее. \ 

    Наконец, идея третья: для умножения $\frac{n}{2}$-значных чисел мы используем тот же самый алгоритм, вызывая его рекурсивно. 
\end{alg}

\begin{proof}
    Пусть у нас есть 2 $n$-значных числа, которые алгоритм $K$ принимает на вход. Тогда у нас есть $C_1$ - $K$ от первых половин, $C_2$ - $K$ от вторых половин, и $C_3$ - $K$ от сумм цифр пары чисел. В итоге мы получаем $D=C_3-C_1-C_2$, и ответ $C_2 D C_1$. Нам нужно понять что-то про $T(n)$ - время на $n$-значных числах, $T(n)=3T(\frac{n}{2})+O(n)$. $3^i$ раз мы вычисляем $K$ от $\frac{n}{2^i}$-значных чисел, суммарно по этажам мы используем $\sum_{n=0}^{\log_2 n}C 3^i \frac{n}{2^i}$, тчо можон допреобразовать к $Cn\frac{(\frac{3}{2})^{\log_2 n+1}-1}{\frac{3}{2}-1}=Cn^{\log_2 3}$.
\end{proof}

\begin{alg}
    В ходе рассуждений мы коснулись идеи \textcolor{magenta}{\hypertarget{t-2}{\textbf{метода ''разделяй и властвуй''}}}, который состоит в том, что мы разбиваем большую задачу на несколько подзадач, которые ещё и можно распараллелить, если так можно, и таким образом, решать меньшие подзадачи вместо решения одной объёмной.
\end{alg}

\subsection{Билет 3.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-3}{\textbf{Сортировка вставкой}}}. Пусть у нас есть массив $a_i$. Рассмотрим первый элемент, он отсортирован. Рассмотрим теперь два элемента; они либо отсортированы, либо нет, тогда мы их меняем. И теперь работаем, фактически, по индукции. 
\end{alg}

\begin{stat}
    Работает алгоритм за время $n^2$, причём зависит он от входных данных довольно сильно.
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-4}{\textbf{Сортировка слиянием}}}. Разбиваем массив на две части, после чего отсортируем каждую половину и сольём. Делается это с помощью дополнительной памяти, ещё одного массива такого же размера, в который мы будем заносить итоговый результат слияния. Для самого слияния нам также нужны будут нужны указатели, которые двигаются и показывают на элементы, которые мы сравниваем. Тот, который получился меньше, заносится в новый массив, и указатель над ним сдвигается.
\end{alg}

\begin{stat}
    Работает алгоритм за $n\log n$, раз уж $\sum_{i=0}^{\log_2 n} C 2^i \frac{n}{2^i}=c n \log_2 n$. 
\end{stat}

\section{Лекция 2.}

\subsection{Быстрая сортировка.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{r1}{\textbf{Быстрая сортировка}}}. Выбираем \textcolor{magenta}{\hypertarget{d1}{\textit{опорный элемент}}}, с которым сравниваем все остальные элементы (на это уходит линейное время). Затем рекурсивно работаем с тем, что справа от него и слева от него.
\end{alg} \ 

\begin{theorem}
    Если все элементы массива различны и опопрный элемент выбирается случайно, то среднее время работы алгоритма - $\Theta(n\log n)$.
\end{theorem}

\begin{proof}
    Время работы пропорционально числу сравнениий между элементами. Расматриваем два элемента $y_i$ и $y_j$, $i<j$, тогда они сравниваются только, если выбран один из них в качестве опорного. Если будет выбран какой-то $y_k$, $i<k<j$, то они никогда больше не будут сравнены, если что-то на отрезке не между ними - плевать, относительно отрезка между ними ничего не поменялось. Тогда среднее количество сравнений между этими элементами: 
    \[
        \frac{2}{j-i+1}. 
    \]
    Тогда всего среднее количество сравнений: 
    \[
        \sum_{i=1}^{n-1}\sum_{j=i+1}^n \frac{2}{j-i+1} = \sum_{i=1}^{n-1}\sum_{k=1}^{n-i} \frac{2}{k+1}= O(n\log n).
    \]
    Последняя оценка получается из
    \[
        \sum_{k=1}^n\frac{1}{k} \approx \int_1^n \frac{1}{x}dx = \ln n. 
    \]
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 1}

\subsection{Сортировка кучей.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d1.5}{\textit{Куча}}} - двоичное дерево, представленное в виде массива. В этом дереве различаются левый и правый потомок каждой вершины, и потому вершины на каждом уровне упорядочены. Дерево почти сбалансировано: на каждом $i$-м уровне, кроме, может быть, последнего, есть все $2^i$ вершин.
\end{defn}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t2}{\textbf{Сортировка кучей}}}. Для начала, мы строим дерево: записываем по порядочку все вершины так, что у вершины $x_i$ потомки - $x_{2i}$ и $x_{2i+1}$. Затем начинаем на все вершины, кроме висячих смотреть и делать вот что: если она меньше потомка, то поменяем с ним (если меньше обоих, то с меньшим). Так доведём её до куда сможем, и продолжим рпссмотрение для оставшихся невисячих вершин (в изначальном дереве). Так сверху окажется наименьшая вершина, вынесем её, затем - по индукции.
\end{alg}

\begin{stat}
    Работает за $O(n\log n)$ (построение дерева - $O(\log n)$, вынесение вершин - $O(n)$), можно разогнать оценку до $2n$. 
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 3}

\subsection{Скорость сортировки.} 
 
\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t2.5}{\textbf{Нижняя оценка скорости}}}).Всякий алгоритм сортировки, основанный на сравнении, требует $\Omega (n\log n)$ операций сравнения.
\end{theorem}

\begin{proof}
    Построим дерево того, как мы спускаемся к определению последовательности, его высота ограничивается $\log_2n!$, оценим факториал $\biggl(\frac{n}{e}\biggr)^n<n!<n^n$, а после - оценим через это логарифм $n\log_2n - O(n)$. 
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t3}{\textbf{Сортировка подсчётом}}}. Если у нас есть массив из конечного обозримого количества типов элементов, можно сначала посчитать количество первого, затем количество второго, и так далее. Время работы - $O(n+k)$, где $k$ - количество типов, $n$ - количество переменных.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t4}{\textbf{Поразрядная сортировка}}}. Сортируем числа сначала по первому разряду, затем по второму, и так далее\dots Время работы: $O(l(n+k))$, где сравниваются строки длины $l$, алфавит из $k$ символов.
\end{alg}  \ 

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 4}

\subsection{Нахождение $i$-го по величине элемента массива.} 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t5}{\textbf{Нахождение $i$-го элемента}}}. Делим массив на пятёрки подряд идущих элементов (возможно, последняя пятёрка будет неполной). Теперь в каждой пятёрке выделяем медианы, и смотрим на медиану медиан. Сделаем её опорным элементом и как в быстрой сортировке, раскидаем всё по сторонам. Если этот элемент под номером $i$, то мы его нашли, иначе - действуем рекурсивно с одной из сторон. Время работы - линейное.
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 8}

\subsection{Метод динамического программирования.}

\hypertarget{e1}{технические трудности}

\begin{prob}
    Имеется стержень длины $n$. Продав стержень длины $i$, можно выручить $p_i$ денежных единиц. Как выгоднее всего распилить имеющийся стержень?
\end{prob}

\begin{alg}
    Начинаем с первого, и делаем полный перебор. Говнище
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t6}{\textbf{Жадный алгоритм}}}. Отпиливаем самый дорогой кусок, затем опять самый дорогой из возможных, и так далее. Не самый оптимальный.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t7}{\textbf{Метод динамического программрования}}}. Суть этого метода такова. Пусть на каждом шаге надо сделать выбор (принять решение). Известно, что какой-то выбор приводит к оптимальному результату. Этому выбору соответствует некий набор подзадач. Тогда сперва находятся ответы для всех подзадач данной задачи, возникающих при различном выборе, после чего, имея все эти ответы перед глазами, можно будет в каждом случае сделать наилучший выбор.
\end{alg}

\begin{exl}
    Пусть стержень длины 0 не стоит нисколько. Для $j$ от 1 до $n$ пока не найдено никаких способов продать стержень, для всякой длины отрезаемого куска, сложим его цену с выручкой за остаток. Если так можно выручить больше известного, то цена стержня длины $j$ улучшается, и так рекурсивно мы дойдём до получения цены за весь стержень. 
\end{exl}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 11}

\section{Лекция 3.}

\subsection{Продолжение динамического программирования.}

\begin{prob}
    Пусть нужно умножить $n$ матриц $M_1 \times \ldots \times M_n$. В силу ассоциативности, скобки можно расставить как угодно. От их расстановки зависит общее число операций, и, чтобы умножить матрицы быстрее, надо заранее определить наилучший порядок их умножения.
\end{prob}

\begin{exl}
    Строим верхнедиагональную матрицу $T$, в которой $T_{i, j}$ - наименьшее число действий, необходимых для вычисления $M_{i+1} \times \ldots \times M_j$. \ 

    Внешний цикл по длине куска $l=j-i$, второй - по $i$, во внутреннем перебируются все разбиения произведения на два, и вычисляется следующее значение:
    \[
        T_{i, j}=\min_{k=-+1}^{j-1}(T_{i, k}+T_{k, j}+m_im_km_j). 
    \]
    Разбираем так все по порядку и вычисляем наилучший способ. Время раюоты: $O(n^3)$ - строим таблицу, далее - $2n-1$ вызовов процедуры перемножить $(i, j)$, в каждом - $O(n)$ итераций цикла. И ещё само умножение матриц.
\end{exl}

\begin{remark}
    Для простого понимания - простой пример с кузнечиком, который прыгает на 1 или 2, и ему нужно пропрыгать $n$, сколькими способами это можно сделать? Мы заводим массив $dp[i]$ длины $n$ (кол-во способов добраться до $i$), тогда $dp[i]=dp[i-1]+dp[i-2]$, и так насчитываем все значения, находим ответ для $n$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 1}

\subsection{Нахождение наибольшей общей подпоследовательности.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d2}{\textit{Строкой над алфавитом}}} $\Sigma$ называется всякая конечная последовательность $w=a_1\ldots a_l$, где $l\geq 0$, и $a_1, \ldots, a_l \in \Sigma$ - символы.
\end{defn}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t8}{\textbf{Народный алгоритм}}}. Динамический способ нахождения наибольшей общей подпоследовательности. Заводим таблицу $T$ и в ячейке $T_{i, j}$ записываем длину наибольшей общей подпоследовательности на префиксах длины $i$ и $j$ первого и второго слова соответственно. Заполняем таблицу последовательно от более коротких мар до самых длинных, и в итоге получим ответ в задаче. \ 

    Строим таблицу так: берём $T_{i, j}$. Если у них одинаковые последние элементы, то получим $T_{i-1, j-1}+1$. Если они разные, то $\max({T_{i-1, j}, T_{i, j-1}})$. \ 

    Саму последовательность элементов потом восстанавливаем с конца понятно как. Недостаток в том, что чтобы найти подпоследовательность, нужно хранить всю таблицу, а это $O(mn)$, и это много. Однако, если нужна только длина, то можно ограничиться лишь двумя столбцами (или двумя строчками).
\end{alg} \

\begin{alg}
    \textcolor{magenta}{\hypertarget{t9}{\textbf{Алгоритм Хиршенберга}}}. Построение наибольшей общей подпоследовательности за время $O(mn)$, используя память $O(\min(m, n))$. Пусть $u=u'u''$ - некоторое разбиение $u$. Тогда оптимальное совмещение $u$ и $v$ совмещает $u'$ с каким-то начальным куском $v$ - пусть это $v'$, и $u''$ - с остатком $v''$. Нужно найти это разбиение $v=v'v''$, чтобы потом отдельно запустить совмещение двух соответствующиих пар кусков. \ 

    Алгоритм делит $u$ на две подстроки примерно равной длины. Сперва динамическим программированием находится последняя строчка таблицы $T^{u', v}$, как в базовом алгоритме. Её $j$-ый элемент содержит длину наибольшей общей подпоследовательности $u'$ и $v_j$ - префикса длины $j$. Аналогично находится последняя строка таблицы $T^{(u'')^R, v^R}$ ($R$ - reverse). Дальше складываем таблицы поэлементно и ссмотрим, где достигается максимум - это и есть искомое разбиение $v=v'v''$. Для этого вычисления алгоритс использовал $O(|v|)$ ячеек памяти, которые теперь можно освободить. \ 

    Далее алгоритм вызывается рекурсивно, чтобы вычислить лучшее совмещение $u'$ и $v'$, и $u''$ и $v''$. Полученные совмещения последовательно приписываются друг к другу.
\end{alg} \

\begin{theorem}
    Алгоритм Хиршберга работает за время $O(mn)$. 
\end{theorem}

\begin{proof}
    Принимая за единицу времени время, затрачиваемое на вычисление значение одного элемента $T_{i, j}$ в ''народном'' алгоритме, утверждается, что в общей сложности будет выполнено не более, чем $2mn$ шагов. \ 

    Пусть $f(m, n)$ - время работы в наихудшем случае. Тогда индукцией по $m$ и $n$ доказывается неравенство $f(m, n)\leq 2mn$. При запуске на строках $u$ и $v$, где их мощности соответственно равны $m$ и $n$, вычисление таблицы $T_{u', v}$ займёт $\frac12 mn$ шагов, и за столько же шагов будет вычисляться таблица $T^{(u'')^R, v^R}$. После этого проводятся два рекурсивных вызова, один из которых занимает $f(\frac{m}{2}, k)$ шагов, а другой - $f(\frac{m}{2}, n-k)$ шагов, для некоторого $k$. Время раюоты рекурсивных вызовов оценивается по предположению индукции, откуда получается оценка того же вида для $f(m,n)$. 
    \[
        f(m, n)=2\cdot \frac12 mn+\max_k (f(\frac{m}{2}, k)+f(\frac{m}{2}, n-k))\leq mn+\max_k(mk+m(n-k))=2mn
    \]
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 3}

\subsection{Поиск в ориентированном графе.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t10}{\textbf{Поиск в ширину (BFS)}}}. В каждый момент времени вершина графа может быть помечена или не помечена. Если вершина уже помечена, значит алгоритм нашёл путь из корня в неё. Кроме пометок на вершинах, алгоритм хранит очередь, в которой находятся все те помеченные вершины, для которых ещё не обработаны исходящие дуги. Таким образом, в каждый момент времени вершина может быть не помеченнной, помечанной и обработанной, и помечанной и необработанной. Идём из корня и последовательно отмечаем и заносим в очередь тех, к кому пришли.
\end{alg}

\begin{stat}
    В каждый момент времени очередь состоит из некоторых вершин, находящихся на расстоянии $l$ от $s$, вслед за которыми идут некоторые вершины, находящиеся на расстоянии $l+1$ от $s$. При этом все вершины на расстоянии, меньшем, чем $l$, уже обработаны, ровно как и все вершины на расстоянии $l$, не вошедшие в очередь. Из вершин на расстоянии $l+1$ в очереди есть ровно все потомки обработанных вершин.
\end{stat}

\begin{stat} \ 
    \begin{itemize}
        \item алгоритм помечает врешину $v$ тогда и только тогда, когда есть путь тз $s$ в $v$; 
        \item если алгоритм находит $v$ по дуге $(u, v)$, то один из кратчайших путей из $s$ в $v$ идёт через $u$; 
        \item все пройденные дуги $(u, v)$ образуют дерево.
    \end{itemize}
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t11}{\textbf{Поиск в глубину (DFS)}}}. Ижём в глубину до конца, отмечаем вершины в чёрный, если из них начали идти вниз, серым, если они нам просто встретились на пути. После того, как дошли до конца, идём наверх до первой вершины. Время работы: $O(|V|+|E|)$. 
\end{alg}

\begin{prob}
    \textcolor{magenta}{\hypertarget{t12}{\textbf{Топологическая сортировка}}}. Нужно найти остовные деревья в орграфе (естественно, он должен быть ациклическим). Решается через DFS из следующих утверждений. 
\end{prob}

\begin{stat}
    Граф ациклический тогда и только тогда, когда при поиске в глубину никогда не рассматривается дуга, ведущая в вершину, находящуюся в стеке возврата (дуга из серой в серую).
\end{stat}

\begin{stat}
    Если в ациклическом графе есть дуга $(u, v)$, то время завершения $v$ ментше, чем время завершения $u$. 
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 6}

\section{Лекция 4.}

\subsection{Окончание поисков в орграфе.}

\begin{prob}
    Найти в данном графе его компоненты сильной связности. 
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t13}{\textbf{Алгоритм Косараджу-Шарира}}}. Спервая запускается поиск в глубину для $G$, а затем запускается поиск в глубину для обращённого графа $G^R$, в котором направления всех дуг изменены на обратные (однако, компоненты связности те же). При поиске в глубину в обращённом графе, во внешнем цикле вершины рассматриваются в порядке их завершения при первом поиске в глубину, от конца к началу. После этого оказывается, что каждый запуск процедуры DFS во внешнем цикле будет находить очередной сильно связный компонент исходного графа. Время работы: $O(|V|+|E|)$, корректность обосновывается следующим:
\end{alg}

\begin{stat}
    Пусть в графе $G$ есть сильно связные компоненты $C$ и $D$, и есть дуга $(u, v)\in E$ из $C$ в $D$. Тогда при поиске в глубину в графе $G$ самое позднее время завершения вершины в $C$ превосходит таковое в $D$. 
\end{stat}

\begin{proof}
    Рассматриваются два случая: самое ранне обнаружение в $C$ меньше, чем в $D$, тогда рассматриваем первую обнаруженную вершину $x\in C$, у всех вершин из $D$ время окончания меньше, чем у неё. Если же это не так, то рассмотрим самую раннюю $y\in D$. Все остальные из этой компоненты будут обнаружены на рекурсивных вызовах, а из $C$ на этом этапе не обнаружатся, поэтому время окончания всех вершин из $C$ больше. 
\end{proof}

\begin{theorem}
    Вершины каждого дерева, найденного алгоритмом Косадаржу-Шарира при втором поиске в глубину - это и есть сильно связные компоненты исходного графа. 
\end{theorem}

\begin{proof}
    Индукция по количеству найденных компонент связности. Надо доказать, что если первые $k$ найденных компонент связности действительно таковы, то и следующая также обладает этим свойством.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 2} 

\subsection{Поиск в алгоритме с весами.}

\begin{prob}
    Пусть в орграфе для каждой дуги задан вес. Нужно найти пуи наименьшего веса из данной вершины $s\in V$ во все вершины графа.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t14}{\textbf{Алгоритм Беллмана-Форда}}}. Для каждой вершины вычисляются значения $d_v$ - наименьший вес пути из $s$ в $v$ и $\pi_v$ - предыдущая вершина на пути наименьшего веса из $s$ в $v$. Изначально полагается, что $d_v=\infty$ и $\pi_v=\text{NULL}$ для всех вершин, и $d_s=0$. Далее алгоритм постепенно находит пути меньшего веса в другие вершины, запоминая веса лучших из найденных путей в этих переменных, Значения уменьшаются с помощью элементарной операции улучшения пути, используя некоторую дугу $(u, v)\in E$. Если $d_u+w_{u, v}<d_v$, то $d_v=d_u+w_{u, v}$, а $\pi_v=u$. Эта операция применяется, пока можно что-то улучшить. Как будет показано, для этого достаточно рассмотреть все дуги $|V|-1$ раз.
\end{alg}

\begin{stat}
    После $i$-ой итерации внешнего цикла алгоритм Беллмана-Форда находит все пути наименьшего веса длины не более чем $i$. 
\end{stat}

\begin{proof}
    Индукция по $i$. 
\end{proof}

\begin{theorem}
    Алгоритм Беллмана-Форда за $|V|\cdot|E|$ шагов или правильно вычисляет пути наименьшего веса из вершины $s$ во все вершины, или сообщает о наличии достижимого цикла отрицательного веса. 
\end{theorem}

\begin{proof}
    Рассмотрим систему после $|V|-1$ итераций, и согласно утв. 7, нейдены все пути наименьшего веса, состоящие не более чем из $|V|-1$ дуг. Если какой-то путь ещё можно улучшить, то в этом пути какая-то вершина встретилась дважды, следовательно, найден цикл отрицательного веса. Если же ничего нельзя улучшить, то любой достижимый цикл будет иметь неотрицательный вес, так как для последовательный верщин цикла можно записать не условие улучшения и сложить по все парам.
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t15}{\textbf{Алгоритм Дейкстры}}}. Этот алгоритм решает ту же задачу, однако на каждом шаге находит очередную вершину $u$, путь наименьшего веса в которую уже известен. Тогда плгоритм рассматривает все дуги, исходящие из вершины $u$. Это позволяет ему рассматривать каждую дугу графа лишь однажды.
\end{alg} \

\begin{theorem}
    Алгоритм Дейкстры работает правильно.
\end{theorem}

\begin{proof}
    Индукцией по длине вычисления доказываем, что для всякой вершины $u\notin Q$, путь наименьшего пути уже построен. Для этого рассматриваются две вершины на кратчайшем пути - одна в среди рассмотренных, другая - среди не рассмотренных ($Q$), и для последней проводятся вычисления, связанные с длиной самого короткого пути для неё.
\end{proof}

\begin{remark}
    Для представления множества $Q$ алгоритм использует особую структуру данных: \textcolor{magenta}{\hypertarget{d3}{\textit{очередь с приоритетами}}}. Каждый элемент $Q$ находится там вместе со своим текущим значением $d_v$. Также заданы операции: 

    \begin{itemize}
        \item $insert(x)$ - вставить новый элемент; 
        \item $min()$ - выдать минимальный элемент; 
        \item $extract_{min}()$ - выдать минимальный и удалить; 
        \item $decrease(x, k)$ - изменить значение элемента $x\in Q$ на $k$, если $k$ меньше текущего значения $x_i$. 
    \end{itemize}

    Скорость работы алгоритма: $|V|$ раз $extract_{min}$ и $|E|$ раз $decrease$, поскольку каждая дуга обрабатывается лишь однажды. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 3}

\subsection{Окончание поиска с весами.}

Сложность алгоритма Дейкстры зависит от того, как реализвана очередь с приоритетами. Тупая реализация: хранить массив $x_v$, индекцированный по $v\in V$. Тогда $decrease$ работает за $O(1)$, но $extract_{min}$ требует время $|V|$. Отсюда общее время работы - $|V|^2+|E|=O(|V|^2)$. \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t16}{\textbf{Улучшение Дейкстры кучей}}}. Используется куча, в которой значение в каждой внутренней вершине не больше, чем значение в любом из её потомков (min-heap). Операции снатовятся следующими:

    \begin{itemize}
        \item $insert(x)$ - вставить новый элемент (он становится листом), после чего дать ему всплыть до его законного места; 
        \item $min()$ - выдать минимальный элемент (просто вернуть $x_1$); 
        \item $extract_{min}()$ - переместить $x_n$ в $x_1$, убрав его в конце, а затем запустить исправление кучи из корня, то есть, heapify(1); 
        \item $decrease(i, k)$ - изменить значение элемента $x_i$ на $k$, после чего дать элементу $x_i$ всплыть наверх, пока возможно. 
    \end{itemize}
\end{alg}

\begin{remark}
    \textcolor{magenta}{\hypertarget{d4}{\textit{Дать всплыть}}} - значит, если $x_i$ меньше своего родителя, то он обменивается так до тех пор, пока не займёт положенное место.
\end{remark}

\subsection{Нахождение минимального остовного дерева.}

\begin{prob}
    Дан неориентированный связный граф, рёбрам которого сопоставлены числа - веса. Нужно найти одно из остовных деревьев с наименьшим весом.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t17}{\textbf{Общий принцип действия алгоритмов}}}. Одно за другим присоединяются рёбра к поддереву какого-то минимального, чтобы опять получилось поддерево минимального.
\end{alg}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d5}{\textit{Сечение}}} графа - разбиение множества вершин на два дизъюнктныхмножества. Ребро \textcolor{magenta}{\hypertarget{d6}{\textit{пересекает сечение}}}, если один из его концов лежит в одной части, а другой - во второй. 
\end{defn}

\begin{stat}
    Пусть $T$ - одно из минимальных остовных деревьев графа, а $F$ - его подмножество. Пусть также имеется какое-то сечение, что никакое ребро из $FF$ его не пересекает. Тогда ребро с наименьшим весом, пересекающее это сечение, принадлежит некоторому минимальному остовному дереву $T'$, которое также содержит $F$. 
\end{stat}

\begin{proof}
    Если $(u, v)$ - такое ребро, входит в $T$, то нам подойдёт $T$. Если его там нет, то рассмотрим цикл, который с нима полчуается и поменяем его на другое ребро из цикла, которое пересекает сечение.
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t18}{\textbf{Алгоритм Прима}}}. Начинаем с произвольной вершины и на каждом шаге добавляем ребро из одной из уже имеющихся вершин в некоторую незадействованную (естественно, из всевозможных рёбер выбирается то, у которого минимальный вес). \ 
    
    Время работы: $|E|\log |V|$, так как выполняется $|V|$ операций внешнего цикла. Операции над очередью с приоритерами - $\log|V|$. Внутренний цикл по $v$: за всё время работы алгоритма каждое ребро рассматривается дважды: с одного конца и с другого. Поэтому тело цикла в общей сложности выполняется $2|E|$ раз, и всякий раз выполняется операция над очередью с приоритетами за время $O(\log |V|)$. 
\end{alg}

\begin{remark}
    Структура данных алгоритма - очередь с приотитетеми, в которой хранятся все вершины, ещё не попавшие в дерево. Значение $d_v$ каждойвершины $v$ - это наименьший вес ребра, соединяющий её с деревом. Также для $v$ запоминается вершина $\pi_v$, через которую $v$ соединена с деревом ребром наименьшего веса. \ 

    Всякий раз, когда в дерево добавляется новая вершина $u$, для любой вершины $v$ не из дерева моэет оказаться, что ребро $(u, v)$ легче, чем ранее известное ребро наименьшего веса $(\pi_v, v)$, соединяющее её с деревом, и это тка, если $w_{u, v}<d_v$. В этом случае значения $d_v$ и $\pi_v$ обновляются, переключая $v$ на соединение с деревом через $u$. 
\end{remark}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t19}{\textbf{Алгоритм Крускала}}}. Текущее подмножество остовного дерева - лес, соединяем компоненты самыми лёгкими путями.
\end{alg}

\begin{theorem}
    На каждом шаге работы алгоритма Крускала переменная $T$ содержит подмножество одного из остовных деревьев минимального веса.
\end{theorem}

\begin{proof}
    Индукция по длине вычисления.
\end{proof}

\begin{stat}
    При использовании леса непересекающихся множеств алгоритм работает за время $|E|\log |E|$ - и, стало быть, $|E|\log |V|$. 
\end{stat}

\begin{proof}
    Сортировка займёт время $|E|\log |E|$. Далее алгоритм выполняет $3|E|$ операций над структурой данных, каждая из которых, при реализации через лес непересекающихся множеств, выполняется за время $O(\log n)$, и в общей сложности получается $|E|\log |V|$, что уже быстрее сортировки.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 8} 

\subsection{Структура данных для непересекающихся множеств.}

\begin{prob}
    Абстрактная структура данных для представителя разбиения множества на непересекающиеся подмножества. У каждого множества есть выжеленный элемент - \textcolor{magenta}{\hypertarget{d7}{\textit{представитель}}}. Заданы операции:

    \begin{itemize}
        \item $make\_set(x)$ - создать одноэлементное множество; 
        \item $find\_set(x)$ - найти представитель множества, содержащего данный элемент; 
        \item $set\_union(x, y)$ - объединение двух множеств.
    \end{itemize}

    Перед нами стоит задача: как эффективно реализовать эту абстрактную структуру данных?
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t20}{\textbf{Лес непересекающихся множеств}}}. У нас есть лес, если одноэлементно - просто вершинка, если представитель - корень дерева, если объединяем, то корень одного будет указывать на корень другого.
\end{alg}

\begin{remark}
    Используются следующие улучшения: \ 
    
    В каждой вершине хранится её условная сложность - \textcolor{magenta}{\hypertarget{d8}{\textit{ранг}}}. Процедура $make\_set(x)$ устанавливает ранг в нуль. При объединении двух деревьев корень дерева меньшего ранга станет указывать на корень дерева большего ранга. Если оба корня имеют одинаковый ранг, то их объединение получит ранг, больший на единицу. \ 

    При выполнении каждой операции поиска все встреченные на пути вершины пренаправятся в корень для ускорения последующих операций цикла.
\end{remark}

\begin{stat}
    Пусь всего элементов - $n$, и к ним применяется $m$ операций. Тогда, с применением первого улучшения, они выполняются за время $O(m \log n)$. 
\end{stat}

\begin{proof}
    Смотрим на дерево, через логарифмы вычисляем.
\end{proof}

\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t21}{\textbf{Тарьян}}}). Пусть всего элементов - $n$, и над ними выполняется $m$ операций. Тогда, с применением первого и второго улучшения, эти операции выполняются за совокупное время $m\alpha (n)$, где $\alpha(n)$ - обратная функция к $A_n(n)$, а $A(k, n)=A_k(n)$ - \textcolor{magenta}{\hypertarget{d9}{\textit{функция Аккермана}}}. 
\end{theorem}

\begin{prob}
    Дан неориентированный граф - возможно, несвязный. Надо определить его компоненты связности и научиться быстро отвечать на вопрос о том, лежат ли две данные вершины в одной компоненте.
\end{prob}

\begin{alg}
    Алгоритм пишется через представление компонент связности в виде структуры данных для непересекающихся множеств.
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 11} 

\section{Лекция 5.}

\subsection{Пути между всеми парами вершин.}

\begin{prob}
    Дан ориентированный граф $G=(V, E)$, где $V = \{1, \ldots, n\}$, а $E\subseteq V \times V$. Стваится задача проверить чуществование пути из каждой вершины в каждую - то есть: для каждой пары вершин $(i, j)$ определить, есть ли путь из $i$ в $j$. После решения мы получим \textcolor{magenta}{\hypertarget{d10}{\textit{транзитивное замыкание}}} графа.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t22}{\textbf{Очевидный алгоритм}}}. Перебираем все пары вершин $(i, j)$ и для каждой пары все промежуточные вершины $k$. Если она соединена с обоими, то добавляем ребро $(i, j)$. Делаем так пока можем.
\end{alg}

\begin{stat}
    После каждой $t$-ой итерации внешнего цикла будут просчитаны все пути длины не более $2^t$, и для каждого из них в граф будет добавлена дуга. Откуда время работы - не более, чем $n^3 \log_2 n$ шагов.
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t23}{\textbf{Алгоритм Варшалла}}}. Можно построить транзитивное замыкание за время $n^3$, обойдясь без внешнего цикла (без пока можем). Для этого достаточно поменять циклы местами и теперь рассматривать каждую вершину как смежную, соединяя все пары, смежные с ней.
\end{alg}

\begin{stat}
    После $k$-ой итерации внешнего цикла найдены все пути, проходящие только через промежуточные вершины из множества $\{1, \ldots, k\}$. 
\end{stat}

\begin{proof}
    Доказательство индукцией по количеству итераций.
\end{proof}

\begin{prob}
    Пусть граф теперь не только ориентированный, но и рёбра имеют вес. Нужно найти кратчайшие пути между вершинами.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t24}{\textbf{Алгоритм Флойда-Варшалла}}}. То же самое, что и алгоритм варшала, но мы ещё и храним вес рёбер.
\end{alg}

\begin{remark}
    О достижимости можно говорит в рамках матриц смежности. Умножение рассматривается как 
    \[
        c_{i, j}=\bigvee_{k=1}^l a_{i, k}\wedge b_{k, l}. 
    \]
    Тогда если $A$ - матрица смежности, то $A^l$ - матрица достижимости по путям длины ровно $l$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 1}

\subsection{Быстрое умножение матриц.}

Ну, какие-то способы усножения - ебанина какая-то, писать не буду.

\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t25}{\textbf{Основная теорема о времени работы рекурсивных алгоритмов}}}). Пусть задача размера $n$ рашается путём разбиения её на $a$ подзадач того же типа, размера $\frac{n}{b}$ каждая, и процесс разбиения, а также соединения результатов занимает время $O(n^c)$. 
    \[
        T(n)=aT(\frac{n}{b})+O(n^c)
    \]
    Тогда время работы алгоритма оценивается как

    \begin{itemize}
        \item Если $c<\log_b a$, то $O(n^{\log_b a})$;
        \item Если $c=\log_b a$, то $O(n^c \log_b n )$; 
        \item Если $c>\log_b a$. то $(n^c)$. 
    \end{itemize}
\end{theorem}

\begin{proof}
    В дереве рекурсии $\lceil \log_b n \rceil$ уровней, на нулевом - одна подзадача размера $n$, рассчёты занимают время $n^c$, на первом - $a$ подзадач размера $\lceil \frac{n}{b} \rceil$, расчёты для каждой занимают врем] $\lceil \frac{n}{b} \rceil^c$, и так далее, на уровне $i$ - а подзадач размера $\lceil \frac{n}{b^i}\rceil$ каждая, расчёты каждой занимают время $\lceil \frac{n}{b^i}\rceil^c$. Общее время работа - сумма по всем уровням рекурсии.
    \[
        T(n)= \sum_{i=0}^{\lceil \log_b n \rceil} a^i\biggl(\frac{n}{b^i}\biggr)^c=n^c \sum_{i=0}^{\lceil \log_b n \rceil} \biggl(\frac{a}{b^c}\biggr)^i
    \] 
    Таким образом, если $a=b^c$, то сумма геометрической прогрессии - $\log_b^n$, и отсюда $O(n^c\log_b n)$. Если $a<b^c$, то сумма ограничена сверху константой, отсюда $O(n^c)$. И если $a>b^c$, то это возрастающая геометрическая прогрессия, и применив формулу для вычисления получим $O(n^{\log_b a})$. 
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 8}

\begin{prob}
    \hypertarget{e2}{Умножение} булевых матриц.
\end{prob}

\begin{alg}
    Можно записать матрицу в виде кучи нулей и единиц, после чего перемножить в кольце вычетов по модулю $n+1$ или по любому удобному модулю, превосходящему $n$. Тогда вместо дизъюнкции конъюнкций будет вычислена сумма произведений по модулю $n+1$ (в точности равная оличества интинных конъюнкций в дизъюнкции конъюнкций, и потому она лежит в диапазоне от 0 до $n$, откуда следует, что по модулю $n+1$ она вычислится точно). Чтобы узнать значение дизъюнкции конъюнкций, достаточно будет проверить вычисленную сумму на равенство нулю.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t26}{\textbf{Метод четырёх русских}}}. Чисто комбинаторный метод умножения булевых матриц за время $o(n^3)$. Пусть $A$ и $B$ - две булевых матрицы размера $n\times n$, цель - вычислить их произведение $C=AB$. Пусть $k$ - число, намного меньшее, чем $\log_2 n$, и пусть $n$ делится на $k$ (если не делится - немного увеличим $n$ до делимости). Каждая строка матрицы $A$ делится на $\frac{n}{k}$ векторов размера $1\times k$, называемых \textcolor{magenta}{\hypertarget{d11}{\textit{кусочками}}}. Кусочки в $i$-ой строке обозначаются через $A_{i, l}, \ldots, A_{i, \frac{n}{k}}\in \mathbb{B}^{1\times k}$. Матрица $B$ разделяется на $\frac{n}{k}$ подматриц размера $k\times n$, называемых \textcolor{magenta}{\hypertarget{d12}{\textit{полосами}}} и обозначаемыми через $B_1, \ldots, B_{\frac{n}{k}}\in \mathbb{B}^{1\times k}$. Тогда всякая $i$-ая строка $C$, обозначаемая через $C_i\in \mathbb{B}^{1\times n}$, представима в виде следующей поэлементной дизъюнкции строк:
    \[
        C_i=\bigvee_{r=1}^{\frac{n}{k}}A_{i, r}B_r. 
    \]
    Произведение кусочка $A_{i, r}$ на соответствующую полосу $B_r$ - это строка размера $1\times n$, и знак дизъюнкуции в формуле для $C_i$ означает поразрядную дизъюнкцию таких строк,
\end{alg}

\begin{remark}
    Казалось бы, никакого принципиального улучшения не получилось, однако с программистсткой точки зрения, это в чём-то удобнее. \ 

    Главная идея метода же состоит в том, что так как $k$ невелико, возможных кусочков всего $2^k$, и кусочки будут часто повторяться. Таким образом, если запомнить произведения таких кусочков на полосы $B$, в плане вычислений это займёт много меньше времени. Общее время работы будет $O\biggl(\frac{n^3}{\log n}\biggr)$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 10}

\section{Лекция 6.}

\subsection{Двоичные деревья поиска}

Речь сейчас пойдёт о структурах данных для представления \textcolor{magenta}{\hypertarget{d13}{\textit{множества}}} различными способами, с разными временем выполнения операций (строго говоря, мы будем говорить о \textcolor{magenta}{\hypertarget{d14}{\textit{мультимножестве}}}, так как может быть несолько элементов с одинаковым значением). Элементы погут быть любого вида, на них определено отношение порядка $''\leq''$. Операции: найти элемент с данным значением; найти наибольший (наименьший) элемент; найти элемент, предшествующий или следующий за данным; вставить / удалить элемент. \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t27}{\textbf{Бинарное дерево поиска}}}. Представление мнлжества в виде дерева так, что вершина содержит одно значение и три указателя: на предка (если корень, то NULL), на левое поддерево и на правое (если их нет, то NULL). При этом все вершины в её левом поддереве содержат не меньшие значения, а все вершины в правом поддереве - не бóльшие. Понятно, как на таком дереве задаются требуемые операции.
\end{alg}

\begin{remark}
    Сложность каждой операции - не более, чем высота дерева. Если дерево сбалансированное, то есть, все пути примерно одинаковой длины, то все операции выполняются за логарифмическое время.
\end{remark}

\begin{prob}
    Даже если дерево сбалансированное, то операции над ним могут првести его к дизбалансу, надо как-то сложниить структуру, чтобы избежать критических изменений.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t28}{\textbf{АВЛ-деревья}}}. Усложнённая разновидность двоичных деревьев поиска. Такую разновидность называют \textcolor{magenta}{\hypertarget{d15}{\textit{почти сбалансированной}}}, то есть, высота деревьев-потомков одной вершины отличается не более, чем на 1. Этого ограничения достаточно, чтобы дерево мело логарифмическую высоту.
\end{alg}

\begin{stat}
    АВЛ-дерево высоты $h$ содержит не менее, чем $F_{h+3}-1$ вершин, где $F_n$ - $n$-ое число Фибоначчи.
\end{stat}

\begin{proof}
    Индукция по $h$. База очевидна, переход таков. Если дерево имело высоту $h$, то один из его потомков имеет высоту $h-1$, а другой - высоту не менее, чем $h-2$, то есть, не менее чем $F_{h+2}-1$ и $F_{h+1}-1$ вершин соответственно. Всего, с учётом корня и по определению чисел Фибоначчи, как раз и получится то, что нужно.
\end{proof}

\begin{stat}
    Высота АВЛ-дерева с $n$ вершинами не превосходит $\log_\varphi n$, где $\varphi$ - золотое сечение.
\end{stat}

\begin{remark}
    Естественно, просто так АВЛ-дерево не получить, поэтому придумана операция \textcolor{magenta}{\hypertarget{d16}{\textit{вращение}}}, которая состоит в том, что если у нас есть корень $y$, из которого направо идёт поддерево $t_3$, а налево - поддерево с корнем $x$ и поддеревьями $t_1$ и $t_2$ налево и направо соответственно. Тогда мы подвешиваем дерево за $x$ и перебрасываем его поддерево $t_2$ в левое поддерево $y$. При помощи такой операции можно исправить балансировку при различных операциях, подробно расписывать я это пока что не буду.
\end{remark}

\begin{prob}
    Двоичные деревья рассчитаны на то, чтобы храниться в оперативной памяти компьютера, а для того, чтобы хранить деревья во внешней, медленной памяти, требуется некая адаптация.
\end{prob}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l6.pdf}{6 лекция, страница 1}

\subsection{B-деревья.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t29}{\textbf{B-деревья}}}. Адаптация деревьев поиска для хранения во внешней памяти. Основная мысль - использовать вершины большой степени - с тем, чтобы каждая вершина занимала один блок, а высота дерева уменьшилась бы. \ 

    Пусть вершины в двоичном дереве - это 2-вершины, поскольку у каждой из них 2 потомка и 1 значение, по которому эти потомки разделяются. У $m$-вершины - $m$ потомков (деревья $t_1, \ldots, t_m$ - возможно, пустые), и в ней находится $m-1$ значение: $x_1, \ldots, x_{m-1}$, где значения упорядочены по неубыванию. Все значения в каждом поддереве $t_i$, больше или равны $x_i$, и меньше или равны $x_{i+1}$. \ 

    В B-дереве могут одновременно соержаться вершины различных степеней: выбирается некоторое число $k\geq 2$, после чего корень может иметь степень от 0 до $2k$, а все остальные вершины - любые степени от $k$ до $2k$. При этом, дерево сбалансировано.
\end{alg}

\begin{remark}
    То, как производятся требуемые операции - лучше один раз увидеть и немного прочитать в конспекте А. С. Охотина, чем читать мои заметки.
\end{remark}

\begin{alg}
    B-дерево для $k=2$ называется $2-3-4$ деревом, и такое дерево очень удобно хранить в оперативной памяти. 
\end{alg} \ 

\begin{alg}
    Красно-чёрное дерево - это представление 2-3-4 дерева в виде двоичного дерева, в котором каждая вершина представлена в виде одной или несколькиз связанных между собою двоичных вершин, каждая из которых покрашена в один из двух цветом. Каждая 2-вершина остаётся собой и считается чёрной. Каждая 3-вершина разбивается на две двоичных: чёрную - корень поддерева, и красныю вершину - правого или левого потомка чёрной. Далее - к этой паре вершин, так же как и к исходной 3-вершине, присоединяются три поддерева. Наконец, каждая 4-вершина разбивается на три двоичных - чёрную и два красных потомка, к которым присоединены четыре поддерева.
\end{alg}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l6.pdf}{6 лекция, страница 7}

\section{Лекция 7.}

\subsection{Полиномиальное хэширование.}

\begin{prob}
    Дана длинная строка $w=a_1\ldots a_n$ и короткая искомая строка $x=b_1\ldots b_m$. Требуется найти все вхождения $x$ в $w$ в качестве подстроки.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t30}{\textbf{Хэш-функции}}}. Каждой строке ставится в соответствие некоторое число $w\mapsto h(w)$, и это число хранится вместе со строкой. Тогда, если нужно сравнить две строки, то мы сравниваем сперва соответствующие им числа. Если числа разные, то и строки точно разные. Если соответствующие числа одинаковые, то надо сравнить строки и получить итоговый результат. Самое тупое - сложить коды всех символов, но тогда распределение неравномерное, и вообще это не очень кайфовый вариант. Намного лучше \textcolor{magenta}{\hypertarget{d17}{\textit{полиномиальное хэширование}}}. Берём некоторое основание степени $p$, пусть $w=a_1\ldots a_l$ - строка, тогда используется сумма $\sum_{i=1}^l a_i\cdot p^{l-i}$, взятая по некоторому модулю $M$.
\end{alg} \

\begin{alg}
    \textcolor{magenta}{\hypertarget{t31}{\textbf{Алгоритм Рабина-Карпа}}}. Алгоритм поиска подстроки $x=b_1\ldots b_m$ в строке $w-a_1\ldots a_n$, основанный на полиномиальном хэшировании степени $m-1$: сперва вычисляется значение хэщ-функции для искомой строки, а затем для всех $m$-символьных подстрок данного текста последовательно вычисляется значение их хэш-функции. Когда значение для подстроки и искомой строки совпадают, производится прямое сравнение. Значение хэш-функции для искомой строки: $X=\sum_{i=1}^m b_i\cdot p^{m-i}$ по модулю $q$, а значение хэш-функции для подстроки со смещением $s$: $W_s=\sum_{i=1}^m a_{s+i}\cdot p^{m-1}$ по модулю $q$. Алгоритм сперва вычисляет $X$, а затем - последовательно все $W_s$. Из соотношения $W_{s+1}=pW_s-a_{s+1}p^m+a_{s+m+1}(\mod{q})$. Сложность: $\Theta(m)$ - на подготовку, и затем в худшем случае $O(mn)$, если хэ=-функция выдаст одинаковые значения для всех подстрок. В среднем случае получается время работы $\Theta(n)$. 
\end{alg}

\begin{prob}
    (Наибольшая общая подстрока). Даны две строки: $u=a_1\ldots a_m$ и $v=b_1\ldots b_n$. Надо найти самую длинную их общую подстроку $x$. 
\end{prob}

\begin{alg}
    Тупое решение: для каждой пары позиций найти самую длинную подстроку, время $((m+n)^3)$. 
\end{alg} \

\begin{alg}
    Улучшение через полиномиальное хэширование - научиться отвечать на вопрос ''Есть ли общая подстрока длины $l$?'' за время $O((m+n)\log(m+n))$. Для этого находятся значения хэш-функции для всех подстрок $u$ длины $l$, размещаются в двоичном дереве поиска - время $m\log m$. То же самое - для $v$ за время $n\log n$. 
\end{alg} \

\begin{alg}
    Улучшение через двоичный поиск по $l$. Его можно записать в виде рекурсивной процедуры, отвечающей на вопрос ''Найти длину наибольшей общей подстроки, если известно, что её длина не меньше, чем $l_1$, и строго больше, чем $l_2$?''. Процедура $f(l_1, l_2)$: если $l_2-l_1=1$, то возвращаем $l_1$, $k=\lfloor \frac{l_1+l_2}{2}\rfloor$. Теперь, если есть общая подстрока длины $k$, то возвращаем $f(k, l_2)$, иначе возвращаем $f(l_1, k)$. 
\end{alg}

\begin{prob}
    Найти в строке самую длинную подстроку-палиндром.
\end{prob}

\begin{alg}
    Самый тупой алгоритм: искать вокруг каждого симыола на длину вплоть до $\frac{n}{2}$, время $O(n^2)$. 
\end{alg} \

\begin{alg}
    Улучшение основано на решении подзадачи: ''Есть ли подстрока-палиндром данной длины $l$?''. После того, как будет построен алгоритм для решения подзадачи, останется использовать двоичный поиск по $l$. 
\end{alg} \

\begin{alg}
    Решение через небольшую переформулировку: разворачиваем строку и для таковой и оригинальной рассматриваем хэш-ффункции для всех подстрок длины $l$. После чего, естественн, надо выполнить посимвольную проверку. Двоичный поис по $l$ даст время $O(n\log n)$. 
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 2}

\subsection{Алгоритм Кнута-Морриса-Пратта.}

\begin{alg}
    \hypertarget{e3}{Строка} $w$ читается слева направо, и после чтения $i$ символов $w$ он помнит длину $j$ самого длинного префикса строки $x$, на которой заканчивается $a_1 \ldots a_i$ - то есть, наибольшее число $j$, для которого верно $a_{i-j+1}\ldots a_i=b_1\ldots b_j$. \ 

    При чтении очередного символа $a_{i+1}$ алгоритм сравнивает его с $b_{j+1}$, и если эти символы равны, это значит, что префикс $x$ длины $j$ продлевается на один символ. Если же $a_{i+1}\neq b_{j+1}$, то самый длинный префикс $x$, на который заканчивается строка $a_1\ldots a_{i+1}$, будет содержать строго меньше, чем $j+1$ символов. Чтобы найти длину этого префикса, нужно рассмотреть следующий по длине префикс $x$ на который заканчивается $a_1\ldots a_i$, и пытаться продлевать уже его. \ 

    Чтобы иметь возможность находить длину такого префикса, алгоритм заранее строит по данной искомой строке $x$ определённую стуктуру данных - \textcolor{magenta}{\hypertarget{d18}{\textit{префиксную сумму}}}.
\end{alg}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d19}{\textit{Префиксная сумма}}} для строки $x=b_1\ldots b_m$ - это функция $\pi\{1, \ldots, m\}\rightarrow \{0, \ldots, m-1\}$, которая для всякого префикса $b_1\ldots b_i$ строки $x$ выдаёт длину наибольшего суффикса подстроки $b_1\ldots b_i$, который также является префиксом $x$. 
    \[
        \pi(i)=\max\{j|j<i, b_1\ldots b_j = b_{i-j+1}\ldots b_i\} 
    \]
\end{defn}

\begin{remark}
    Повторно применяя функцию $\pi$ можно получить все префиксы $x$, являющиеся суффиксами $b_1\ldots b_i$, и алгоритм будет их перебирать, пока не найдёт такой, который можно продолжить следующим символом текста. 
\end{remark}

\begin{stat}
    Имея готовую таблицу значений функции $\pi$, алгоритм KMP работает за время $\Theta(n)$. 
\end{stat}

\begin{remark}
    Осталось научиться быстро строить значение префиксной функции.
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 4}

\subsection{Поиск с помощью конечных автоматов.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d20}{\textit{Детермированный конечный автомат (DFA)}}} - пятёрка $A=(\Sigma, Q, q_0, \delta, F)$, со следующим значением компонентов:
    
    \begin{itemize}
        \item $\Sigma$ - алфавит; 
        \item $Q$ - конечное множество состояний; 
        \item $q_0\in Q$ - начальное состояние; 
        \item $\delta:Q\times \Sigma \rightarrow Q$ - функция переходов. Если автомат находится в состоянии $q\in Q$ и читает символ $a\in \Sigma$, то его следующее состояние - $\delta(q, a)$; 
        \item $F\subseteq Q$ - множество \textcolor{magenta}{\hypertarget{d21}{\textit{принимающих состояний}}}.
    \end{itemize}

    Для всякой входной строки $w=a_1\ldots a_l$, где $l\geq 0$ и $a_1, \ldots, a_l\in \Sigma$, вычисление - последовательность состояний $p_0 \ldots, p_{l-1, p_l}$, где $p_0=q_0$, и всякое следующее состояние $p_i$, где $i\in\{1, \ldots, l\}$, однозначно определено как $p_i=\delta(p_{i-1}, a_i)$. 
    \[
        p_0\rightarrow^{a_1}p_1\rightarrow^{a_1}\ldots \rightarrow^{a_l} p_l
    \]

    Строка \textcolor{magenta}{\hypertarget{d22}{\textit{принимается}}}, если последнее состоение $p_l$ принадлежит множеству $F$ - иначе \textcolor{magenta}{\hypertarget{d23}{\textit{отвергается}}}. Множество строк, \textcolor{magenta}{\hypertarget{d24}{\textit{распознаваемое}}} автоматом, обозначаемое через $L(A)$ - состоит из всех строк, которые он принимает.
\end{defn}

\begin{theorem}
    Для всякой строки $x=b_1\ldots b_m$ над алфавитом $\Sigma$ существует DFA с $m+1$ состояниями, распознающий множество $\Sigma^* x$ всех строк, заканчивающихся на $x$, и этот DFA можно построить за время $\Theta(|\Sigma|\cdot m)$. 
\end{theorem}

\begin{proof}
    Пока что записывать не буду.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 6}

\subsection{Представления множеств строк.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d25}{\textit{Лексикографический порядок}}}. Поэлементное сравнение слева направо.
\end{defn}

\begin{prob}
    Как хранить строки?
\end{prob}

\begin{alg}
    В виде списка или двоичного дерева поиска, однако это всё хуйня.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t32}{\textbf{Префиксное дерево}}} - структура для хранения множеств строк. Корневое дерево с дугами, помеченными символами алфавита. Дуги, исходящие ииз всякой вершины, должны быть помечены различными символами. Каждая вершина соответствует некоторой строке, которая хранится неявно в виде последовательности дуг на пути к ней. Также всякая вершина хранит один бит информации, принадлежит ли эта строка ммножеству; вместе с битом можно хранить любые дополнительные значения, сопоставленный в этой строке. Корень соответствует пустой строке. Если вершина соответствует строке $w$, и исходящая из неё дуга помечена символом $a\in \Sigma$, то эта дуга идёт в вершину, соответствующую строке $wa$. \ 

    Все операции с любой данной строкой длины $l$ (поиск, вставка, удаление) выолняются за время $O(l)$, не зависящее от числа элементов в хранимом множестве. \ 

    \textcolor{magenta}{\hypertarget{d26}{\textit{Компактное префиксное дерево}}} - объединяем несколько дуг в одну, если они идут одинаково параллельно.
\end{alg}

\begin{stat}
    В компактном префиксном дереве для множества из $n$ элементов не более, чем $n$ внутренних вершин.
\end{stat}

\subsection{Поиск нескольких строк одновременно: алгоритм Ахо-Корасик.}

\begin{prob}
    Пусть для данного текста нужно найти все вхождения не одной строки, а всех строк из конечного множества $K=\{x_1, \ldots, x_k\}$. 
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t33}{\textbf{Алгоритм Ахо-Корасик}}}. Данный алгоритм будет помнить самый длинный только что прочитанный префикс одной из строк $x_1, \ldots, x_k$, а точнее сказать, самую длинную такую строку $u$, что $u$ - префикс какой-то строки из $K$, и алгоритм только что прочитал $u$. 
\end{alg}

\begin{remark}
    Для решения обобщается префиксная сумма и построение функции $\pi$ (работающее за линейное время), но пока что упущу подробности.
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 9}

\subsection{Суффиксные деревья.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t34}{\textbf{Суффиксное дерево}}} - структура данных, строящаяся по данной строке и обеспечивающая эффективный поиск подстрок в этой строке. Понятно, что это такое.  
\end{alg}

\section{Лекция 9.}

\subsection{Сжатие данных, основанное на повторении строк.}

\begin{prob}
    Нужно научиться использовать часто повторяющиеся подстроки, чтоы хранить их отдельно в памяти и эффективно использовать при хранении целого текста.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t35}{\textbf{Метод Лемпеля-Зива LZ77}}}. Алгоритм сжатия данных, использующий повторяемость подстрок. Основная идея в том, что каждый раз, когда ранее прочитанная подстрока встречается повторно, алгоритм пишет вместо неё ссылку на предыдущее вхождение. На каждом шаге выводится тройка $(d, l, a)$, что означает: сперва повторяется подстрока длины $l$, ранее встречавшаяся $d$ символов назад, а потом идёт символ $a$. Реализуется самым простым образом через суффиксное дерево, в котором на каждом шаге читаются следующие символы входной строки, пока не находится самая длинная раньше встречавшаяся подстрока. Обычно используется небольшое улучшение - \textcolor{magenta}{\hypertarget{d27}{\textit{''скользящее окно''}}}, то есть, подстроки ищутся (тем же суффиксным деревом, скорее всего) в окне из последних $m$ элементов, а остальные забываются.
\end{alg} \ 

\begin{theorem}
    Жадный LZ77 оптимален.
\end{theorem} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t36}{\textbf{Метод Лемпеля-Зива LZ78}}}. Тут уже по мере чтения строки строится словарь из часто встречающихся строк в виде префиксного дерева. При декодировании строится в точности этот же словарь. В сжатом представлении строки словарь не хранится. \ 

    В начале в словаре содержится только один элемент под номером нуль: $T_0=\varepsilon$, иными словами, префиксное дерево состоит из корня, помеченного номером 0. На каждом шаге читается самая длинная строка $T_j=v$, и уже имеющаяся в словаре, и выводится её код $j$; также читается и выводится следующий символ $a$. При этом в словарь добавляется новая трока $va$ - конкатенация только что прочитанной со следующим входным символом. \ 

    На префиксном дереве это выглядит так: после вывода очередной пары $(j, a)$ алгоритм переходит в корень префиксного дерева, и дальше читает столько входных символов, сколько возможно. Когда очередной символ прочитать нельзя, создаётся новый лист, при этом выводится номер предыдущей вершины и прочитанный символ. \ 

    При декодировании строится то же самое префиксное дерево. Алгоритму при этом потребуется находить строку по номеру в таблице: можно, например, завести для этого отдельный массиы строк, или же читать в префиксном дереве путь из вершины в корень. Прочитав очередную пару $(j, a)$ алгоритм выводит строку $T_j$ и символ $a$, после чего сразу перескакивает в вершину $j$ и присоединяет к ней новый лист, с переходом по символу $a$. 
\end{alg} \ 

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l9.pdf}{9 лекция, страница 1} \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t37}{\textbf{Преобразование Берроуза-Вилера}}}. Перутся все циклические сдвиги строки $w=a_1\ldots a_n$. Они сортируются лексикографически, получается таблица $T$ размера $n\times n$, где в каждой строчке - один из циклических сдвигов. наконец, берётся последовательность последних символов $b_1\ldots b_n$ в таблице $T$ и номер $t$ строки $w$ в этой таблице. \ 

    Если в исходной строке какая-то подстрока $au$ часто повторялась, то есть много циклических сдвигов, начинающихся с $u$ и заканчивающихся на $a$, и после сортировки они окажутся рядом. Поэтому в преобразованной строке будут длинные последовательности повторяющихся символов. Использование в качестве метода сжатия данных: преобразовать, а потом применять другие методы к преобразованной строке. \ 

    Утверждается, что по последовательности $b_i$, и $t$ восстанавливается исходная $w$. 
\end{alg}

\begin{stat}
    BWT можно вычислить за линейное время.
\end{stat}

\begin{stat}
    Обратное к BWT преобразование можно вычислить за линейное время.
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l9.pdf}{9 лекция, страница 4}

\newpage

\begin{center}
\includegraphics{tiskin}

Торжественно начинается часть конспекта, основанная на лекциях \ 

доцента ФМКН СПБГУ, кандидата физико-математических наук \ 

Александра Владимировича ТИСКИНА 
\end{center}

\section{Преобразование Фурье.}

\subsection{Введение в преобразование Фурье.}

\begin{alg}
    \textbf{Дискретное проеобразование Фурье степени $n$ (DFT$_n$)} - $F_{n, \omega}\cdot a = b$, где $n$ - обратим в $R$, коммутативном кольц без делителей нуля, $\omega$ - первообразный корень $\sqrt[n]{1}$, $a=[a_0, \ldots, a_{n-1}]^T\in R^n$, $b=[b_0, \ldots, b_{n-1}]^T\in R^n$, а $F_n, \omega$ - матрица
    
    \begin{equation*}
        [\omega^{ij}]_{i, j=0}^{n-1}=
        \begin{vmatrix}
            1 & 1 & 1 & \dots & 1 \\
            1 & \omega & \omega^2 & \dots & \omega^{n-1} \\
            1 & \omega^2 & \omega^4 & \dots & \omega^{n-2} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            1 & \omega^{n-1} & \omega^{n-2} & \dots & \omega 
        \end{vmatrix}
    \end{equation*}

    Трудоёмкость: наивный алгоритм, время - $O(n^2)$. 
\end{alg} \

\begin{alg}
    \textbf{Обратное DFT}. $\frac{1}{n}F_{n, \omega^{-1}}\cdot b = a$.
\end{alg}

\begin{remark}
    Используется для \textit{вычисления значений} полинома $a(x)=a_0+a_1x+\ldots+a_{n-1}x^{n-1}$ на $C=\{1, \omega, \ldots, \omega^{n-1}\}$. Аналогично для \textit{интерполяции} полинома $a(x)$ по значениям на $С$ - $\frac{1}{n}F_{n, \omega^{-1}}\cdot a$. \ 

    $\frac{1}{\sqrt{n}}F_{n, \omega^{-1}}\cdot a$ - разложение $a$ по \textit{базису Фурье} (строкам $\frac{1}{\sqrt{n}}F_{n, \omega}$). 
\end{remark}

\subsection{Быстрое преобразование Фурье.}

\begin{alg}
    \textbf{Бвстрое преобразование Фурье (FFT)}. Общая идея: исользовать разложение $n$ на множители и структцтц множества корней из единицы для ускорения вычислений. То есть, пусть $n=n'n''$, $1<n'\leq n'' < n$. В алгоритме FFT 

    \begin{itemize}
        \item вектор $a$ записывается в виде $n'\times n''$-матрицы по строкам; 
        \item вектор $b$ записывается в виде $n''\times n'$-матрицы по строкам; 
        \item DFT$_n$ выражается в виде DFT$_{n'}$, DFT$_{n''}$ над столбцами матриц; 
        \item DFT$_{n'}$, DFT$_{n''}$ вычисляются рекурсивно. 
    \end{itemize}
\end{alg} \

\begin{alg}
    Некоторые классические варианты: $n=\frac{n}{2}\cdot 2$ - \textbf{FFT с прореживанием по времени} (FFT-DIT), и $n=2\cdot \frac{n}{2}$ - \textbf{FFT с прореживанием по частоте} (FFT-DIF). 
\end{alg} \ 

\begin{alg}
    Наиболее общий слуяай - \textbf{шастиэтапное FFT}. Пусть $n=n'n''$, запишем матрицы в таком виде:

    \begin{equation*}
        A=
        \begin{vmatrix}
            a_0 & a_1 & \dots & a_{n''-1} \\
            a_{n''} & a_{n''+1} & \dots & a_{2n''-1} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{n-n''} & a_{n-n''+1} & \dots & a_{n-1} 
        \end{vmatrix}, 
    \end{equation*}

    и аналогично $B$. Тогда получим, что $B=F_{n'', \omega^{n'}}\cdot (G_{n', n'', \omega}\circ (F_{n', \omega^{n''}}\cdot A))^T$, где $G_{n', n'', \omega}=[\omega^{tv}]_{t, v}: n'\times n''$ - матрица \textit{поворотных множителей}, а оператор $\circ$ - \textit{умножение Адамара} (поэлементное умножение матриц). \ 

    И, действительно, данная схема почти шестиэтапная:

    \begin{itemize}
        \item вектор $a$ записывается в матрицу по строкам; 
        \item $n''$ независимых DFT$_{n'}$ над столбцами, вычисляются рекурсивно; 
        \item транспозиция; применение поворотных множителей (2 этапа); 
        \item $n'$ назависимых DFT$_{n''}$ над столбцами, вычисляются рекурсивно; 
        \item вектор $b$ считывается из матрицы по строкам.
    \end{itemize}
\end{alg}

\begin{remark}
    Трудоёмкость FFT-DIT - $O(n \log n)$, трудоёмкость симметричного FFT ($n'=n''$) - $O(n \log n)$. 
\end{remark}

\begin{remark}
    Если вам интересен \textit{граф-бабочка} и схемы FFT, то можете заглянуть в лекции Тискина. Я их, конечно же, перерисовывать не буду.
\end{remark}

\subsection{Задачи и алгоритмы решения.}

\begin{prob}
    Умножение полиномов. Пусть у нас есть два полинома с комплексными коэффициентами. Нас интересует их произведение. Тупейший алгоритм сделает это за время $O(n^2)$. 
\end{prob}

\begin{alg}
    \textbf{Алгоритм Карацубы.} Поделим многочлены $a$ и $b$ (сделаем их одинаковой длины) на две равные части (по $m$), и запишем $a(x)=a'(x)+a''(x)x^m$, аналогично с $b(x)$. Тогда $c(x)=$
    \[
        = a'b'+((a'+a'')(b'+b'')-a'b'-a''b'')x^m+a''b''x^n. 
    \]
    Таким образом, вместо 4 умножений полиномов с $m$ членами получилось 3. Трудоёмкость процесса получилась $O(n^{\log_2 3})$.  
\end{alg}

\begin{prob}
    Быстрая интерполяция.
\end{prob}

\begin{alg}
    Вот если мы посмотрим на ту красивую матрицу $F$, то заметим, что применив обратное преобразование Фурье к столбцу из значений в корнях из единицы, то как раз получим то, что нужно. В этом нетрудно убедится, провернув преобразования не в эту обратную, а в прямую сторону. Трудоёмкость: $O(n\log n)$. 
\end{alg}

\begin{defn}
    \textit{Свёртка} (линейная) $a$ и $b$ - двух последовательностей длины $n$ - $c=a*b$, что находится по формуле $c_i=\sum_{0\leq j\leq i <2n} a_j'b_{i-j}'$, где $a'$ и $b'$ - дополнения изначальных до длины $2n$. Есть ещё циклическая и косоциклическая свёртки, но они противные.
\end{defn}

\begin{alg}
    Суть - то же умножение полиномов. $c=\frac{1}{n} F_{2n, \omega^{-1}}((F_{2n, \omega}a')\circ (F_{2n, \omega}b'))$.
\end{alg}

\begin{remark}
    Немного про количество битов, необходимых для операций прямого и обратного $DFT$ в $\ZZ_n$:

    \begin{itemize}
        \item сумма двух значений: в результате $(\frac{nr}{2}+1)+1=\frac{nr}{2}+2$ бит;
        \item умножение на поворотный множитель $\omega^q=2^{qr}$, $0\leq q< n$: сдвиг на $qr<nr$ бит: в результате - не более $(\frac{nr}{2}+1)+nr-1=\frac{3nr}{2}$ бит; 
        \item приведение значений с записью из $\frac{3nr}{2}=O(nr)$ бит.
    \end{itemize}

    Трудоёкость каждой операции над значениями: $O(nr)$ битовых операций. Что касается трудоёмкости всего DFT$_n$ в $\ZZ_N$: на вход/выход - $n\cdot O(nr)=O(n^2r)$ бит, и $O(n \log n)$ сложений и умножений на поворотные множители, итого $O(n^2r \log n)$ битовых операций.
\end{remark}

\begin{prob}
    Эффективное умножение многозначных чисел.
\end{prob}

\begin{alg}
    \textbf{Алгоритм Карацубы.} Как и умножение многочленов, просто сделаем из чисел многочлены поразрядно. В качестве значения на лекции рассматривалась двойка.
\end{alg} \

\begin{alg}
    \textbf{Алгоритм Шенхаге-Штрассена.} Основная мысль состоит в том, что мы разбиваем $a$ и $b$ на разряды правильно выбранной длины $l$ и рассматриваем каждый разряд как коэффициент полинома, всего $n/l$ коэффициентов в каждом полиноме. Используем быстрое преобразование Фурье для их умножения. Попарное умножение значений полиномов выполняется рекурсивно, затем учитываются переносы. Сам алгоритм, ну там пиздец намешали и КТО, и косоциклическую свёртку, и Карацубу. В сухом итоге, трудоёмкость - $O(n\log n \log \log n)$.
\end{alg}

\section{Параллельные алгоритмы.}

\subsection{Базовые определения и понятия.}

\begin{defn}
    \textit{Схема} (вычислительная) - ориентированный ациклический граф (dag), с фиксированным количеством входов и выходов. Вычисления неадаптивны (oblivious): последовательность операций не зависит от входа. \ 

    Вычисления над переменным количеством входов: (бесконечное) \textit{семейство схем}. Семейство схем с конечным описанием - \textit{алгоритм}.
\end{defn}

\begin{defn}
    \textit{Размер} схемы - количество узлов, \textit{глубина} - максимальная длина пути от входа до выхода.
\end{defn}

\begin{defn}
    \textit{Схема сравнения} - схема, которой узлы - \textit{элементы сравнения}. Сама схема выглядит как куча вертикальных полосок и перпендикулярных им отрезочков, которые соединяют некоторые пары длинных линий. Вертикальные линии - элементы сравнения, а отрезочки - сами операции сравнения.
\end{defn}

\begin{defn}
    \textit{Схема слияния} - схема сравнения, которая берёт на вход две отсортированные последовательности и на выходе даёт отсортированную последовательность из всех их элементов.
\end{defn}

\begin{defn}
    \textit{Схема сортировки} - схема сравнения, у которой на входе произвольная последовательность, а на выходе - отсортированная последовательность.
\end{defn}

\begin{remark}
    Семейство схем с конечным описанием - алгоритм сортировки. Их размер и глубина - последовательная и параллельная сложность алгоритма. 
\end{remark}

\subsection{Больше о сортировках и слияниях.}

\begin{theorem}
    \textbf{Принцип нулей-единиц.} Схема сравнения является сортирующей тогда и только тогда, когда она сортирует все последовательности нулей и единиц.
\end{theorem}

\begin{proof}
    В одну сторону очевидно, в обратную пойдём от противного, ничего сложного.
\end{proof}

\begin{remark}
    Между прочим, очень применимая хуйня. Ей можно проверять сеть сортировки путём проверки только $2^n$ входных последовательностей, а не $n!$. А также, сеть слияния путём проверки $(n'+1)(n''+1)$ вместо много.
\end{remark}

\begin{alg}
    \textbf{Чётно-нечётное слияние (OEM).} Если $n'=n''=1$, то сравниваем $(x_1, y_1)$, иначе рекурсивно

    \begin{itemize}
        \item слияние $\langle x_1, x_3, \ldots \rangle , \langle y_1, y_3, \ldots \rangle$; 
        \item слияние $\langle x_2, x_4, \ldots \rangle , \langle y_2, y_4, \ldots \rangle$;
        \item попарное сравнение: $(u_2, v_1), (u_3, v_2), \ldots$
    \end{itemize}

    Размер - $O(n\log n)$, глубина - $O(\log n)$. 
\end{alg}

\begin{proof}
    Доказательство корректности приводится посредством индукции.
\end{proof}

\begin{alg}
    \textbf{Сортировка нечётно-чётным слиянием.} Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item сортировка $\langle x_1, \ldots, x_{\lceil n/2 \rceil} \rangle$; 
        \item сортировка второй половины; 
        \item слияние результатов при помощи OEM.
    \end{itemize}

    Размер - $O(n(\log n)^2)$, глубина - $O((\log n)^2)$.
\end{alg}

\begin{defn}
    \textit{Битонная последовательность}: $\langle x_1 \geq \ldots \geq x_m \leq \ldots \leq x_n \rangle$, $1\leq m \leq n$.
\end{defn}

\begin{alg}
    \textbf{Битонное слияние (BM)}: сортировка битонной последовательности. Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item битонное слияние $\langle x_1, x_3, \ldots \rangle $ в упорядоченную последовательность; 
        \item аналогично по чётным; 
        \item попарное сравнение $(u_1, v_1), (u_2, v_2), \ldots$. 
    \end{itemize}

    Размер - $O(n \log n)$, глубина $O(\log n)$. 
\end{alg} \ 

\begin{alg}
    \textbf{Сортировка битонным слиянием.} Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item сортируем первую половину в обратном порядке; 
        \item сортируем вторую половину в прямом порядке; 
        \item битонное слияние.
    \end{itemize}

    Размер - $O(n(\log n)^2)$, глубина $O((\log n)^2)$.
\end{alg}

\begin{prob}
    Возможна ли неадаптивная сортировка схемой размера $o(n(\log n)^2)$? $O(n \log n)$?
\end{prob}

\begin{alg}
    \textbf{Схема AKS.} Размер - $O(n \log n)$, глубина - $O(\log n)$. Использует глубокие понятия теории графив (\textit{экспандеры}). Асимптотически оптимальна, но имеет огромный константный множитель.
\end{alg}

\subsection{Модели параллельных вычислений.}

\begin{alg}
    \textbf{Parallel Random Access Machine (PRAM).} Простая, идеализированная модель общих параллельных вычислений, включающая неограниченное количество \textit{процессоров} (1 операция за единицу времени), и глобальную общую память. Вычисления полностью синхронны. Само вычисление $PRAM$ - последовательность параллельных \textit{шагов}, коммуникауия и синхронизация считаются ''бесплантыми'', однако эта хрень очень трудно реализуется на практике. \ 

    Варианты PRAM: 

    \begin{itemize}
        \item concurrent/exclusive read;
        \item concurent/exclusive write.
    \end{itemize}
\end{alg}

\begin{alg}
    \textbf{Bulk-Synchronous Parallel (BSP) computer.} Простая, реалистичная модель общих параллельных вычислений - масштабируемая, переносимая, предсказуемая. Включает $p$ процессоров, каждый с \textit{локальной памятью} (1 операция за единицу времени), \textit{коммуникационную среду}, состоящую из сети и (возможно) \textit{внешней памяти} (1 единица данных за $g$ единиц времени), и маханизм \textit{барьерной синхронизации} (не чаще 1 раза за $l$ единиц времени). \ 

    Примеры коммуникационной среды:

    \begin{itemize}
        \item $g$ - \textit{communication gap} (inverse bandwidth), время (в худшем случае) для единицы данных войти в сеть или покинуть сеть; 
        \item $l$ - \textit{latency}, время (в худшем случае) для единицы данных быть переданной внутрь сети.
    \end{itemize}

    Вычисление BSP - последовательность параллельных \textit{супершагов}. Сначала происходят вычисления / коммуникация внутри шага (коммуникация включает обмен данными с внешней памятью), а затем происходит синхронизация между супершагами. \ 

    Альтернативная модель - CSP, взаимодействующие последовательные процессы.
\end{alg}

\begin{remark}
    Рассмотрим композиционную модель стоимости вычислений. Для конкретного процессора $proc$ на супершаге $sstep$:

    \begin{itemize}
        \item $comp(sstep, proc)$ - объём локальных вычислений и операций над локальной памятью процессора $proc$ на супершаге $sstep$; 
        \item $comm(sstep, proc)$ - объём данных, отправленных и полученных процессором $proc$ на супершаге $sstep$.
    \end{itemize}

    Для компьютера BSP в целом на одном супершаге $sstep$:

    \begin{itemize}
        \item $comp(sstep)=\max_{0\leq proc < p}comp(sstep, proc)$; 
        \item $comm(sstep)=\max_{0\leq proc <p}comm(sstep, proc)$; 
        \item $cost(sstep)=comp(sstep)+comm(sstep)\cdot g+l$.
    \end{itemize}

    Для вычисления BSP, состоящего из sync супершагов:

    \begin{itemize}
        \item $comp-\sum_{0\leq sstep <sync}comp(sstep)$; 
        \item $comm=\sum_{0\leq sstep < sync}comm(sstep)$; 
        \item $cost = comp+comm\cdot g+sync\cdot l$. 
    \end{itemize}

    Входные или выходные данные хранятся во внешней памяти, стоимость ввода / вывода включена в $comm$.
\end{remark}

\begin{remark}
    Что касается разработки алгоритмов для BSP. Нас интересует минимизация $comp$, $comm$, $sync$ как функций от $n$, $p$. \\
    
    Соглашения: 

    \begin{itemize}
        \item размер задачи $n \gg p$ (\textit{допуск}); 
        \item входные или выходные во внейшней памяти, ввод или вывод - односторонние коммуникации.
    \end{itemize}

    Баланс вычислений:

    \begin{itemize}
        \item \textit{work-optimal} $comp=O(\frac{seq \: work}{p})$.
    \end{itemize}

    Баланс коммуникации:

    \begin{itemize}
        \item цель \textit{scalable} $comm = O(\frac{iinput+output}{p^c})$, $o<c\leq 1$; 
        \item в идеале $fully-scalable$, $c=1$.
    \end{itemize}

    Крупоблочность:

    \begin{itemize}
        \item цель - $sync$, не зависящая от $n$ (может зависеть от $p$); 
        \item ещё лучше - \textit{quasi-flat} $sync=O((\log p)^{O(1)})$; 
        \item в идеале - \textit{flat} $sync=O(1)$. 
    \end{itemize}
\end{remark}

\subsection{Всякая хуйня.}

\begin{alg}
    Схема на основе \textit{сбалансированного двоичного дерева} - tree(n), 1 вход, $n$ выходов (или наоборот). Размер - $n-1$, глубина - $\log n$. Каждый узел вычисляет произвольно заданную операцию за $O(1)$. Последовательная сложность - $O(n)$, сложность на PRAM - $O(\log n)$. Строим равномерно распределяя одинаковые количества операций на процессоры ($n/p$). \ 
    
    Описанный на лекциях BSP-алгоритм вычисления сбалансированнного дерева полностью оптимален:

    \begin{itemize}
        \item оптимальное $comp=O(n/p)=O(\frac{sequential \: work}{p})$; 
        \item оптимальное $comm = O(n/p)=O(\frac{input/output \: size}{p})$; 
        \item оптимальное $sync=O(1)$. 
    \end{itemize}
\end{alg}

\begin{prob}
    Задача \textit{префиксного накомпления}. Пусть нам дан массив $a=[a_0, \ldots, a_{n-1}]$. Нам нужно вычислить $b_{-1}=0$, а точнее $b_i=a_i+b_{i-1}$ для $0\leq i < n$. В более общем виде нуль заменяется на какой-то элемент, а сложение - на любой ассоциативный оператор.
\end{prob}

\begin{alg}
    Схема \textit{префиксного накопления} - prefix($n$) - красивая вещь, которая позволяет справиться с поставленной задачей за размер $2n-2$, глубину $2 \log n$ и сложность на PRAM - $O(\log n)$. \ 

    Рассматривая префиксное накопление на BSP, граф prefix($n$) состоит Из

    \begin{itemize}
        \item верхнего подерева, вычисляемого от листьев к корню tree($n$); 
        \item переноса значений от узлов верхнего поддерева к узлам нижнего; 
        \item нижнего поддерева, вычисляемого от корня к листьям tree($n$). 
    \end{itemize}

    Оба поддерева можно вычислить предыдущим алгоритмом. Перенос значений задаёт $comm=O(n/p)$, $comp=O(n/p)$ и $sync=O(1)$, предполагая допуск $n\geq p^2$. 
\end{alg}

\begin{prob}
    Даны массивы $a=[a_0, \ldots, a_{n-1}]$, $b=[b_0, \ldots, b_{n-1}]$. Нам нудно вычислить $c_{-1}=0$, а ещё лучше $c_i=a_i+b_i\cdot c_{i-1}$ для $i\leq 0 < n$. Ещё это можно записать в виде произведений матриц, но это так, чуть более красивая форма. Пусть $A_i$ - матрица $2 \times 2$, в верхней строке которой 1, 0, а во второй - $a_i$, $b_i$. $C_i$ - матрица $2 \times 1$, сверху - 1, снизу - $c_i$. Тогда суть рассматриваются рекурренты $C_k=A_k\ldots A_1A_0\cdot C_{-1}$. 
\end{prob}

\begin{alg}
    Эта задача - суть предыдущая с префиксными накоплениями. С итоговым размером $O(n)$, глубиной $O(\log n)$. 
\end{alg}

\begin{prob}
    Побитово складываем массивы. И переносим, а потом ещё и параллелим.
\end{prob}

\begin{alg}
    Параллельное \textit{двиочное сложение} при помощи булевой логики. Если $x+y = z$, и каждый из них представлено соответственно как $[x_{n-1}, \ldots, x_0]$ (понятно, как другие). Тогда если $c - [c_{n-1}, \ldots, c_0]$, где $c_i$ - $i$-ый бит переноса, то $x_i+y_i + c_{i-1} = z_i + 2c_i$, для $0 \leq i < n$. \ 

    Определим тогда битовые массивы $u = [u_{n-1}, \ldots, u_0]$, $v = [v_{n-1}, \ldots, v_0]$, $u_i = x_i \wedge y_i$, $v_i = x_i \oplus y_i$, $0 \leq i \leq n$. Тогда $z_0 = v_0$, а далее $z_{k-1} = v_{k-1} \oplus c_{n-2}$, $c_0 = u_0$, а далее - $c_{n-1} = u_{n-1} \vee (v_{n-1} \wedge c_{n-2})$. Это получилась схема \textit{сумматора со сквозным переносом} размера и глубины $O(n)$. \ 

    Хотелось бы также её распареллелить. Для этого скажем, что $c_i = u_i \vee (v_i \wedge c_{i-1})$. Вычисляем массив $c$ при помощи линейного рекуррентного соотношения со входами $u, v$ и операторами $\vee, \wedge$: размер $O(n)$, глудина - $O(\log n)$; массив $z$ - дополнительный размер $O(n)$, глубина $O(1)$. Получилась схема \textit{сумматора с ускоренным переносом} размера $O(n)$, глубины $O(\log n)$. 
\end{alg}

\section{Оптимизационные задачи и приближённые алгоритмы.}

\begin{prob}
    Задача \textit{линейного программирования (LP)}: оптимизировать (максимизировать или минимизировать) многомерную вещественную линейную \textit{целевую функцию} при линейных \textit{ограничениях} (неравенствах или равенствах). \ 

    Вектор $x = (x_1, \ldots, x_n) \in \RR^n$ - \textit{допустимое решение} задачи LP, если оно удовлетворяет всем ограничениям. Задача LP разрешима, если у неё есть допустимое решение. Допустимое решение \textit{оптимально}, если целевая функция достигает на нём (нестрогого) максимума (соответственно, минимума). \ 

    \textit{Стандартная форма} задачи LP состоит из максимизации выражения $\sum c_i x_i$ при $m$ условиях $\sum_j a_{ij}x_j \leq b_i$, и $x_1, x_2, \ldots, x_n \geq 0$. $n$ переменных, $m+n$ лграничений, включая $n$ ограничений неотрицательности (будем их опускать, предполагая по умолчанию). \textit{Каноническая форма} задачи $LP$ - то же самое, что и стандартная, только в куче тех рассматриваемых сумм у нас не знаки неравенства, а значи равенства. Чтобы привести задачу от стандартной формы к канонической вводятся вспомогательные переменные $s_i = x_{n+i}$. 
\end{prob}

\begin{theorem}
    (Основная теорема линейного программирования). Задача $LP$ в стандартной форме имеет оптимальное решение, если она разрешима и ограничена. Если задача LP имеет оптимаьное решение, то хотя бы одно из них - вершина допустимого многогранника.
\end{theorem} \

\begin{alg}
    (Симплекс-метод). Последовательные перемещения по вершинам допустимого могогранника, улучшающие целевую функцию. Общая идея симплекс-метода состоит в том, что мы берём одну из вершин в качестве начального решения, и если решение не оптимально, но оптимальное решение существует, то в какой-то из соседних вершин значение целевой функции лучше, чем в текущей - перемещаемся в эту вершину и повторяем. 
\end{alg}

\begin{defn}
    Задачи называются \textit{двойственными}, если поменять местами $b_i$ и $c_j$, и перевернуть неравенства в условиях. И если первичная задача разрешима, то и двойственная также разрешима и их оптимальные решения равны.
\end{defn}

Всякие схемы про три задачи из билетов начинаются примерно на сотом слйде мистера Тискина, а поэтому я добавлю только некоторые важные определения и формулировки.

\begin{defn}
    \textit{Сохранение потока}: для любого $u \in V \backslash \{s, t\}$ суммарный входящий поток в $u$ должен равняться суммарному исходящему потоку из $u$. 
\end{defn}

\begin{theorem}
    Следующие утверждения равносильны:

    \begin{itemize}
        \item $f$ - максимальный поток; 
        \item $f$ не имеет увеличивающихся путей; 
        \item существует $(s, t)$-разрез со значением $c(S, T) = |f|$. 
    \end{itemize}
\end{theorem}

\begin{alg}
    (Алгоритм Форда-Фалкерсона). Начинаем с произвоьлного допустимого потока (например, нулевого), затем

    \begin{itemize}
        \item строим для текущего потока остаточнкую сеть;
        \item ищем увеличивающий путь в остаточной сети. если его нет, то поток максимальный; 
        \item включаем увеличивающий путь в поток, добавляя его значение к прямым рёбрам и вычитая из обратных; 
        \item повторяем для нового потока.
    \end{itemize}
\end{alg}

\begin{theorem}
    В целочисленной задаче о максимальном потоке значение оптимального потока $|f^*|$ - натуральное число. Алгоритм Форда-Фалкерсона находит его за $|f^*|$ итераций. Общая трудоёмкость алгоритма - $|E||f^*|$. 
\end{theorem}

\newpage

\section{Edward Hirsh}

\subsection{Проверки равенств}

\begin{alg}
(Алгоритм Фрейвальдса). Если нам хочется проверить верно ли перемножены две матрицы ($A\times B = C$?) размера $n \times n$, то можно выполнить следующий алгоритм: \ 

Сгенерируем случайный вектор $r$ из нулей и единиц размера $n \times 1$. Вычисляем $P = A \times (B r) - Cr$, и если получился нулевой столбик, то алгоритм скажет, TRUE, иначе - FALSE. \ 

Если произведение выполнено, то алгоритм всегда выведет TRUE, но если произведение не выполнено, то вероятность того, что алгоритм выдаст TRUE - не больше $\frac{1}{2}$. Таким образом, если выполнить его $k$ раз, и на каждой итерации получать TRUE, то полученный алгоритм будет работать со скоростью $O(kn^2)$ и вероятность ошибки будет составлять не больше $2^{-k}$.
\end{alg} 
\begin{proof}
Рассмотрим матрицу $D=AB-C$. Если она ненулевая, то есть в ней ненулевой элемент $D_{i^*, j^*}$. Тогда у вектора $(AB-C)r$ в $i^*$-ой строчке написано число $\sum_{j \neq j^*} D_{i^*, j}r_{j} + D_{i^*, j^*}r_{j^*}$. Поскольку вектор $r$ случайный, то мы можем считать, что элемент $r_{j^*}$ был выбран последним. Тогда существует не больше одного значения $r_{j^*}$, при котором написанное выше выражение занулится. Значит, и вероятность, что весь вектор $(AB-C)r$ занулится, не больше $\frac{1}{2}$ (так как в поле, над которым мы умножаем матрицы, хотя бы два элемента).
\end{proof}

\begin{defn}
Пусть есть язык $L \subseteq \{0, 1\}^*$. $A$ - алгоритм с \textit{односторонней вероятностью ошибки}, если для любого $x \in \{0, 1\}^*$:
\begin{itemize}
\item $x \notin L \implies A(x)=0$
\item $x \in L \implies \mathbb{P}(A(x) \neq 1) <\frac{1}{2}$.
\end{itemize}
Такие алгоритмы иногда называются \textit{алгоритмами типа Монте-Карло}
\end{defn}
\begin{remark}
Константа $\frac{1}{2}$ не очень важна. Главное, чтобы она была $<1$. Тогда мы с помощью константного количества запусков алгоритма можем уменьшить её до нужного значения.
\end{remark}

\begin{lemma}
(Лемма Шварца-Циппеля) Пусть $P \in F[x_1, x_2, \ldots, x_n]$ - ненулевой сногочлен степени $d \geq 0$ над полем $F$. Пусть $S$ - конечное подмножество $F$, и пусть элементы $r_1, r_2, \ldots, r_n$ были выбраны из $S$ равномерно и независимо друг от друга. Тогда $\mathbb{P}[P(r_1, \ldots, r_n) = 0] \leq \frac{d}{|S|}$.
\end{lemma} 
\begin{proof}
Индукция по числу переменных. База $n=1$ очевидна (у многочлена степени $d$ не больше, чем $d$ корней). Чтобы доказать для $n$ переменных, рассмотрим наш многочлен как многочлен от одной из своих переменных: $P=P(x_1, ... x_n)=x_1^kP_k(x_2, ... x_n)+x_1^{k-1}P_{k-1}(x_2, ... x_n)+...$. Пусть $j=\max_{P_{i} \not \equiv 0}(i)$. По формуле полной вероятности получаем $\mathbb{P}(P(r_1, ... r_n)=0)=\mathbb{P}(P=0 | P_j=0)\mathbb{P}(P_j=0)+\mathbb{P}(P=0 | P_j \neq 0)\mathbb{P}(P_j \neq 0)$. $P_j$ - многочлен от $x_2, ... x_n$, и его степень $\deg P_j \leq \deg P - j$, а тогда по предположению индукции $\mathbb{P}(P=0 | P_j=0)\mathbb{P}(P_j=0) \leq 1 \cdot \frac{\deg P - j}{|S|}$. Зафиксируем теперь такой набор $r_2, ... r_n \in S$, что $P_j(r_2, ... r_n) \neq 0$. Тогда $P(x_1, r_2, ... r_n)$ - многочлен одной переменной, и $\mathbb{P}(P=0 | P_j \neq 0)\mathbb{P}(P_j \neq 0) \leq \frac{j}{|S|} \cdot 1$. Суммируя, получаем требуемый переход индукции. 
\end{proof}

\begin{alg} 
Пусть у нас есть как-то сложно записанный многочлен $P$, и нам хочется проверить, является ли он тождественным нулём. Для этого достаточно выбрать какое-нибудь подмножество в $F$ размера $> \deg P$ и несколько раз применить лемму Шварца-Циппеля.
\end{alg}

\begin{defn}
Может быть так, что алгоритм всегда выдаёт правильный ответ, но за случайное время. Такие алгоритмы называются \textit{алгоритмами типа Лас-Вегас}
\end{defn}
\begin{exl}
Быстрая сортировка.
\end{exl}

\begin{prob}
Интересно нам сравнить две строки $a, b$ (которые можно считать побитовыми), затратив как можно меньше информации на это сравнение. Основная идея - сравнивать не сами строки, а функции от них. 
\end{prob}

\begin{defn}
\textit{Расстояние} между строками $a, b$ будем считать количество несовпадающих у них
 
битов.
\end{defn}

\begin{defn}
Под \textit{editing distance} между $a, b$ будем понимать минимальное количество операций редактирования, необходимых для преобразования строки $a$ в строку $b$ (длин $m\leq n$ соответственно). Операциями редактирования мы считаем: вставку, удаление или уничтожение бита, а также перемещение сплошного блока битов.
\end{defn}

\begin{alg}
(Алгоритм Рабина-Карпа). Определим $b(i)=b_ib_{i+1} .. b_{i+m-1}$. Необходимо провести $n-m+1$ сравнений строк на равенсто, а это мы уже умеем делать: будем сравнивать $(a \mod_p)$ и $(b(i) \mod_p)$. Но можно упростить задачу, вычисляя $(b(i) \mod_p)$ через $(b(i-1) \mod_p)$. 

\bea
b(i)=b_i + 2b_{i+1} + .. + 2^{m-1}b_{i+m-1} \\
b(i-1)=2b_i + 2^2b_{i+1}+ .. +2^{m-1}b_{i+m-2}+b_{i-1} \\
\Rightarrow b(i)=\frac{b(i-1)-b_{i-1}}{2}+2^{m-1}b_{i+m-1} \\
\eea

Опять будем брать простое число $p \in [2, r]$, где $r = n^2m\log n^2 m$. Тогда

\be
P_{\text{ошибки}}\leq
\frac{m}{\frac{\tau}{\log \tau}} \leq
\frac{2m\log n^2m}{n^2m \log n^2m} =
O\biggl(\frac{1}{n^2}\biggr)
\ee

Можно вообще избавить этот алгоритм от необходимости ошибаться. Если $a\equiv_p b(i)$, то просто проверим равенство $a=b$. Этот алгоритм уже не ошибается. \ 

Математическое же ошидание его работы посчитать легко:

\be
{\bf E}T(n,m) \leq mn*\frac{1}{n}+(m+n)(1-\frac{1}{n})=
O(m+n).
\ee

\end{alg}

\subsection{Проверка простоты.} 

\begin{theorem}
(Критерий Эйлера). Если $n$ - нечётное простое, то $\biggl( \frac{a}{n} \biggr) a^{\frac{n-1}{2}} \equiv 1 \mod n$, для всех $a \in \{1, \ldots, n-1\}$. 
\end{theorem} \ 

\begin{alg}
(Алгоритм Соловея-Штрассена). Получив на входе число $n$, случайно выбирается $\in \{1, \ldots, n-1\}$ и проверить условие критерия Эйлера - то есть, вычислить $\biggl( \frac{a}{n} \biggr) a^{\frac{n-1}{2}} \mod_n$, и если получилось 1, то сообщить, что $n$ - простое.
\end{alg}
\begin{proof}
\begin{lemma}
Если $n \notin \mathbb{P}$, то существует $a$, для которого $\gcd(n, a)=1$, но $\biggl( \frac{a}{n} \biggr) \not \equiv a^{\frac{n-1}{2}} \mod n$
\end{lemma}
\begin{proof}
Возможны два случая:
\begin{itemize}
\item $N=p_1\cdot ... \cdot p_k$, где все $p_i$ различны. Существует $r$ такое, что $\biggl( \frac{r}{p_1}\biggr)=-1$. По китайской теореме об остатках найдётся такое $a$, что $a \equiv r \mod p_1$, $a \equiv 1 \mod p_i$, $i>1$. По определению символа Якоби получаем, что $\biggl( \frac{a}{n} \biggr)=\biggl( \frac{a}{p_1} \biggr) \cdot \biggl( \frac{a}{p_2} \biggr) \cdot ... \biggl( \frac{a}{p_k} \biggr)=(-1) \cdot 1 \cdot ... \cdot 1=-1$, что и требовалось.
\item $n=p^tn'$, где $t>1$, $\gcd(p, n')=1$. Тогда существует первообразный корень $r'$ по модулю $p^t$. По китайской теореме об остатках выберем $r \equiv r' \mod p^t$, $r \equiv 1 \mod n'$, тогда $\gcd(r, n)=1$. Если утверждение леммы неверно, то $r^{n-1} \equiv (r^{\frac{n-1}{2}})^2 \equiv \biggl ( \frac{r}{n} \biggr)^2 \equiv 1 \mod n$ и получаем, что $p | p^{t-1}(p-1) | N-1$. Но в то же время $p | N$, противоречие.
\end{itemize}
\end{proof}
\begin{lemma}
Если $n \notin \mathbb{P}$, то для более чем половины всех $a$, взаимно простых с $n$, выполняется $\biggl( \frac{a}{n} \biggr) \not \equiv a^{\frac{n-1}{2}} \mod n$
\end{lemma}
\begin{proof}
По предыдущей лемме существует какое-то $a$, для которого $\biggl( \frac{a}{n} \biggr) \not \equiv a^{\frac{n-1}{2}} \mod n$. Пусть $b_1, ... b_k$ - все взаимно простые с $n$ остатки, для которых равенство всё-таки выполнено. Рассмотрим тогда остатки $ab_1, ... ab_k$. Во-первых, они все различны, так как $a$ - обратим в $\mathbb{Z}_n$. Во-вторых, для всех них не выполняется нужное сравнение: $\biggl (\frac{ab_i}{n} \biggr) = \biggl( \frac{a}{n} \biggr)\biggl( \frac{b_i}{n} \biggr)=\biggl( \frac{a}{n} \biggr) \cdot b_i^{\frac{n-1}{2}} \not \equiv (ab_i)^{\frac{n-1}{2}} \mod n$, так как $b_i \in \mathbb{Z}_{n}^{\times}$. Значит, остатков, для которых искомое сравнение не выполнено, не меньше, чем тех6 для которых выполнено. 
\end{proof}
Перейдём к доказательству алгоритма Соловея-Штрассена. По предыдущей лемме, алгоритм ошибается с вероятностью
 
$\leq \frac{1}{2}$. Более того, он полиномиальный от числа битов в $n$, так как символ Якоби считается по сути так же, как НОД двух чисел алгоритмом Евклида, в котором используется деление по модулю, а это всё - модулярная арифметика, т.е. полиномиальная от длины чисел.
\end{proof}
\subsection{3-SAT}
Пусть у нас есть \textit{3-КНФ}, т.е. конъюнктивная нормальная форма, в каждой скобке которой находятся не более трёх переменных.
\begin{exl}
$(x \vee y \vee \neg z) \wedge (\neg t \vee \neg y \vee u) \wedge ...$
\end{exl}.
Хотим понять, можно ли присвоить участвующим в ней булевским переменным значения $0$ или $1$ так, чтобы форма стала истинной. Понятное дело, можно устроить полный перебор за $O(2^n)$, где $n$ - число переменных, но хочется быстрее.
\\
\textbf{Первый способ:} Представим перебор в виде дерева и начнём перебирать значения переменных по порядку, начиная с какого-то конъюнкта. НУО, пусть он выглядит так: $(x \vee y \vee z)$. Заметим тогда, что нам нет смысла перебирать поддерево, соответствующее случаю $x=y=z=0$, так как в нём КНФ точно будет равна $0$. Поэтому если обозначить через $v_i$ количество перебираемых вариантов для 3-КНФ с $i$ переменными, то получается цепочка неравенств $v_n \leq 7v_{n-3} \leq 7^2v_{n-3 \cdot 2} \leq ... \leq 7^{\lfloor \frac{n}{3} \rfloor +1}v_0$, получается асимптотика $O(7^{\frac{n}{3}}) \leq O(1.92^n) « 2^n$. 
\\
\\
Но на самом деле можно ещё быстрее. Снова будем перебирать, начиная с переменных какого-то конъюнкта, НУО это $(x \vee y \vee z)$. Тогда достаточно пойти в поддеревья, соответствющие случаям $(x=0, y=0, z=1)$, $(x=0, y=1)$ и $x=1$. Получаем неравенство $v_n \leq v_{n-1}+v_{n-2}+v_{n-3}$. По индукции легко доказать, что верна оценка $v_n \leq c^n$, где $c$ - единственный положительный корень уравнения $1-x^{-1}-x^{-2}-x^{-3}=0$. Более того, $c \approx 1.84$
\begin{alg}
(2-SAT) Хотим решить ту же задачу, только для 2-КНФ. Для этого представим каждый конъюнкт в виде импликации (например, $x \vee y$ - в виде $\neg x \ra y$) и построим ориентированный граф с вершинами в $x_i$ и $ \neg x_i$, в котором проведём ориентированные рёбра в соответствии с конюънктами. Более того, если конъюнкту соответствует импликация $a \ra b$, то помимо ребра из $a$ в $b$, проведём также ребро из $\neg b$ в $\neg a$ (здесь и далее мы будем отождествлять $x$ и $\neg \neg x$). Если вдруг оказалось, что для какой-то переменной $x$ вершины $x$ и $\neg x$ лежат в одной компоненте сильной связности, то задача $2-SAT$ неразрешима, В противном же случае применим к компонентам сильной связности топологическую сортировку, и если из $x$ достижима $\neg x$, то положим $x=0$, иначе - $1$. 
\end{alg}
\begin{proof}
Если для какой-то переменной $x$ вершины $x$ и $\neg x$ лежат в одной компоненте сильной связности, то задача $2-SAT$ неразрешима, потому что должны быть выполнены импликации $x \ra \neg x$ и $\neg x \ra x$. 
\\
Если такой переменной не нашлось, то докажем, что выбранные значения переменных действительно дают решение. Предположим противное: какой-то конъюнкт ложен, т.е. в графе есть импликация из $x=1$ в $y=0$. Но по построению можно добраться из $\neg x$ в $x$, из $y$ в $\neg y$, а также есть ребро из $\neg y$ в $\neg x$. Значит, все эти четыре вершины лежат в одной компоненте сильной связности, противоречие с тем, что $x$ и $\neg x$ лежат в разных.
\end{proof}

\begin{alg}
Пусть у нас $n$ переменных. Зафиксируем какое-то решение задачи (если оно существует). Случайно выберем значения переменным, а затем несколько раз применим следующую операцию: выберем конъюнкцию, которая ложна, и изменим значение любой из переменных. Заметим, что с вероятностью $\geq \frac{1}{2}$ мы "попадём в цель" (для простоты будем считать, что ровно $\frac{1}{2}$). Посчитаем матожидание количества таких операций, необходимых для того, чтобы получить зафиксированное решение. Определим расстояние по Хэммингу двух наборов значений, т.е. просто количество переменных, значение которых отличается от "эталонного". Пусть $x_k$ - ожидаемое количество операций, чтобы от расстояния $k$ перейти к расстоянию $k-1$. Тогда $x_n=1$,
 
$x_k=1+\frac{1}{2}(x_k+x_{k+1})+\frac{1}{2} \cdot 0$, откуда $x_k=2+x_{k+1}$, и $x_1+...+x_n=n^2$. Значит, матожидание кол-ва ходов, необходимых чтобы добраться до "эталонного" решения, не больше $n^2$.
\end{alg}
\begin{alg}
(аналогичный алгоритм для 3-SAT) Сделаем аналогичное случайное блуждание. Для этого случайно выберем значения переменных и организуем аналогичный процесс. В этот раз мы попытаемся "высадиться" в точке $\frac{n}{3}$ - вероятность этого равна ${ n \choose \frac{n}{3}} \approx (\frac{3}{2^{\frac{5}{3}}})^n$ - и затем сделаем ровно $n$ шагов. Какова вероятность, что мы после ровно $t=n$ шагов впервые придём в точку 0, впервые находясь в точке $s=\frac{n}{3}$? Представим путь в виде ломаной линии на координатной плоскости из точки $(0, 0)$ в точку $(t, s)$ - по оси абсцисс мы откладываем количество шагов, а по оси ординат - наше такущее положение. Пусть $p$ - число ходов "влево", а $q$ - вправо, тогда $t=p+q, s=p-q$. Всего путей ${p+q-1 \choose p-1}$ (можно считать, что мы идём в точку $(1, 1)$, так как последний ход определён однозначно), но некоторые из них пересекают ось абсцисс раньше. Отразим участок пути от конца (точки $(1, 1)$) до последнего (ближайшего к концу) момента пересечения с осью абсцисс относительно неё же. Получим биективное соответствие между путями, которые нам не подходят, и всеми путями из точки $(1, -1)$ в точку $(t, s)$ - их ${p+q-1 \choose p}$. В итоге, подходящих нам путей ${p+q-1 \choose p-1} - {p+q-1 \choose p}=\frac{p-q}{p+q} {p+q \choose p}$, и итоговая вероятность добраться равна (с точностью до полиномиального множителя) ${p+q \choose p} (\frac{1}{3})^p(\frac{2}{3})^q$. Подставляя наши значения и применяя формулу Стирлинга, получаем вероятность (опять-таки, с точностью до полиномиальных множителей) $\approx \frac{1}{2^{\frac{n}{3}}}$. Значит, вероятность того, что мы "высадимся" именно в $\frac{n}{3}$, а потом ещё и попадём куда надо, равна $(\frac{3}{2^{\frac{5}{3}}})^n \cdot \frac{1}{2^{\frac{n}{3}}} = (\frac{3}{4})^n$. Применим теперь эту процедуру $l=(\frac{4}{3})^n$ раз. Вероятность того, что мы ни разу не попадём в верный набор значений, равен $(1-\frac{1}{l})^l \ra \frac{1}{e} < \frac{1}{2}$.
\end{alg}
\subsection{Хэш-таблицы} 
\begin{defn}
Что такое хэш-таблица? У нас есть массив размера $m$, в каждой вершине которого хранится список (изначально пустой). Также у нас есть что-то, проиндексированное ключами от $0$ до $n-1$ ($n>m$), и мы хотим это что-то добавлять в нашу структуру данных, находить или удалять. Как происходит добавление: мы считаем хэш-функцию $h(i)$ от ключа $i$ и идём в ячейку с этим номером. Возможно, там уже есть элементы, в таком случае нам надо дойти до конца списка и записать новый элемент туда. Аналогично происходит поиск - обращаемся в нужную ячейку и идём, пока не встретим наш элемент, и удаление (как в поиске, но потом ещё и удаляем элемент из списка за $O(1)$). Легко видеть, что все операции выполняются за $O(l)$ , где $l$ - максимальный размер всех списков. Поэтому нам бы хотелось иметь такую хэш-фукнцию, которая образует не слишком длинные списки.
\end{defn}
\begin{defn}
(Универсальное семейство хэш-фукнций)
Пусть $n>m$. $H \subseteq \{h: 0, ... n-1 \ra 0, 1 ... m-1\}$, и для любых $x, y < n$ $\mathbb{P}_{h \in H}(h(x)=h(y)) \leq \frac{1}{m}$.
\end{defn}
\begin{exl}
(Пример универсального семейства хэш-функций) Выберем $p \in \mathbb{P}, p>n$. $H=\{h_{a, b} | 0<a<p, 0 \leq b < 0\}$, где $h_{a, b}(x)=((ax+b) \mod p) \mod m$ . Но почему это будет универсальным семейством?
\end{exl}
\begin{proof}
Для начала заметим, что если $ax+b \equiv ay+b \mod p$, то $x \equiv y \mod p$. Также отметим, что если вдруг $ax+b \equiv r \mod p$, $ay+b \equiv s \mod p$, и мы знаем числа $x, y, r, s$ ($x \neq y$, $r \neq s$), то мы можем однозначно восстановить $a, b$. Получается, пары $(r, s)$ и $(a, b)$ находятся во взаимно однозначном соответствии. Это значит, что если мы случайно и равновероятно выберем числа $a, b$, то $r$ и $s$ тоже будут распределены равновероятно. Т.е. мы можем считать, что равновероятно выбераем два различных числа от $0$ до $p-1$, а
 
потом берём их по модулю $m$. Так как $m<p$, то они могут "склеиться". В каждом классе "склеивающихя" остатков не больше $\lceil \frac{p}{m} \rceil $. Значит, если мы выбрали, например, $b$, то вероятность, что $a$ попадёт в то тже класс, не больше $\frac{\lceil \frac{p}{m} \rceil -1}{p-1} \leq \frac{\frac{p-1}{m}}{p-1}=\frac{1}{m}$, что и требовалось.
\end{proof}
Пусть у нас есть универсальное семейство хэш-функций $H$. Также пусть у нас есть какая-то ячейка с номером $i$, где уже лежит какой-то ключ $k^*$. Пусть у нас есть элементы с ключами $k_1, ... k_N$, и мы хотим посчитать матожидание длины списка в $i$-ой ячейке массива. Поскольку мы случайным образом выбираем хэш-функцию из универсального семейства (и потом работаем только с ней), то $\mathbb{E}(\text{длина списка})=\mathbb{P}(h(k_1)=h(k^*))+...+\mathbb{P}(h(k_N)=h(k^*)) \leq \frac{1}{m}+...=\frac{1}{m}=\frac{N}{m}$.
\\
\textbf{Совершенное хэширование}
\\

\subsection{Минимальное остовное дерево.} 

Остовное дерево —- это дерево, состоящее из некоторых ребер графа и
содержащее все его вершины. Требуется построить остовное дерево с
минимальным суммарным весом ребер. (Считаем, что все ребра разного
веса: этого можно добиться, если к равным ребрам добавить
достаточно малые $\varepsilon_i$.)

Сперва рассмотрим простой детерминированный алгоритм:
{\bf алгоритм Bor{\r u}vka}.
Выберем у каждой вершины ребро наименьшего веса. После этого в
полученном графе каждую компоненту связности стянем в вершину. При
каждой такой итерации число вершин уменьшается вдвое. Так что
сложность этого алгоритма $O(m\log{n})$. Фактически, этот алгоритм
мало чем отличается от алгоритма Краскала, но его можно улучшить
следующим образом: \\

\begin{alg}
По графу $G_1$ с $n$ вершинами и $m$ ребрами
строим минимальный остовный лес (ибо граф может быть
несвязен) для него.

\begin{enumerate}
\item Применим к $G_1$ 3 шага алгоритма Bor{\r u}vka - получим граф $G_2$ и
некое множество ребер S, которые принадлежат остовному дереву. В
графе $G_2$ максимум $\frac{n}{8}$ вершин.

\item Из $G_2$ выкинем примерно половину ребер (именно, каждое ребро
выкинем с вероятностью $\frac{1}{2}$) —- это будет граф
$G_2(\frac{1}{2})$ (в нем $\le\frac{n}{8}$ вершин,
а мат. ожидание количества ребер $\le\frac{m}{2}$.

\item Для $G_2(\frac{1}{2})$ применяем алгоритм рекурсивно; получим $F$ —-
минимальный остовный лес для этого графа.

\begin{center}
\small
Далее требуется определение. Пусть в графе $H$ есть остовное
дерево (или лес) $T$. Ребро, соединяющее вершины $v$ и $w$
называется {\bf тяжелым в } $H$ {\bf относительно } $T$, если ее
вес больше веса максимального ребра на пути из $v$ в $w$ в $T$.
Иначе ребро называется {\bf легким}. Понятно, что тяжелые ребра
нас не интересуют.

\begin{prob}
Найти все легкие ребра за линейное время.
\end{prob}
\end{center}

\item Пусть $V_2$ —- множество ребер $G_2$, легких относительно $F$.
Возьмем только их —- получим граф $G_3$.

\item Рекурсивно обрабатываем $G_3$ и выдаем полученный результат.

\end{enumerate}
\hfill$\qed$
\end{alg}

Пусть $T(n,m)$ —- мат. ожидание времени работы нашего алгоритма
на графе с $n$ вершинами и $m$ ребрами (более строго, максимум
этого мат. ожидания по всем графам). На рекурсивный вызов от
$G_2(\frac{1}{2})$ тратится время $T(\frac{n}{8},\frac{m}{2})$.
Покажем, что на рекурсивный вызов от $G_3$ тратится время
$T(\frac{n}{8},\frac{n}{4})$. \\ 

\begin{lemma}
Пусть $G(p)$ —- граф, полученный из какого-то графа $G$
выкидыванием каждого ребра с вероятностью $1-p$, и $F$ —-
минимальный остовный лес в $G(p)$. Тогда мат. ожидание числа
легких ребер в $G$ относительно $F$ не превосходит $\frac{n}{p}$,
где $n$ —- число вершин.
\end{lemma}
\begin{proof}
Лес $F$ будем строить одновременно с графом $G(p)$. Упорядочим ребра
$G$ по возрастанию весов. Очередное ребро с вероятностью $p$
добавляем в $G(p)$. Если добавленное ребро соединяет в $F$ разные
компоненты связности, то добавляем его в $F$ (как в алгоритме
Краскала). Таким образом, в $F$ попадет $n-1$ ребро. Легкими
в G будут эти $n-1\leq n$, а также те, которые попали бы в $F$, если
бы их случайно
 
не отбросили. Найдём матожидание количества попыток, требуемых, чтобы добавить очередное ребро в остовный лес. Для этого посмотрим на последовательность лёгких рёбер, упорядоченных по возрастанию. С вероятностью $p$ мы сразу взяли лёгкое ребро в остовный лес, с вероятностью $qp$ ($q=1-p$) - со второй попытки и так далее. С $k$-ой попытки мы возьмём с вероятностью $q^{k-1}p$. Это геометрическое распределение, и его матожидание равно $\frac{1}{p}$. Поскольку в итоге мы берём в остовный лес не более $n-1<n$ рёбер, то матожидание числа легких ребер не превосходит $\frac{n}{p}$.
\end{proof}

\begin{lemma}
Мат. ожидание количества ребер в графе $G_2$, легких относительно
леса $F$, не превосходит $n/4$.
\end{lemma}
\begin{proof}
В этом графе $\frac{n}{8}$ вершин, а $p=\frac{1}{2}$. Применяем
предыдущую лемму.
\end{proof}
Таким образом, для мат. ожидания времени работы нашего алгоритма,
очевидно, верно соотношение: $$ T(n,m)\leq
T\left(\frac{n}{8},\frac{m}{2}\right)+
T\left(\frac{n}{8},\frac{n}{4}\right)+c(m+n). $$

\begin{stat}
Доказать, что $T(n,m)=O(n+m)$.
\end{stat}

\subsection{Предонлайн хрень.}

\textbf{Оценки Чернова.} \\

Основной случай оценки Чернова для случайной величины $X$ достигается применением неравенства Маркова к $e^{tX}$. Для каждого $t > 0$ 

$$P(X \geq a) = P(e^{t\cdot X} \geq e^{t\cdot a}) \leq \frac{\mathrm{E}\left [e^{t\cdot X}\right]}{e^{t\cdot a}}.$$

Когда $X$ является суммой $n$ случайных величин $X_1, \ldots, X_n$, для любого $t > 0$

$$P(X \geq a) \leq e^{-ta}\mathrm{E} \left [\prod_i e^{t\cdot X_i} \right].$$

В частности, оптимизируя по $t$ и предполагая, что $X_i$ независимы, мы получаем

$$ P(X \geq a) \leq \min_{t>0} e^{-ta} \prod_i \mathrm{E} \left [e^{tX_i} \right ].$$ 

Аналогично

$$P(X \leq a) = P\left (e^{-tX} \ge e^{-ta}\right) $$

и, таким образом,

$$ P(X \leq a) \leq \min_{t>0} e^{ta} \prod_i \mathrm{E} \left[e^{-t X_i} \right ].$$

Конкретные значения оценок Чернова получаются вычислением $\mathrm{E} \left[e^{-t\cdot X_i} \right ]$ для конкретных величин $X_i$.
\\ 
\begin{theorem}
Пусть случайные величины $\{X_i\}$ —- бернуллиевские с вероятностями $\{p_i\}$, и $M_n=\mathbb{E}\sum_{i=1}^n X_i$. Тогда:
\begin{itemize}
\item $\mathbb{P}(\sum_{i=1}^n X_i \geq (1+\delta)M_n) \leq \big ( \frac{e^{\delta}}{(1+\delta)^{1+\delta}}\big )^{M_n} \leq 2^{-M_n(1+\delta)}$, где первая оценка выполнена для любого $\delta > 0$, а вторая - при достаточно больших $\delta$ (например, при $\delta > 5$)
\item $\mathbb{P}(\sum_{i=1}^n X_i \leq (1-\delta)M_n) \leq \big ( \frac{e^{\delta}}{(1-\delta)^{1-\delta}}\big )^{M_n} \leq e^{-\frac{M_n\delta^2}{2}}$ для любого $0 \leq \delta < 1$
\end{itemize}
\end{theorem}
\begin{proof}
В билетах этого не требуют, поэтому подробно расписывать я это не буду, но надо воспользоваться вышеизложенными рассуждениями и тем, что $\mathbb{E}(e^{tX_i})=(1-p)\cdot e^0+p \cdot e^t=1+p(e^t-1) \leq e^{p(e^t-1)}$, а дальше в первом случае выбрать $t=\log(1+\delta)$, а во втором - $t=-\log(1-\delta)$ (мог ошибиться, но эти числа находятся из нулей производной функции по $t$).
\end{proof}

\textbf{Уменьшение вероятности двусторонней ошибки}.
\begin{defn}
Пусть есть язык $L$, и алгоритм $A$ ($range(A)=\{0, 1\}$) такой, что:
\begin{itemize}
\item если $x \in L$, то $\mathbb{P}(A(x)=1) \geq \frac{3}{4}$
\item если $x \notin L$, то $\mathbb{P}(A(x)=0) \geq \frac{3}{4}$
\end{itemize}
Тогда такой алгоритм называется \textit{алгоритмом с двусторонней ограниченной вероятностью ошибки}
\end{defn}
Пусть у нас есть алгоритм $A$ с двусторонней ограниченной вероятностью ошибки, и мы хотим узнать правильный ответ для входа $x$. Запустим алгоритм $A$ несколько раз (пусть $m$). Он несколько раз выдал $1$ и несколько раз $0$. Выберем более частый ответ. Какова вероятность, что мы ошибёмся? Матожидание количества правильных ответов —- $\geq \frac{3}{4}$, поэтому по неравенству Чернова вероятность $\mathbb{P}(\text{кол-во правильных ответов < 1/2}) \leq \mathbb{P}(\text{кол-во правильных ответов < (1-1/3) (матожидание кол-ва правильных ответов)}) \leq e^{Cn}$, где $C$ - некоторая константа.
 
Получили экспоненциально быстрое убывание вероятности ошибки.

\subsection{On-line shit.}

\textbf{Задача кэширования (paging problem.)} 
\\ 

Рассмотрим следующую задачу. 
Есть два вида памяти - дисковая, большая по объему, но медленная, 
и кэш, быстрая, но очень небольшая. Вся память разбита на блоки. 
Пусть в кэш-памяти k блоков, а на диске, соответственно, намного больше. 
К нам поступают запросы на те или иные блоки, 
и мы должны сразу же их обработать. 
Есть два варианта: либо этот блок уже есть в кэше, либо его там нет. 
Если он в кэше есть, то в этом случае мы почти не тратим время. 
И тогда время работы алгоритма можно измерять количеством обращений 
к винчестеру. 

Алгоритмы отличаются друг от друга реакцией на запросы отсутствующих 
в кэше блоков. Понятно, что мы должны запрашиваемый блок подгрузить в кэш, 
непонятно только, какой из уже присутствующих там блоков мы должны для этого 
выгрузить. Конечно, хотелось бы выгрузить ``ненужный'' блок, но поскольку мы 
обрабатываем запросы в реальном времени, мы не знаем, какой блок нам 
понадобится в дальнейшем. 

Встает и другой вопрос: как сравнить, какой алгоритм лучше.

\newcommand{\cost}{\textit{cost}}
Путь $A$ --- алгоритм, а $r$ --- последовательность запросов. 
Стоимость обработки последовательности запросов алгоритмом
--- ${\cost}_A(r)$ --- это количество обращений к диску. 
Пусть $opt$ --- некий оптимальный алгоритм. 
Тогда алгоритм $A$ называется $c$-оптимальным ($c$-competitive), 
если для любого $r$, ${\cost}_A(r) \le c \cdot {\cost}_{opt}(r) + c_1$. 
Подразумевается, что $c$ --- константа относительно $r$, 
но она может зависеть от $k$ или других параметров.
Но это --- для детерминированных алгоритмов, для вероятностных же 
рассматривается на стоимость, а ее математическое ожидание.

Мы будем рассматривать случай, когда $A$ --- вероятностный online алгоритм, 
а $opt$ --- детерминированный \emph{offline} алгоритм (``oblivious adversary''),
т.е. он \emph{заранее} всю знает последовательность запросов $r$.
\\ 

\begin{alg}[MARKER]
Элементы кэша помечаются 0 или 1.
Все время работы разделяется на периоды. 
В начале каждого периода все элементы кэша помечены 0.

Приходит запрос. Если это запрос на блок, который уже есть в кэше, 
то он помечается 1. Если запрашивают элемент, которого нет в кэше, 
то мы случайным образом выбираем из непомеченных (то есть помеченных 0) блок, 
куда мы будем подгружать требуемый. Подгрузив, мы помечаем его 1.

Рано или поздно у нас не останется блоков, помеченных 0. 
В этом состоянии мы можем работать, пока у нас просят блоки,
находящиеся в кэше. Как только поступит запрос на элемент, 
не содержащийся в кэше, мы обнулим все пометки и начнем новый период. 
\hfill$\qed$
\end{alg} \

Теперь попробуем оценить, насколько же хорош этот алгоритм. 
Разделим все поступающие запросы на три вида: помеченные, устаревшие и чистые.
Помеченный запрос --- это запрос на блок, образ которого находится в кэше 
и помечен 1. Устаревший --- это запрос на блок, образ которого находится 
в кэше и помечен 0, или на блок, которого в кэше нет, но который там был 
в предыдущем периоде. И соответственно, чистый запрос --- это запрос на блок, 
которого в кэше нет и не было в предыдущем периоде.

Обозначим $l_i$ количество чистых запросов за $i$-тый период; 
$d_{I,i}$ --- разница между кэшами $A$ и $opt$, 
то есть можность симметрической разности этих двух множеств
(количество элементов, входящих ровно в одно из них), 
в начале $i$-того периода, 
а $d_{F,i}$ --- в конце.

Оценим снизу количество загрузок, которые сделает $opt$. 
С одной стороны, это не меньше чем $(l_i-d_{I,i})$, 
так как $l_i$ блоков алгоритм A обязан подгрузить, 
и $opt$ может выиграть у $A$ только за счет того, 
что в начале периода у него в кэше уже будут блоки, 
на которые поступят запросы. 
А так как разница между кэшами в начале периода составляет $d_{I,i}$ 
элементов, то и выиграть $opt$ может не более $d_{I,i}$ загрузок. 

С другой стороны, число загрузок не менее $d_{F,i}$, 
так как все блоки, лежащие в кэше алгоритма $A$ к концу периода, 
были запрошены. Следовательно, они должны были побывать и в кэше 
алгоритма $opt$, и если какого-то из них нет, значит, на его место 
был загружен другой блок, а нет там ровно $d_{F,i}$ элементов из кэша $A$. 

Таким образом, число загрузок, которые должен сделать оптимальный алгоритм,
составляет не менее 
$$\max(l_i-d_{I,i}, d_{F,i}) \ge \frac{l_i-d_{I,i}+d_{F,i}}{2}.$$
Это --- за один период. Просуммировав по все периодам, получим, 
что число загрузок, которые должен сделать $opt$ за все периоды не менее 
$$\sum_i\frac{l_i - d_{I,i} + d_{F,i}}2 = 
\sum_i \frac{l_i}2 + \frac{d_{F,n}}2 \ge \frac{L}2,$$ где $L = \sum_i l_i$.

Теперь оценим сверху, сколько загрузок сделает алгоритм $A$. 
Ясно, что на чистые запросы ему придется подгружать блоки, 
на помеченные --- нет. Сложнее дело обстоит с устаревшими запросами. 
Во-первых, сколько таких запросов может быть? 
Ответ: $k-l$. При устаревшем запросе мы обращаемся к блоку, 
который на предыдущем периоде был в кэше, но есть ли он там сейчас, 
мы сказать не можем. Он мог быть выгружен, когда пришел чистый запрос,
либо устаревший запрос на блок, который до этого был уже выгружен. 

Рассмотрим {\bfseries первый} устаревший запрос. 
Поскольку нам надо оценить вероятность сверху, 
то мы можем рассмотреть наиболее неблагоприятный вариант, 
когда все чистые запросы произошли до данного. 
Тогда вероятнось выгрузить нужный нам блок будет наибольшей. 
Посчитаем, какова вероятность в этом случае оставить наш блок в кэше. 
При первом чистом запросе мы оставим наш блок с вероятностью $(k-1)/k$, 
при втором --- $(k-2)/(k-1)$ и т.д. 
Вероятность того, что мы не выгрузим нужный блок при последнем, 
$l$-том чистом запросе, равна $(k-l)/(k-l+1)$. 
Перемножая эти вероятности, получим вероятность того, 
что мы оставим наш блок в кэше: $(k-l)/k$. 
Таким образом, вероятность того, что мы выгрузим его, не превосходит $l/k$.

Общий случай оставляется для самостоятельных раздумий. 
Именно, утверждается, что вероятность выгрузить нужный нам блок при 
{\bfseries $j$-том} устаревшем запросе не превосходит $l/(k-j+1)$.

Таким образом, математическое ожидание количества загрузок устаревших блоков 
меньше или равно 
$$
l/k + \ldots + l/(k-(k-l)+1) = 
l \cdot (1/k+\ldots+1/(l+1)) = 
l \cdot (H_k-H_l+1),
$$ 
где $H_n = 1+1/2+\ldots+1/n$. 
Вспомнив, что мы должны еще загрузить $l$ блоков при чистых запросах, 
мы получим, что 
$$
\mathbf{E}(\textrm{число загрузок}) \le l \cdot (1+H_k-H_l+1) \le l \cdot H_k.
$$
Это за один период. Просуммируем по всем периодам. 
Получим: $\mathbf{E}{\cost}_A = H_k \cdot L$. 
Но ${\cost}_{opt} = L/2$, и мы доказали следующую теорему. \\ 

\begin{theorem}
Алгоритм MARKER является $2H_k$-оптимальным.
\end{theorem}

\begin{defn}
Рассмотрим ситуацию, когда adversary не только знает все запросы наперед,
но и может их придумывать в зависимости от наших действий.

\emph{Adaptive offline adversary} так и поступает; придумав все запросы,
он порождает свой вариант их обработки стоимостью ${\cost}_{opt}$.
Заметим, что хотя он порождает новые запросы, зная наши ответы
на предыдущие, он не может предугадать наших \emph{будущих} действий,
ибо они зависят от выпавших нам случайных чисел. Естественно, в определении
\emph{$c$-оптимальности относительно adaptive offline adversary} фигурируют
не все $r$, а лишь $r$, порождаемые adversary.

\emph{Adaptive online adversary} отличается от adaptive offline adversary тем,
что он обязан обрабатывать (``порождать свой вариант обработки'')
придумываемые им запросы online.
\end{defn}

\begin{prob}
Оценить силу алгоритма MARKER относительно adaptive online и 
adaptive offline или 
показать, что этого сделать нельзя.
\end{prob}

\begin{prob}
Придумать алгоритм лучше.
\end{prob}

\newpage

\section{Указатель}

\hypertarget{uk}{Основные понятия.}

\begin{multicols}{2}

\hyperlink{t10}{BFS} \ 

\hyperlink{t11}{DFS} \ 

\hyperlink{d-2}{RAM} \    

\hyperlink{d-1}{алгоритм} \

\hyperlink{t14}{алгоритм Бэллмана-Форда} \ 

\hyperlink{t23}{алгоритм Варшала} \ 

\hyperlink{t15}{алгоритм Дейкстры} \ 

\hyperlink{t13}{алгоритм Косараджу} \ 

\hyperlink{t19}{алгоритм Крускала} \ 

\hyperlink{t18}{алгоритм Прима} \ 

\hyperlink{t24}{алгоритм Флойда-Варшала} \ 

\hyperlink{t9}{алгоритм Хиршберга} \ 

\hyperlink{t27}{бинарное дерево поиска} \ 

\hyperlink{r1}{быстрая сортировка} \ 

\hyperlink{e1}{динамическое программирование} \ 

\hyperlink{d1.5}{куча} \ 

\hyperlink{t20}{лес непересекающихся мн-в} \ 

\hyperlink{t-2}{метод ''Р&В''} \ 

\hyperlink{t26}{метод четырёх русских} \ 

\hyperlink{t8}{народный алгоритм} \ 

\hyperlink{t5}{нахождение $i$-го элемента} \ 

\hyperlink{t2.5}{нижняя оценка скорости} \ 

\hyperlink{t25}{основная теорема о времени работы} \ 

\hyperlink{d3}{очередь с приоритетами} \ 

\hyperlink{t4}{порязрядная сортировка} \ 

\hyperlink{d-2}{псеводкод} \  

\hyperlink{d-3}{рекурсия} \ 

\hyperlink{t-3}{сортировка вставкой} \ 

\hyperlink{t2}{сортировка кучей} \ 

\hyperlink{t3}{сортировка подсчётом} \ 

\hyperlink{t-4}{сортировка слиянием} \ 

\hyperlink{d-4}{стэк} \ 

\hyperlink{t12}{топологическая сортировка} \ 

\hyperlink{d10}{транзитивное замыкание графа} \ 

\hyperlink{t-1}{умножение Карацубы} \ 



\end{multicols}




\end{document}
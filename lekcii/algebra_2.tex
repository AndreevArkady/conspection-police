\documentclass[a4paper,100pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[unicode, pdftex]{hyperref}
\usepackage{cmap}
\usepackage{mathtext}
\usepackage{multicol}
\setlength{\columnsep}{1cm}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{icomma}
\usepackage{euscript}
\usepackage{mathrsfs}
\usepackage{stmaryrd}
\usepackage{geometry}
\usepackage[usenames]{color}
\hypersetup{
     colorlinks=true,
     linkcolor=magenta,
     filecolor=red,
     citecolor=black,      
     urlcolor=cyan,
     }
\usepackage{fancyhdr}
\pagestyle{fancy} 
\fancyhead{} 
\fancyhead[LE,RO]{\thepage} 
\fancyhead[CO]{\hyperlink{t2}{к списку объектов}}
\fancyhead[LO]{\hyperlink{t1}{к содержанию}} 
\fancyhead[CE]{текст-центр-четные} 
\fancyfoot{}
\newtheoremstyle{indented}{0 pt}{0 pt}{\itshape}{}{\bfseries}{. }{0 em}{ }

%\geometry{verbose,a4paper,tmargin=2cm,bmargin=2cm,lmargin=2.5cm,rmargin=1.5cm}

\title{Алгебра. Конспект 2 сем.}
\author{Мастера Конспектов\\ \\ (по материалам лекций В. А. Петрова,\\ а также других источников)}
\date{12 февраля 2021 г.}

\theoremstyle{indented}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\theoremstyle{definition} 
\newtheorem{defn}{Определение}
\newtheorem{exl}{Пример(ы)}

\theoremstyle{remark} 
\newtheorem{remark}{Примечание}
\newtheorem{cons}{Следствие}
\newtheorem{stat}{Утверждение}

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Tors}{Tors}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Imf}{Im}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\cont}{cont}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\ev}{ev}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\chard}{char}
\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\NN}{\mathbb{N}}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\Ann}{Ann}
\DeclareMathOperator{\Real}{Re}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Rad}{Rad}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\rk}{rk}

\begin{document}

\newcommand{\resetexlcounters}{%
  \setcounter{exl}{0}%
} 

\newcommand{\resetremarkcounters}{%
  \setcounter{remark}{0}%
} 

\newcommand{\reseconscounters}{%
  \setcounter{cons}{0}%
} 

\newcommand{\resetall}{%
    \resetexlcounters
    \resetremarkcounters
    \reseconscounters%
}

\maketitle 

\newpage

\hypertarget{t1}{Некоторые} записи по алгебре.
\tableofcontents

\newpage


\section{Лекция 30.}

Пусть $R$ - кольцо главных идеалов, а $M$ - конечно порождённый $R$-модуль (левый). 

\[
    m_1, \ldots, m_n\in M, \:\:M=\{\sum r_im_i \vert r_i\in R\}
\]

Пусть $\varphi: R^n\rightarrow M$ - функция, которая действует по правилу $e_i\mapsto m_i$ (базисные элементы $R^n$ (именно тривиального базиса) в элементы $m_i$). \

Тогдя ядро $\Ker \varphi \leq R^n$ - подмодуль. Причём равен он $\{(r_i)\vert \sum r_im_i=0\}$ - \textit{соотношения} (линейные) между $m_i$. А также он есть \textit{свободный} модуль $R^k, \: k\leq n$. 

\[
    \Ker \varphi =R^k, \:\: R^k\leq R^n 
\]
\[
    \psi: R^k\rightarrow R^n
\]

Подходящей заменой базиса в $R^k$ и $R^n$ можно добиться того, чтобы $\psi$ стала диагональной матрицей (с нижними нулевыми строками, естественно) и числами $d_1\vert d_2\vert \ldots\vert d_k$ на диагонали.

Тогда $M\cong R^{n-k}\oplus R/(d_i)\oplus\ldots \oplus R/(d_k)$ (это планируется доказывать, но перед этим нужно ввести несколько определений).\ 

\begin{defn} 
    Пусть $R$ кольцо (не обязательно коммутативное), тогда $M$ - \textit{циклический}, если он порождён одним элементом ($M=\{rm\vert r\in R\}$).
\end{defn}

Пусть $\theta: R\rightarrow M$ - гомоморфизм $R$-модулей, действующий по правилу $r\mapsto rm$, он сюръективен и $M\simeq R/\Ker \theta$ по теореме о гоморфизме.

\[
    \Ker\theta = \{r\in R\vert rm=0\} \leq R, 
\]
что также является левым идеалом.\ 

А если $R$ - область главных идеалов, то циклический модуль выглядит как $R/(d)$. Если $d = 0$, то $R$ - свободный модуль ранга $1$, а если он не равен нулю, то это есть \textit{модуль кручения} $\forall x\in M \: dx=0$.\\

\begin{theorem}
    Конечнопорождённый модуль над областью главных идеалов - конечная прямая сумма циклических модулей.
\end{theorem}\

Была доказана в прошлом семестре (не у нас). Однаком мы можем сформулировать следствие:

\begin{cons}
    Конечнопорождённая абелева группа - конечная прямая сумма циклических групп.
\end{cons}

Пусть $R$ - область, $M$ - $R$-модуль, тогда подмодуль кручения - 

\[
    \Tors (M) = \{m\in M\vert \exists r\neq 0, \: rm=0\}
\]

\begin{stat}
    $\Tors (M)$ - модмодуль в $M$.
\end{stat}

Нужно выполнить проверку этого утверждения, но для этого достаточно проверить, что всё хорошо с нулём (он там лежит и $1\cdot 0 = 0$), а затем несколько свойств:

\[
    m_1, m_2\in \Tors (M), \: r_1, r_2 \neq 0, \: r_1m_1=r_2m_2=0, 
\]
тогда
\[
    r_1r_2(m_1+m_2)=0, \: r_1r_2\neq 0, 
\]
а также, если
\[
    m\in \Tors (M), \: s\in R, \: rm=0 \Rightarrow r(sm)=rsm=s(rm)=0.
\]

Пусть $r\in R$, $r\neq 0$, $M[r]:=\{m\in M:\: rm=0\}\leq M$ - подмодуль, $p$ - пргстой элемент $R$. Рассмотрим $M[p]\leq M[p^2]\leq M[p^3]\leq \ldots$ - получили цепочку вложенных модулей.\ 

$M_p:=\bigcup_{i\geq 1}M[p^i]$ - подмодуль, $p$-кручение в $M$. \\

Сейчас начнётся пиздец. Наша цель: доказать, что $\Tors (M)\cong \bigoplus_{p-\text{простое}} M_p$.\ 

$N_i$ - модули $i\in I$, $\bigoplus :=\{(n_i)_{i\in I}\vert n_i\in N_i, \text{ почти все }n_i=0\}$, операции покомпонентные. Это, получается, (бесконечная) прямая сумма модулей.\\

\begin{theorem}
    (О примарном разложении). Пусть $R$ - область главных идеалов, $M$ - $R$-модуль. Тогда $\bigoplus M_p \rightarrow \Tors (M)$, дествующий по правилу $(m_p)\mapsto \sum m_p$ (конечная сумма) - изоморфизм модулей.
\end{theorem}

\begin{proof}
    Докажем всё по порядку:\

    \begin{itemize}
        \item Докажем, что это гомоморфизм. $(m_p+n_p)\mapsto \sum m_p+n_p=\sum m_p+\sum n_p$, а также $(rm_p)\mapsto \sum rm_p=r(\sum m_p)$. 
        \item Теперь нужно доказать сюръективность. $m\in \Tors (m)$, $rm=0$, $r=\Pi_{i=1}^np_i^{\alpha_i}$, где $p_i$ - простое. Рассмотрим линейное разложение $\text{НОД}$: 
        \[
            r_1p_2^{\alpha_2}\ldots p_n^{\alpha_n}+\ldots+r_np_1^{\alpha_1}\ldots p_{n-1}^{\alpha_{n-1}}=1.
        \]
        Тогда если мы домножим равенство на $m$, получим, что $r_i=\frac{rm}{p_i^{\alpha_i}}\in M_{p_i}$, тогда получили, что $(r_1p_2^{\alpha_2} \ldots  p_n^{\alpha_n}m, \ldots, r_np_1^{\alpha_1}\ldots p_{n-1}^{\alpha_{n-1}}m)\mapsto m$.
        \item Осталась инъективность. Пусть $0\neq(m_p)\mapsto 0$, возьмём наименьшее число индексов, что $\sum m_p=0$. А теперь начнём его уменьшать. Пусть у нас есть $p_1, \ldots , p_n$, $p_i^{\alpha_i}m_{p_i}=0$. Всё домножим на $p_n^{\alpha_n}$, получим $\sum p_n^{\alpha_n}m_p=0$. Тогда раньше было $m_{p_n}\neq 0$, а теперь $p_n^{\alpha_n}m_{p_m}=0$. Докажем, что ничего, кроме последнего не обнулилось. Предположим противное, $p_1^{\alpha_1}m_1=0$, $p_n^{\alpha_n}m_1=0$, но $p_1^{\alpha_1}, \: p_n^{\alpha_n}$ - взаимно просты, тогда есть линейное разложение $r_1p_1^{\alpha_1}+ r_n p_n^{\alpha_n}=1$, домножим на $m$, получим $r_1p_1^{\alpha_1}m_1+ r_n p_n^{\alpha_n}m_1=m_1$, но оба они не могут быть равны нулю.
    \end{itemize}
\end{proof}

Сейчас будем заниматься в основном кольцом многочленов. Пусть $R=F[t]$, $F$ - поле, $V$ - $R$-модуль. В частности, $V$ - $F$-модуль, то векторное пространство $A:v\rightarrow tv$ - $F$-линейное отображение $V\rightarrow V$ \textit{оператор}. Линейные операторы образуют кольцо (сумма - поточечно, умножение - композиция). $A(v)$ или $Av$.
\[
    (a_0+a_1t+\ldots+a_nt^n)V=a_0v+a_1Av+\ldots+a_nA^nv
\]
$V$ - векторное порстранство с оператором, значит, $F[t]$ - модуль.\ 

Пусть $a$ - матрица $n\times n$ $F^n\rightarrow F^n$, $F[t]$ - модуль на $F^n$. $F[t]$ - как модуль над собой векторное пространство со счётным базисом.\ 


\begin{stat}
    Пусть $V$ возьмём конечнопорождённый модуль над $F[t]$, тогда $V$ - конечномерное векторное пространство над $F$ тогда и только тогда, когда $V=\Tors (V)$ (как $F[t]$-модуль).
\end{stat}

\begin{proof}
    $F[t]^n\oplus F[t]/(f_i)\oplus\ldots\oplus F[t]/(f_k)$, где $f_i\neq 0$. Если $n\neq 0$, то в $V$ есть бесконечномерное подпространство $F[t]$. Если $n=0$, то $\dim _F F[t]/(f_i)=\deg f_i < \infty$.
\end{proof}

Теперь рассмотрим матрицы. Пусть $\dim V =n $, $A: V\rightarrow V$. Если зафиксировать базис в $V$, получается матрица $a$ $n\times n$. Взали другой базис, получим матрицу перехода $c$. $V\rightarrow V$ посредством $A$, причём стороны соответственно изоморфны вот таким вещам (по центру, я не умею так круто чертить, загляните в лекцию) $F^n\xrightarrow{c^{-1}}F^n\xrightarrow{a}F^n\xrightarrow{c}F^n$. И, кстати, $a\sim c^{-1}ac$ (сопряжённая матрица).\\

Рассмотрим модуль $F[t]/(f)$, что также есть $V$, $A$. Поймём, что такое $f$. Он обладает таким свойством: $(f)=\Ker(F[t\rightarrow F[t/(f)]])=\{g(t)\vert \: g(t)\cdot v=0\: \forall v\in V\}$. Однако последнее равенство неочевидно. По определению там может быть написано $\{g(t)\vert \: g(t)\cdot [1]=0\}$, но $[h(t)]=h(t)\cdot 1$, поэтому он обнуляется $g(t)$: $g(t)\cdot[h(t)]=h(t)\cdot g(t)\cdot [1]=0$, откуда и получаем искомое. \ 

Давайте теперь запишем это в терминах оператора. Если 

\[
    g(t)=a_0+a_1t+\ldots+a_kt^k, 
\]

тогда 

\[
    g(t)\cdot v = a_0v+a_1Av+\ldots+a_kA^kv.
\]

Каждый раз писать такие длинные вещи неудобно, поэтому введём следующее обозначение: 

\[
    g(A):= a_0v+a_1A+\ldots+a_kA^k.
\]

В силу того, что $A$ коммутирует с собой, то такая запись корректна. Тогда мы можем переписать:

\[
    \{g(t)\vert \: g(t)\cdot v=0\: \forall v\in V\}=\{g(t)\vert \: g(A)v=0\: \forall v\in V\},
\]

но если последнее выполнено для любого $v\in V$, то получаем, что оператор - тождественный нуль, получаем $\{g(t)\vert \: g(A) = 0\}$. \ 

Также можно пойти и в обратныую сторону, то есть, пусть мы знаем $A$, рассмотрим $\{g(t)\vert \: g(A)=0\}$. Это - идеал в $F[t]$, скажем, что это $(f(t))$, тогда $f(t)$ мы будем называть \textit{минимальным многочленом} оператора $A$. Можно заметить, что минимальный многочлен не равен нулю, если у нас имеется конечномерное пространство, не может быть такого, что никакой многочлен $A$ не обнуляет. Покажем это. \ 

Найдём некую линейную зависимость между степенями $A$. Рассмотрим $\text{Id}, A, A^2, \ldots$ - элементы кольца операторов. Рассмотрим это кольцо как векторное пространство над $F$. Если $\dim V = т$, то у полученного пространства размерность есть $n^2$, то есть, конечна. Потому бесконечной линейно независимой системы быть не может, тогда когда-то мы получим линейную зависимость:

\[
    a_0+a_1 A+\ldots+a_k A^k=0, 
\]
тогда отсюда мы и нашли требуемый многочлен.

\section{Лекция 31.}

Начинаем опять с оператора. Рассматриваем векторной пространство $V$ над каким-то полем $F$ и мы действуем на него оператором $A:V\rightarrow V$. Мы его также рассматривали как $F[t]$-модуль, $t\cdot v=Av$. Мы определили минимальный многочлен $A$ такой, что $\{g(t)\in F[t]\vert g(a)=0\}\triangleleft F[t]$, причём $F[t]=(f(t))$ - идеал унитарного (нуо) многочлена. Такой $f(t)$ и называется минимальным многочленом. \
    
Теперь немного понятнее на языке модулей. Рассмотрим $V$ - $F[t]$-модуль, а также $\Ann(V):=\{r\in V\vert rv=0, \: \forall v\in V\}$. Это - идеал в $R$, причём даже двусторонний (можно будет потом записать проверку). Причём получаем, что $\Ann(V)=(f(t))$, легко заметить, что они совпадают.\ 

$g(A)v=0$, но тогда 

\[
    g=a_0+a_1t+\ldots+a_kt^k
\]
\[
    g=a_0+a_1Av+\ldots+a_kA^tv=0,
\]
что также и равно $g(t)\cdot v$. Тогда $f(A)v=g(t)\cdot v$ как оператор и из структуры модуля соответственно. Тогда $g(A)=0$ $\Leftrightarrow$ $g(A)\cdot v=0$ для любого $v\in V$ $\Leftrightarrow$ $g(t)\cdot v=0$ $\forall v\in V$ $\Leftrightarrow$ $g(t)\in \Ann (v)$.\ 

Мы уже начинали рассматривать такой модуль: $F[t]/(f(t))$ - $F[t]$-модуль, имеем также $V$, $Av=t\cdot v$. Мы хотим придумать базис $V$, в которм матрица $A$ имеет простой вид. Возьмём такой базис: $[1], [t], \ldots, [t^{k-1}]$, тогда $[t^k]=-a_0[1]-\ldots-a_{k-1}[t^{k-1}]$. Как выглядит матрица $A$ в этом базисе?

\begin{equation*}
    \begin{pmatrix}
        0 & 0 & \dots & 0 & -a_0\\
        1 & 0& \dots & 0 & -a_1\\
        \vdots & \vdots &\ddots & \vdots  & \vdots  \\
        \vdots & \vdots & \dots & 0 & -a_{k-2}\\
        0 & 0 & \dots & 1 & -a_{k-1}
    \end{pmatrix}
\end{equation*}

Такая матрица называется \textit{фробениусовой клеткой}. А вообще, в итоге мы получили, что если $V$ - циклический $F[t]$-модуль, то $A$ в некотором базисе записывается фробениусовой клеткой, причём последним столбцом будут коэффициенты минимального многочлена, только со знаком ''минус''.\ 

А если модуль не циклический (произвольный и с конечномерным $V$), то мы можем его разложить в прямую сумма циклических: 

\[
    F[t]/(f_1(t))\oplus F[t]/(f_2(t))\oplus\ldots\oplus F[t]/(f_m(t)), 
\]
причём мы можем даже потребовать, чтобы $f_1\vert f_2\vert\ldots\vert f_n$. \ 

Умножение на $t$ будет действовать поккординатно. \ 

Для каждого слагаемого мы умеем выписывать матрицу оператора $A$ в подходящем базисе. Матрица $A$ тогда выглядит на всём пространстве как цепочка фробениусовых клеток, расставленных по порядку по диагонали.\ 

Зададимся теперь вопросом: чему же в таком случае равен минимальный многочлен? Ответ таков:
\[
    A=f_m(t),
\]
причём принципиально условие цепочки делений. \

Как считать инвариантные факторы (то есть, $f_1(t), \ldots,f_n(t)$)? Рассмотрим $V$ и $F[t]$. $e_1, \ldots, e_n$ - базис $V$ как векторное пространство над $F$, а тем более, это система образующих $V$ как $F[t]$-модуля. Какими соотношениями обладает этот набор? $t\cdot e_i=Ae_i$ - линейная комбинация $e_1, \ldots, e_n$. Это соотношение между $e_i$ с коэффициентами из $f(t)$, получаем $(t\cdot I-A)e_i=0$.\ 

Мы имеем $n$ образующих и $n$ таких последних соотношений. Рассмотрим матрицу $(t\cdot I-A)$, она имеет размер $n\times n$ над $F[t]$ и выглядит так:

\begin{equation*}
    \begin{pmatrix}
        t-a_{11} & -a_{12} & \dots & -a_{1n}\\
        -a_{21} & t-a_{22} & \dots & -a_{2n}\\
        \vdots & \vdots &\ddots   & \vdots  \\
        -a_{n1} & -a_{n2} & \dots & -a_{nn}
    \end{pmatrix}
\end{equation*}

Домножим её слева и справа на обратимые над $F[t]$ матрица и приведём её к диагональному виду, а на диагонали будут расставлены $f_1, \ldots, f_m$ (перед которыми $n-m$ единиц). Последний многочлен будет минимальным многочленом $A$. \ 

Сравним определители этих матриц. Определитель обратимой матрицы лежит в $F[t]^*=F^*$. Идеал, порождённый в $F[t]$ определителем, не поменяется, тогда 

\[
    (\det (t\cdot I-A))=(f_1(t)\ldots f_n(t)), 
\]
тогда $\det (t\cdot I-A))\in F[t]$ мы будем называть \textit{характеристическим многочленом} матрицы $A$ (обозначаем $\chi_A(t)$). Имеет он степень $n$, причём он ещё и унитарный в силу того, что максимальная степень будет содержаться в $(t-a_{11})(t-a_{22})\ldots(t-a_{nn})$.\ 

Причём тогда мы можем получить такое равенство из того, что и характеристический многочлен, и призведение $f_i$ унитарно: 

\[
    \chi_a(t)=f_1(t)\cdot\ldots\cdot f_n(t),
\]

откуда минимальный многочлен делит характеристический многочлен, а характеристический делит минимальный в степени $n$. \ 

Наборы неприводимых делителей у минимаьлного и характеристического многочленов совпадают. В частности, наборы корней без учёта кратности совпадают.\\

\begin{theorem}
    (Теорема Гамильтона-Кэли). Минимальный многочлен делит характеристичесий, имеет такие же корни [и у них совпадают неприводимые делители].
\end{theorem}\

Приступим теперь к рассмотрению \textit{нильпотентным} операторам.

\begin{defn}
    $A:V\rightarrow V$ - \textit{нильпотентный}, если $A^k=0$ для некоторого $k$.
\end{defn}

Нужно теперь научиться понимать, когда это выполнено. Берём $k:A^k=0, A^{k-1}\neq 0$ (наименьшее возможное?). Минимальный многочлен у $A$ - $t^k$, потому что он подходит, и никакой его делитель не подходит. Какой же характеристический многочлен у $A$? Это есть $t^n$, где $n=\dim V$ из теоремы Гамильтона-Кэли. \ 

Пусть $A^k=0$ - минимальная такая степень. Рассмотрим $V$ как $F[t]$-модуль.
\[
F[t]/(t^{k_1})\oplus F[t]/(t^{k_2})\oplus\ldots\oplus F[t]/(t^{k_m}), \: k_1\leq k_2\leq\ldots\leq k_m=k,
\]
а само $k$ мы называем \textit{степенью нильпотентности}. Кстати, фробениусова клетка нильпотентного оператора теперь выглядит ещё лучше, весь правый столбец теперь состоит из нулей (в подходящем базисе). В общем случае, она составлена из квадратиков такого вида. Получили мы матрицу строгонижнетреугольного вида.

\begin{defn}
    \textit{Нижнетреугольная матрица} - всё, выше главной диагонали - нули. \textit{Строгонижнетреугольная матрица} - ещё и диагональ - нули.
\end{defn}

Как найти такой базис (без формы Смита)? Запишем по индукции:

\begin{align*}
    V[t] & =\{v\in V \vert tv=0\}=\Ker (A),  \\ 
    V[t^2] & =\{v\in V \vert t^2v=0\}=\Ker (A^2),\\
    &\dots \\
    V[t^{k-1}] & =\Ker (A^{k-1}),\\
    V[t^k] & =\Ker (A^k) = V.
\end{align*}

Рассмотрим цепочку вложенных пространств:
\[
    0<\Ker(A)\leq \Ker(A^2)\leq\ldots\leq\Ker(A^{k-1})<V.
\]

Посмотрим на образ $A$ (то есть, $\Imf A$), он попадёт в $\Ker (A^{k-1})$, а вот $A(\Ker(A^{k-2}))\leq\Ker(A^{k-2})$. \ 

Осталось найти тот самый базис, в котором матрица $A$ имеет нужный вид. Рассмотрим фактор-пространство $V/\Ker(A^{k-1})$, и выберем в нём базис. Это даёт нам относительный базис $V$ относительно $\Ker (A^{k-1})$ (скажем, это $e_1, \ldots, e_s$). Тогда что с ними происходит: $e_1. Ae_1, \ldots, A^{k-1}e_1$, причём получается, что все они не равны нулю, так как они не лежат в классе нуля. \ 

Рассмотрим $\langle e_1. Ae_1, \ldots, A^{k-1}e_1\rangle$ - $A$ переводит его в себя. Рассмотрим матрицу $A$ в данном базисе, это как раз будет фробениусова клетка размера $k$. Так проделаем для каждого элемента базиса и получим $s$ фробениусовых клеток размера $k$, где $s$ также было размерностью отфакторизованного пространства, тогда $s=\dim V-\dim \Ker (A^{k-1})$. \ 

Теперь рассмотрим $\Ker(A^{k-1})/(\Ker A^{k-2}+\Imf A)$ -  подпространство, порождённое $\Ker A^{k-2}$ и $\Imf A$. Возьмём относительный базис $e_{1, 1}, \ldots, e_{s_1, 1}$, опять перейдём к $\langle e_{1, 1}. Ae_{1, 1}, \ldots, A^{k-1}e_{1, 1}\rangle$ - тут $A$ имеет матрицу в виде фробениусовой клетки размера $k-1$ (если фробениусовых клеток такого размера нет, это пространство равно нулю). $s_1$ - количество таких клеток. \ 

И, наконец, клетки размера $k-i$: $\Ker(A^{k-i})/(\Ker(A^{k-i-1})+\Imf A^i)$, рассмотрим тут базис и проделаем аналогичные операции. \ 


\section{Лекция 32.}


\begin{remark}
    В предыдущей лекции была допущены небольшая ошибка, в месте, где записано $\Ker A^i/(\Ker A^{i-1}+\Imf A^{n-i})$, нужно записать $\Ker A^i/(\Ker A^{i-1}+(\Imf A\cap \Ker A^i))$.
\end{remark}

Допустим, у нас есть два разных поля: пусть раньше мы рассуждали над полем $K$, а сейчас есть ещё $L\geq K$. Над $K$ было векторное пространство $V$ с базисом $e_1, \ldots, e_n$. Мы можем рассмотреть такое же пространство над $L$, размерности тоже $n$. Рассмотрим $V_L$ - пространство, натянутое на $e_1, \ldots, e_n$ над $L$, то есть, все линейные комбинации вида $\{\alpha_1e_1+\ldots+\alpha_ne_n\}$. То есть, $\dim V = \dim V_L = n$. Тогда понятно, если у нас есть оператор $A: V\rightarrow V$, то мы можем его продолжить до оператора $A_L:V_L\rightarrow V_L$. \ 

Представить себе это можно по-разному. Представим себе матрицу изначального оператора в этом базисе, это какая-то матрица $M_n(K)\subseteq M_n(L)$ - можем ''расширить'', и получим, что первое - подкольцо второго. И тогда можно написать оператор с точно такой же матрицей на $L$. Можно также сказать, что мы рассматриваем $A(\alpha_1e_1+\ldots+\alpha_ne_n)$, тогда раскроем по линейности $\alpha_1A(e_1)+\ldots+\alpha_nA(e_n)$, и посчитаем необходимые элементы внутри первого кольца. \ 

Что же меняется при переходе от ожного поля к другому? У нас есть инвариантные факторы, например, если у нас есть оператор $A$, то для него есть многочлены $f_1, \ldots, f_m\in K[t]$ (последний - минимальный). Тогда для $A_L$ они также инвариантны, причём даже минимальный многочлен такой же. Давайте вспомним, как они строятся в терминах оператора $A$. \ 

Пусть у нас имеется матрица $a$ (перехода $A$), рассмотрим матрицу $a-t\cdot I$, тогда инвариантные факторы - $\frac{\text{НОД}(\text{все миноры порядка } i-1)}{\text{НОД}(\text{все миноры порядка } i)}$. А наибольший общий нелитель не зависит от того, в каком поле ме его рассматривали. Значит, инвариантные факторы не изменятся. \ 

Мы знаем, что в каком-то базисе матрицу $A$ можно привести к фробениусовой форме (на диагонали - квадратики, последняя клетка - соответствующая $f_m$). 

\begin{defn}
    $\End(V)=\{A:V\rightarrow V\}$ - множество всех линейных операторов (эндоморфизмы $V$). Кстати, это кольцо (поточечное сложение и композиция), которое изоморфно $M_n(K)$, посредством выбора базиса. 
\end{defn}

Пусть $c$ - матрица перехода при изменении базиса и $A$ - операотр с матрицей $a$, тогда в новом базисе у него будет матрица $c^{-1}ac$ - \textit{сопряжённая} к $a$ матрица.

\begin{defn}
    $A, \: B$ - \textit{сопряжённые}, если существует $C$ - обратимый $B=C^{-1}AC$. 
\end{defn}

Сформулируем такую теорему, которую мы уже по сути доказали: \\

\begin{theorem}
    $A$, $B$ сопряжены тогда и только тогда, когда у них одинаковые инвариантные факторы.
\end{theorem}

\begin{proof}
    Найдём базис, в котором матрица $A$ записывается в фробениусовой нормальной форме. Существует какой-то другой базис, в котором матрица $B$ записывается точно также. Тогда нужно взять просто матрицу, которая переведёт один базис в другой. В обратную сторону - если $A$ известно в какой фробениусовой форме, то легко определить, что $f_j$ - инвариантные факторы.
\end{proof}

\begin{cons}
    $A,\: B$ сопряжены тогда и только тогда, когда $A_L, \: B_L$ сопряжены. Анаолгично можно записать и для матриц из изоморфности колец.
\end{cons}

Приведём другое доказательство второго пункта в случае бесконечного $K$. Мы хотим найти такую обратимую матрицу, что $ac=cb$. Пусть $с$ - матрица с неизвестными коэффициентами. Тогда у нас имеется система однородных линейных уравнений на $x_{i, j}$, где $c=(x_{i,j})$. Она имеет нетривиальное решение над $L$, причём набор решений образует подпространство $L^{n^2}$ размерности $K$. Тогда над базовым полем $K$ размерность подпространства будет точно такая же, поскольку метод Гаусса не зависит от поля, над которым мы работаем, поэтому он выдаст одинаковые ответы для $K$ и для $L$. \ 

Возьмём базис в этом подпространстве: $c_1, \ldots, c_k$. Рассмотрим всевозможные комбинации $\{\lambda_1c_1+\ldots+\lambda_kc_k\}$, и будме искать такую линейную комбинацию, определитель которой не равен нулю. Мы знаем, что над $L$ такие существуют, потому что над $L$ у нас есть решение. Но определитель - суть многочлен от $\lambda_i$, причём ненулевой, поскольку над $L$ можно найти такие $\lambda_i$, значение при которые не нуль. А поскольку поле $K$ бесконечное, то можно такие коэффициенты найти и над $K$ (индукция по $k$). Доказательство завершили.

Рассмотрим теперь за место $L$ алгебраическое замыкание $K$, $K\leq \overline{K}$. Мы уже знаем, что есть фробениусова нормальная форма, но она не очень удобна. Рассмотрим оператор $A_{\overline{K}}$. Применим для кольца $\overline{K}[t]$ теорему о строении модулей над кольцами главных идеалов, но сначала применим примарное разложение. То есть, возьмём какой-то неприводимый многочлен над алгебраически замкнутым полем, он линейный ($t-\lambda$). И начинаем теперь образовывать блоки. У нас есть $V_{\overline{K}}[t-\lambda]= \{v:\: (t-\lambda)v=0\}=\{v: \: Av=\lambda V\}$ - собственное подпространство, соответствующее собственному числу $\lambda$. $v\neq 0$ из этого множества - собственные векторы, соответствующие собственному числу $\lambda$. \ 

Нас интересуют в качестве $\lambda$ - корни минимального многослена (корни характеристического) (чтобы мы получали ненулевые множества), но можно и проще, преобразуем к $(A-\lambda I)v=0$, но это раносильно тому, что $\det(A-\lambda I)=0$, что и равносильно первому. Далее мы смотрим на степени $V_{\overline{K}}[(t-\lambda)^2]=\{v:\: (t-\lambda)^2v=0\}$, и так далее, а затем берём объединенение $V_\lambda=\bigcup_{i\geq 1}V_{\overline{K}}[(t-\lambda)^i]$, это - корневое подпространство, отвечающее собственному числу $\lambda$. Не стоит путать это с собственным подпространством (по сути, первый и последний член цепи). \ 

Из общей теории мы теперь знаем, что $V_{\overline{K}}=\oplus V_\lambda$, где суммируем по $\lambda$ - собственным числам. Мы получили корневре разложение. Посмотрим теперь, что происходит на каком-то корневом подпространстве. Ограничим $A|_{V_\lambda}$, тогда минимальный многочлен этого ограничения - $(t-\lambda)^k$. А если рассмотреть оператор $A-\lambda I$, то его минимальный многочлен будет $t^k$, то есть, ограничение такой вещи нильпотентное, то есть, матрица будет состоять из квадратиков по диагонали, на диагонали которых нули, а под ними - диагональ из единиц. А вот если мы вернёмся к изначальному сужению, то мы получим матрицу, состоящую из \textit{жордановых блоков}, это то же самое, что и предыдущая матрица, только на главной диагонали везде $\lambda$. \ 
 
Сама матрица $A$ тогда будет состоять из кучи таких блоков для всех $\lambda$ по диагонали, и целиком такое представление $A$ будет называться \textit{жордановой нормальной формой}. Таким образом, для любого ооператора существует базис, в котором матрица выглядит в такой форме. \ 

Рассмотрим такой важный частный случай. Пусть имеется характеристический многочлен $\chi_A(t)$ (по сути, $\det(\lambda I - A)$), и степень его тогда есть размерность пространства ($n$). Предположим, что он раскладывается в произведение линейных $(t-\lambda_1)\ldots(t-\lambda_n)$ (это какое-то условие на характеристический многочлен ($\text{НОД}(\chi_A(t), \chi_A'(t))=1$)). Но всё же, если они все различны, то жорданова форма просто диагональная, так как на диагонали под ними просто ничего не поместится, квадратики единичные. В каком же базисе матрица имеет такой вид? Для того, чтобы это понять, достаточно решить $Av_i=\lambda_iv_i$, $v_i=\neq0$. Такой базис, который мы найдём, \textit{диагонализирует} матрицу. Пока что всё это происходило над алгебраическим замыканием. \ 

Перейдём к случаю $K=\RR$, $\overline{K}=\CC$ и придумаем вещественную жорданову форму (именно её, а не фробениусову, потому что она удобнее). Для этого, нам нужно разобрать немного подробнее процедуру переходу из поля в замыкание с одним и тем же базисом ($V\rightarrow V_{\CC}$). Как нам тогда восстановить $V_{\CC}$? Вообще, никак, но если ввести некую дополнительную структуру это можно сделать. Хочется ввести на $V_{\CC}$ какую-то инволюцию - аналог комплексного сопряжения. Если у нас уже есть базис, то пусть $\overline{z_1e_1+\ldots+z_ne_n}=\overline{z_1}e_1+\ldots+\overline{z_n}e_n $, это операция из $V_{\CC}$ в $V_{\CC}$, которая не будет линейной, а будет \textit{полулинейной}, то есть, выполнено $\overline{zv}=\bar{z}\bar{v}$, а не $\overline{zv}=z\bar{v}$. С суммой же всё нормально. Получили мы \textit{полулинейный оператор}, который является инволюцией. \ 

А само $V$ тогда восстанавливается: $V=\{v\in V_{\CC}: \: \bar{v}=v\}$. Это уже пространство над $\RR$ такой же размерности. \ 

Итого, вещественные векторные пространства по сути есть комплексные векторные пространства такой же размерности с полулинейной инволюцией. Придумаем теперь вещественный аналог жордановой формы. Пусть есть $V$, $A: \: V\rightarrow V$, тогда $V_{\CC}$ назовём \textit{комплексификацией} $V$, а наоборот - \textit{овеществлением}. Также мы можем рассмотреть и комплексификацию $A_{\CC}: \: V_{\CC}\rightarrow V_{\CC}$. Есть базис, в котором он представляется жордановой начальной формой. У $A$ есть характеристический многочлен $f(t)\in \RR[t]$, у которого есть вещественные корни $\alpha_i$ и мнимые $\lambda_j$ вместе со своими сопряжёнными парами. Пусть $\lambda$ - комплексный корень этого многочлена, тогда у нас сеть корневые пространства $V_{\lambda}$ и $V_{\bar{\lambda}}$, тогда их сумма устойчива относительно нашей полулинейной инволюции. Потому что, допустим, $Av=\lambda v$, тогда $\overline{AV}=\bar{\lambda\bar{v}}$, но про $A$ мы знаем, что у его матрицы коэффициенты вещественные, поэтому $\overline{Av}=A\bar{v}$ (можно расписать умножение матрицы на столбец для наглядности). \ 

Таким образом, если $v$ - собственный вектор, отвечающий $\lambda$, то $\bar{v}$ - собственный вектор, отвечающий $\bar{\lambda}$, и со степенью, конечно, то же самое: $(A-\lambda I)^iv=0$, тогда $\overline{(A-\lambda I)^i}\bar{v}=0=(A-\bar{\lambda} I)^iv$. То есть, вся эта прямая сумма относительно $\lambda$ и его сопряжённого, будет устойчивой относительно нашей полуинволюции. Мы знаем, что вещественное векторное пространство это то же самое, что и комплексное с полуинволюцией, тогда все пары $\lambda_i$ и их сопряжённый будут объединяться в пары и давать вещественные подпространства. 

\section{Лекция 33.}

В прошлый раз мы рассматривали $V$ - векторное пространство над полем вещественных чисел и какой-то оператор $A: V\rightarrow V$, а также его комплексификацию $V_{\CC}$ и переход от одного к другому брагодаря полулинейному отображению. \ 

$V$ над вещественными разбивается в сумму пространств, часть из них соответствует вещественным корням, а часть - мнимым. 
\[
    V=\oplus_{\lambda=\bar{\lambda}}V_\lambda^{\RR}\oplus_{\lambda\neq\bar{\lambda}}V_{{\lambda, \bar{\lambda}}}^{\RR}.
\]

Как выглядит ограничение оператора $A$ на эти подпространства? Давайте начнём с какого-то жорданового блока (для начала, случай $\lambda\neq \bar{\lambda}$). Обозначим базис данной жордановой формы $v_1, \ldots, v_k$, тогда 

\begin{align*}
    A_{\CC}v_1&=\lambda v_1+v_2, \\
    A_{\CC}v_2&=\lambda v_2+v_3, \\
    &\vdots \\
    A_{\CC}v_{k-1}&=\lambda v_{k-1}+v_k, \\
    A_{\CC}v_k&=\lambda v_k.
\end{align*}

Что плохо в эитх векторах, почему их нельзя спустить до нашего пространства над вещественными? Они могут быть не инвариантны относительно инволюции. Применим ко всему, кроме $A_{\CC}$ черту. Это означает, что в пару к данному жордановому блоку идёт другой жорданов блок, базис которого соответственно сопряжён изначальному, а на диагонали стоят $\bar{\lambda}$. Пусть теперь у нас есть $v_1$ и $\overline{v_1}$. Чтобы получить вектор, инвариантный относительно инволюции, надо их сложить. Также можно вычесть и умножить на $i$, нетрудно проверить, что эта вещь также будет инвариантна относительно инволюции. \ 

И теперь возьмём прямую сумму тех самых парных пространств, но сделаем замену базиса на $v_j+\bar{v_j}, i(v_j-\bar{v_j})$ по всем $j$. Несложно проверить, что это также базис, однако теперь все его элементы инвариантны относительно инволюции, а значит, просто ''живут'' в самом $V$. Осталось переписать матрицу $A$ в новом базисе. Тут какая-то муть с вычислениями, в итоге получается

\begin{equation*}
    \begin{pmatrix}
        \Real(\lambda) & -\Imf(\lambda) \\
        \Imf(\lambda) & \Real{\lambda}
    \end{pmatrix}
\end{equation*}

- расставлены по диагонали квадратиками $2\times 2$, а под каждым из них - единичные матрички $2\times2$. \ 

Кстати, $\CC\leq M_2(\RR)$ посредством перехода $\lambda$ в такие матрицы. Теперь пора перейти к случаю вещественного $\lambda$. \ 

Возможны два варианта: $v_1$ и $\bar{v_1}$ могут быть либо линейно зависимы, либо линейно независимы (раньше-то они лежали в разных пространствах, а сейчас такое утверждать нельзя). В первом случае скажем, что $\bar{v_1}=\alpha v_1$. Тогда $\alpha$ может равняться чему-то на единичной окружности, так как из инволюции $\alpha \bar{\alpha}=1$. \\

\begin{lemma}
    (Простейший случай теоремы Гильберта 90). Если $\alpha \bar{\alpha}=1$, то существует $\beta$ такой, что $\alpha=\frac{\beta}{\bar{\beta}}$ (всё в $\CC$). 
\end{lemma}

Эта лемма была к рассуждению о том, что если $v_1\rightarrow \beta v_1$, то $\overline{\beta v_1}=\bar{\beta}\alpha v_1 = \bar{\beta}\frac{\alpha}{\beta}(\beta v_1)$, тогда мы и выбираем $\frac{\beta}{\bar{\beta}}=\alpha$. То есть, можем считать, что $v_1=\bar{v_1}$. \ 

Но тогда $v_1$ лежит в нашем вещественном пространстве, и то, что им порождено, также лежит в этом пространстве ($v_1\in V$, $v_2=Av_1-\lambda v_1\in V$, и так далее). То есть, в этом случае, жорданов блок так и остаётся жордановым блоком в том же самом базисе. \ 

Ну и, наконец, если $\langle v_1\rangle\neq\langle \bar{v_1}\rangle$ - сделаем то же, что и раньше, от того, комплексное $\lambda$ или вещественное, зависело только то, будет ли система базисом или нет. То есть, матрица состоит из блоков $2\times 2$ с $\lambda$ по диагонали, под которыми, опять-таки, единичные $2\times 2$. А если перенумеровать базис (сначала идут нечётные, а потом чётные), то просто получатся два жордановых блока одинакового размера. Окончательно, теорема такая: \\ 

\begin{theorem}
    Есть $V$, $A$, $\chi_A(t)$, корни которого есть $\lambda_i$ - мнимые и $\alpha_j$ - вещественные (причём, суммарно количество корней - размерность пространства, конечно же). Тогда в некотором базисе $A$ имеет вид блочный, состоящий из жордановых блоков, каждый из соответствующих комплексным $\lambda_i, \bar{\lambda_i}$ выглядит как квадратик $2\times 2$, вид которого был показан выше, под каждым из которых единичная матричка $2\times 2$, а что касается вещественных, они просто выглядят без изменений, обычная жорданова форма. 
\end{theorem}\

Вернёмся опять к ситуации алгебраически замкнутого поля. Мы говорили, что если $\chi_A(t)$ имеет различные корни $\lambda_i$, то $A$ диагонализируема и принимает вид - $n$ её корней по диагонали по порядку. Давайте поймём, когда все корни $f(t)\in K[t]$ в $\overline{K}$ различны. Мы уже знаем, что можно сказать, что требуемо соотношение $НОД(f(t), f'(t))=1$, но мы хотим переписать это в каком-то более явном виде многочлена от коэффициентов. Пусть у нас есть $f(t)$ и $g(t)$, $\deg(t)=n$, $\deg(g)=m$. Как узнать по коэффициентам $f$ и $g$, когда их $НОД$ есть единица. Так мы перешли к теме \textit{результанты}. \ 

Для начала, немного в общих чертах. Если $НОД=1$, то существуют $p, \: q$: $pf+qg=1$. Давайте расссмотрим гомоморфизм $F[t]\times F[t]\rightarrow F[t]$, действующий по правилу $(p, q)\rightarrow pf+qg$. Это $F$-линейное отображение (не гомоморфизм колец), причём единица представляется тогда и только тогда, когда оно сюръективно. Критерий хороший, плохо только то, что трудно проверить сюръективность. \ 

Рассмотрим $F[t]/(g)\oplus F[t]/(f)\rightarrow F[t]/(fg)$ по формуле $([p], [q])\mapsto ([pf+qg])$. Необходимо, конечно, проверить корректность этого отображения (но это - в запись, если интересно). Размерность пространств из отобажения - $m+n$ у обоих. Тогда матрица отображения квадратная, и сюръективность отображения контроллируется определителем. Осталось выписать матрицу отображения в каком-то базисе. Пусть $f=a_nt^n+\ldots+a_0$, $g=b_mt^m+\ldots+b_0$. Тогда в базисах - степенях $t$ от нулевой до $n$-ой, $m$-ой и $n+m$-ой соответственно, матрица так и выглядит:

\begin{equation*}
    \begin{pmatrix}
        a_0 & 0 & 0 & \ldots & 0 & b_0 & 0 & \ldots & 0 \\
        a_1 & a_0 & 0 & \ldots & 0 & b_1 & b_0 & \ldots & 0 \\
        a_2 & a_1 & a_0 & \ldots & 0 & b_2 & b_1 & \ldots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_n & a_{n-1} & a_{n-2} & \ldots & a_i & \vdots & \vdots & \ddots & \vdots \\
        0 & a_n & a_{n-1} & \ldots & a_{i+1} & b_m & b_{m-1} & \ldots & b_j \\
        0 & 0 & a_n & \ldots & a_{i+2} & 0 & b_m & \ldots & b_{j+1} \\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \ldots & a_n & 0 & 0 & \ldots & b_m
    \end{pmatrix}
\end{equation*}

И чуть менее легко, чем её написание, мы можем найти её определитель, который и называется \textit{результантом} $f$ и $g$ (как многочлен). Таким образом, сформулируем теорему: \\ 

\begin{theorem}
    $НОД(f, g)=1\Leftrightarrow \Res (f, g)\neq 0$
\end{theorem}

\begin{cons}
    $f$ не имеет кратных корней в $\bar{F}$ тогда и только тогда, когда $\Res(f, f')\neq 0$. И, кстати, если определить \textit{дискриминант} в общем случае: $\disc(f)=\frac{\Res(f, f')}{a_n}$, то можно говорить, что дискриминант не равен нулю.
\end{cons}

Таким образом, мы узнали, что кратность еорней контроллируется каким-то многочленом, которых зависит от коэффициентов изначальных многочленов. Благодаря принципу продолжения алгебраических тождеств можно перейти к \textit{плотности диагонализируемых матриц}. Каков принцип? Пусть, имеются матрицы $n\times n$, хотим проверить, что какой-то многочлен от коэффициентов равен нулю $P(a_{ij})=0$ - хотим проверить (например, мы хотим доказать теорему Гамильтона-Кэли: $\chi_A(A)=0$). Мы хотим, чтобы многочлен принимал значение нуль также и при коэффициентах сопряжённой матрицы. Тогда достаточно это проверять только для диагональных матриц - если это выполнено для диагональных, то выполнено и для всех. \ 

(сюда можно пару чертежей пояснения перенести)\ 

Почему этот принцип верен? Перейдём сначала к $\bar{K}$, затем рассмотрим $\disc(\chi_A(t))$. Тогда если дискриминант не равен нулю, то матрица диагонализуема, то есть, сопряжена с диагональной, а тогда многочлен $P$ на ней обнуляется. То есть, если дискриминант не равен нулю, то $P=0$, применяем принцип продолжения и получим, что $P$ тождественно равен нулю. Сам принцип - по сути тавтология, просто надо умножить $P$ на дискриминант, это всегда нуль, тогда и получаем, что требовалось.

\section{Лекция 34.}

Начинааем изучать билинейные и квадратичные формы. Для начала, вспомник некоторые старые моменты. $F$ - поле, $V$ -векторное пространство (скаляры пишем справа). Определяли мы также $V^*=\{f:V\rightarrow F - \: \text{линейное отображение}\}$ (двойственное пространство). Эта вещь также будет векторным простоанством, скаляры слева. Проверим: 

\begin{equation*}
    \begin{aligned}
        (\alpha f)(v)=\alpha\cdot f(v); 
        (\alpha f)(v\beta)=\alpha\cdot f(v)\cdot \beta.
    \end{aligned}
\end{equation*}

Мы знаем, что если $V$ конечномерное, то $\dim V^*$ имеет такую же размерность, и они изоморфны, хоть и невозможно найти естественный изоморфизм. Однако есть канонический изоморфизм $V\rightarrow V^*$ по правилу $v\mapsto $ линейная форма на $V^*$, которая задаётся по правилу $v(f)=f(v)$. Пояснение: мы хотим перевести вектор в функцию, которая переводит функцию в скаляр. Тогда мы возьмём такую функцию, которая переводит вектор в функцию, которая переводит все изначальные функции в их образы от этого вектора (возможно, понятнее кому-то и не стало). Это отображение не зависит от базиса, кстати (всё это в предположении, что $\dim V< \infty$). \ 

Давайте посчитаем двойственное простанство к фактор-пространству $(V/U)$, $U\leq V$. Как $(V/U)^*$ соотносится с $V^*$? Рассмотрим такую цепочку: 
\[
    V \rightarrow V/U \rightarrow F,
\]
мы получили два линейных отображения, композиция которых, конечно, также линейна. Поэтому любой элемент $f\in(V/U)^*$ мы можем перевести в $V^*$ по правилу $f\circ\pi$, где $\pi$ - факторизация. Сформулируем таким образом, теорему: \\

\begin{theorem}
    Это отображение задаёт изоморфизм $(V/U)^*$ и подпространства $V^*$: $\{g\in V^*|g|_U=0\}=:W$. 
\end{theorem}

\begin{proof}
    Почему $f\circ \pi \in W$. Надо просерить, что эта композиция обнуляется на $U$, но это очевидно, так как ограничение $\pi$ на $U$ уже обнуляется. Покажем инъективность и сюръективность: \ 

    Инъективность. Предположим, что $f\circ \pi =0$. Посчитаем $f([v])=f\circ \pi(v)=0$, поэтому $f=0$ (воспользовались сюръективностью $\pi$). \ 

    Сюръективность. $g\in W$, $g|_U=0$, рассмотрим $f([v]):=g(v)$. Это определение корректно (нетрудно рассмотреть два элемента из одного класса). Тогда $f\circ \pi(v)=f([v])=g(v)$, то есть, $g=f\circ \pi$. 
\end{proof}

Перейдём, наконец, к билинейным формам. Рассмотрим какое-то линейное отображение $V\rightarrow^B V^*$. Любое такое отображение называется \textit{билинейной формой}. Для каждого $v, u\in V$, мы определяем какой-то элемент $B(v)\in V^*$. Однако вспомним, что $B(v)(u)\in F$. То есть, тут дважды линейная функция и задать билинейную форму - всё равно, что задать линейное отображение от двух элементов (линейное по обоим аргументам). $B: V\times V\rightarrow F$, $(v, u)\mapsto B(v, u)\: (=B(v)(u))$. Множество всех билинейных форм образует векторное пространство, размерность которого равна квадрату размерности $V$. \ 

Если в $V$ зафиксирован базис $e_1, \ldots, e_n$, то $B$ можно записать в виде матрицы. $B(e_i, e_j)=\Gamma_{ij}$-матрица $n\times n$ (\textit{матрица Грама}). Эта матрица полностью задаёт линейную форму. Рассмотрим $B(u, v)$, разложим оба вектора в базис и раскроем по линейности, получим $\sum_{i, j}\alpha_i \Gamma_{i, j} \beta_j$ ($B(u \alpha, v \beta)=\alpha B(u, v) \beta$, если что). А вообще, $\Gamma_{ij}\beta_j$ (все из последней формулы) - произведение матрицы на вектор, тогда это, по сути, $u^T\Gamma v=B(u, v)$. \ 

Что происходит с матрицей Грама при замене базиса? Допустим, матриса перехода - $C$. Матрица линейного оператора, например, заменяется на сопряжённую, тут немного по другому. Рассмотрим $B(Cu, Cv)=(Cu)^T\Gamma Cv=u^T(C^T\Gamma C)v$. Таким образом, при замене базиса, матрица Грама заменяется на $C^T\Gamma C$, где $C$ - матрица перехода. \ 

Начинали мы с того, что мы хотим иметь изоморфизм $V$ и $V^*$. При каком условии $B$ будет изоморфизмом? Так как оба они конечномерные векторные пространства с одинаковыми размерностями, то эта вещь изоморфизм тогда и только тогда, когда ядро тривиально. $\Ker(B)=:\Rad(B)$ - \textit{радикал} $B$. $u\in \Rad B $ тогда и только тогда, когда $\forall v\in V$ $B(u, v)=0$ и вотрадикал должен быть равен 0, то есть, таких $u$ быть не должно, чтобы был изоморфизм. Как ещё можно рассмотреть биективность? 

\begin{defn}
    $B$ - \textit{невырожденная}, если $\Rad B=0$. 
\end{defn}

\begin{lemma}
    $B$ - невырожденная тогда и только тогда, когда $\det \Gamma \neq 0$. 
\end{lemma}

\begin{remark}
    Определитель $\Gamma$ не является инвариантом, так как $\det(C^T\Gamma C)=\det(C)^2\det \Gamma$ - что может быть инварантом, тольно если рассматривать отфакторизованно по квадрату. В любом случае, нуль и так будет нулём, тут без разницы.
\end{remark}

\begin{proof}
    $B(u)(v)=u^t\Gamma v$, пусть определитель не равен 0 (тогда $\Gamma$ сюръективна) и предположим, что у нас есть $u\neq 0$ такой что всегда $u^T\Gamma v=0$. Если Тогда в качестве $\Gamma v$ мы можем получить любые векторы из $V$ ($\Gamma$ сюръективна же, просто рассматриваем её как линейный оператор). В частности, мы можем получить базисные векторы: $\exists v_i: \: \Gamma v_i = e_i$. А $u^Te_i$ - $i$-ая координата $u$. Значит, все координаты равны нулю и сам векторы нулевой, противоречие. \ 

    Теперь в обратную сторону. Предположим, что $\det \Gamma = 0$, тогда $\{\Gamma v|v\in V\}$ - собственное подпространство $V$. Мы знаем, что свойство $\det \Gamma =0$ не зависит от выбора базиса, тогда выберем $e_1, \ldots, e_r$ - базис подпространства и рассмотрим $u:=e_{r+1}$. Тогда $u^T \Gamma v$, но это равно нулю (надо посмотреть на матрички, там почти всё нули). Тогда $e_{r+1}\in \Rad B$. 
\end{proof}

Теперь перейдём ещё ближе к теме. Посмотрим теперь на билинейные формы, как на функции от двух переменных, а ещё точнее на те, которые \textit{симметричны}, то есть, $B(u, v)=B(v, u)$ $\forall u, v\in V$. Что тогда с матрицей Грама? Она должна быть симметричной: $\Gamma^T = \Gamma$. Симметричные билинейные формы образуют пространство размерности $\frac{n(n+1)}{2}$. \ 

Однако мы хотим узнать не то, как симметричная форма выглядит в конкретном базисе, а с точностью до эквивалентности $\Gamma \sim C^T \Gamma C$. Основная задача теории квадратичных форм - как раз узнать, как выглядят симметричные билинейные формы над данным полем $F$ с точностью жо этого отношения эквивалентности. Это сложная задача, которая, конечно, зависит от $F$. Мы легко справимся со случаями $\CC$, $\RR$, конечными полями, а вот уже над полем рациональных - нереалочка, 4 семестр. \ 

Давайте рассмотрим чуть более инвариантно, пусть у нас есть $B$ - на пространстве $V$ и $B'$ на пространстве $V'$. Мы говорим, что $B$ \textit{изометрично} $B'$ ($\simeq$), если $\exists \varphi: V \tilde{\rightarrow}V'$ $B'(\varphi(u), \varphi(v))=B(u, v)$. В терминах $\Gamma$ это то же самое соотножение, в качестве $C$ надо взять матрицу $\varphi$. \ 

Если у нас есть билинейная симметричная форма, что $B$ восстанавливается по $Q(u)=B(u, u)$ - ассоциированная квадратичная форма (начиная с этого момента характеристика $F$ не равна 2, иначе тут пиздец различать надо всякое непотребное). Если нам задана $Q(u)$, то можно найти $B(u, v)=(Q(u+v)-Q(u)-Q(v))/2$ (из-за этой двойки и ограничение на характеристику). \ 

Квадратичная форма - отображение $Q: V\rightarrow F$ - отображение со свойствами: 

\begin{itemize}
    \item $Q(u+v)- Q(u)-Q(v)= Q(u, v)$ - билинейная форма; 
    \item $Q(u\alpha)=Q(u)\alpha^2$, $\forall u\in V$, $\alpha \in F$. 
\end{itemize}

Если характеристика не равна 2, то есть биекция между симметричными билинейными и квадратичными формами. В одну сторону: $Q \mapsto Q(u, v)$, в другую - $B(u, u)\mapsfrom B$ (где-то тут надо то ли умножить, то ли поделить на 2). \ 

Наша задача - классиыицировать симметричные билинейные формы с точностью до изометрии. Начать можно с радикалов: если у нас есть форма $B: V\rightarrow V^*$, рассмотрим $\Rad B$. Рангом симметричной билинейной формы называется $\rk B:=\dim V - \dim \Rad B = \rk \Gamma$, что является инвариантом. Если билинейная форма невырожденная, то есть ещё и инвариант под названием \textit{дискриминант}: $\disc B = (-1)^{\frac{n(n-1)}{2}}\det \Gamma \: \mod{F^*}$. \ 

Таким образом, у нас есть инваранты ранг и дискриминант (первые и очевидные). Поймём теперь как по любой форме построить некоторую невырожденную форму (естественно, на каком-то другом пространстве). Давайте посмотрим на $(V/\Rad B)^*$, эта вещь изоморфна подпространству в $V^*$, состоящем из форм, обнуляющихся на $\Rad B$. Вот мы и хотим придумать билинейную форму на пространстве $V/\Rad B$. $\overline{B}([v]):=B(v)$ - единственное, что приходит в голову, но надо проверить корректность: $\overline{B}([u+v])=B(v+u)=B(v)+B(u)=B(v)$, ($u \in \Rad B$). Нужно проверить, что мы получили образы именно в нужном подпространстве, а не абы где в $V^*$, то есть, $B(v)\in(V/\Rad B)^*$, то есть, обнуляется на $\Rad B$. $B(v)(u)=B(u)(v)=0$, так как $u\in \Rad B$, то есть, мы построили некую билинейную форму на $V /\Rad B$. Сформулируем лемму: \\

\begin{lemma}
    Форма $\overline{B}$ на $V/\Rad B$ невырождена.
\end{lemma}

\begin{proof}
    Пусть $[v]\in \Rad $, тогда $0=\overline{B}([v])=B(v)$, значит, $v$ лежит в радикале, то есть, его класс равен нулю.
\end{proof}

В терминах матрицы Грама это всё тоже очевидно (если рассмотреть хороший базис). Допустим, у нас есть $\Rad B$, $e_1, \ldots, e_k$ - базис $\Rad B$, и его дополнение до полного базиса $e_{k+1}, \ldots, e_n$. Тогда матрица выглядит так, что только лишь нижний правый квадратик $(n-k)\times (n-k)$ ненулевой и есть матрица полного ранга, $n-k=\rk B$. \ 

Таким образом, изучение симметричных билинейных форм сводится к изучению невырожденных симметричных билинейных форм. 

\section{Лекция 35.}

В прошлый раз мы определили понятие билинейных форм, которое можно понимать двояко: либо как функцию $V\times V \rightarrow F$, либо как $V\rightarrow V^*$. Определили мы также понятие изометрии, существование изоморфизма $\varphi: V\rightarrow V'$, такого что $B'(\varphi(u), \varphi(v))=B(u, v)$ $\forall u, v \in V$ (такой $\varphi$ мы и называем изометрией). \\

\begin{lemma}
    $V$ и $V'$ изометричны, тогда их радикалы изометричны. Более точно - $\Rad (V')=\varphi(\Rad (V))$, если $\varphi$ - изометрия. (напомним, радикал - ядро $B$ как отображения $V \rightarrow V^*$)
\end{lemma}

\begin{defn}
    \textit{Индуцированная линейная форма} - сужение $B$ на $U$ - подпространство $V$ (только пары из $U$). Если рассуждать в другом определении, то
    \[
        B\bigg|_U:U\rightarrow V\rightarrow V^* \rightarrow U^*.
    \]
\end{defn}

\begin{proof}
    Для начала, немного предшествующей информации. Если у нас есть произвольное $V$, тогда $B\bigg|_{\Rad(V)}(u, v)=0$, $u, v\in\Rad(V)$ (из определения радикала). \ 
    
    Проверим два включения. Для начала, $v\in \Rad (V)$, рассмотрим $\varphi(v)$. Нужно показать, что $B'(\varphi(v), u)$ - нуль. Но так как мы имеем дело с изоморфизмом, то $u$ - также образ чего-то, то есть, $\varphi(w)$. Тогда раскроем по изометрии, получим $B(v, w)=0$, так как $v\in \Rad(V)$. тогда $\varphi(v)\in \Rad(v')$. \ 

    Теперь в обратную сторону. Пусть $v \in \Rad(V')$ есть $\varphi(w)$, $w\in V$. Тогда $B(w, u)=B'(\varphi(w), \varphi(u))=B'(v, \varphi(u))=0$, то есть, $w\in \Rad(V)$.  
\end{proof}

\begin{defn}
    (\textit{Ортогональная сумма}). Пусть у нас есть два пространства $(V, B)$ и $(V', B')$. Тогда $V\perp V'$ - просто прямая сумма, а $B\perp B'((u, u'), (v, v'))=B(u, v)+B'(u' ,v')$. В частности, $B\perp B'((u, 0), (0, v'))=0$. 
\end{defn}

\begin{theorem}
    $(V, B)\simeq (V/\Rad(V)), \overline{B})\perp (\Rad(V), B\bigg|_{\Rad(V)})$. 
\end{theorem}

\begin{proof}
    Левая часть может быть представлена как вырожденное, так и невырожденной формой. Наша цель - свести всё к рассмотрению невырожденной формы. Первое славаемое правой части - невырожденное, второе - очевидно, вырожденное. Таким образом, мы покажем, что любая форма представляется в виде ортогональной суммы невырожденной формы с нулём.

    Посмотрим на ситуацию со стороны матриц Грама. Это означает, что нам нужно придумать базис, в которм матрица грама выглядит как куча нулей и квадратная матрица $\overline{\Gamma}$ в нижнем правом углу, причём определитель её не нуль. \ 

    Если у нас есть произвольная симметричная матрица Грама, то мы можем заменить её на эквивалентную $\Gamma \sim C^T\Gamma C$, $C$ - обратимая. Мы можем подобрать $C$, чтобы изначальная $\Gamma$ имела тот самый вид с матричкой в углу. \ 

    Теперь к доказательству. Возьмём сначала какое-то произвольное дополнение 
    \[
        V = \Rad(V)\oplus U
    \]
    (не ортогонально, просто прямая сумма). Давайте рассмотрим ограничение $B\bigg|_U$. Покажем, что $(U, B\bigg|_U)\simeq (V/\Rad(V), \overline{B})$. Отправим элемент $u$ в класс $[u]$ в $V/\Rad(V)$. Это должна быть биекция, проверим инъективность. Если $[u]=0$, то $u\in\Rad(V)$, но $u\in \Rad$ и в $U$, значит, $u=0$. Теперь сюръекция, рассмотрим любой элемент $[v]$, но $v$ тогда можно расписать как $v_1+u$, где первый элемент суммы лежит в радикале, а другой - в $U$. Тогда $[v]=[u]$. \ 

    Воспользуемся теперь определением радикала и покажем, что сохраняется билинейная форма. Нужно проверить, что $\overline{B}([u], [u'])=B(u, u')$. Для этого надо вспомнить, как мы определяется $\overline{B}$. Оно определяется как если у нас есть $V/\Rad(V)$ и мы отправляем его в $V/\Rad(V^*)$, а определяем само отображение есть взятие произвольного элемента $v$ и отправка его в $V^*$ (нихуя не видно на видео. мразь, которая записывала, решила взять в кадр всю доску и съебать, сука). $\overline{B}([u], [u'])=B([u])([u']) = B(u)(u') = B(u, u')$. То есть, по сути, это некая тавтология, а содержательная часть корректности $\overline{B}$ была доказана ранее. \ 

    Таким образом, у нас есть изометрия $(U, B\bigg|_U)\simeq (V/\Rad(V), \overline{B})$. К чему тогда сводится утверждение, которое мы доказываем? Оно сводится к тому, что мы одно слагаемое уже нашли, и осталось показать, что $\Rad(V)$ ортогонален $U$, тогда их сумма ортогональна, то есть, для любого $v\in \Rad(v)$ и $u\in U$, $B(v, u)=0$, но это, конечно, так, по определению радикала. 
\end{proof}

\begin{remark}
    Для понимания. Второй элемент ортогональный суммы классный, а первый нет. Мы хотим его поменять. Возьмём дополнение к радикалу в смысле векторных пространств, и покажем изометрию как в рассуждении. Потом покажем, что $U$ и $\Rad$ ортогональны и всё круто.
\end{remark}

Эта теорема позволяет почти все утверждения про билинейные симметричные формы достаточно проверять для случая невырожденных форм. Рассмотрим $\dim V = 1$. Как выглядит билинейная форма в этом случае? Матрица Грама есть просто скаляр, поэтому если у нас есть базис $e$, то $B(e\alpha, e\beta)= \alpha\beta B(e, e)$, и скаляром $B(e, e)$ всё определяется. Тогда это можно записать как $\alpha \beta a$, $\Gamma = a$, такую одномерную форму записывают как $\langle a \rangle$. Мы можем рассмотреть ортогональную сумму таких форм. Пусть у нас есть скаляры $a_1$, $a_2$. Тогда рассмотрим $\langle a_1 \rangle \perp \langle a_2 \rangle$. Какая у неё матрица Грама? Это матрица $2\times 2$ диагональная, на диагонали как раз $a_1$ и $a_2$. Ортогональная сумма двух пространств с матрицами Грама $\Gamma$ и $\Gamma'$ вообще как раз и будет иметь матрицу Грама блочного вида с блоками из матриц элементов суммы. Тогда мы можем получить $\langle a_1, \ldots, a_n\rangle $ индуктивно. Такая форма невырождена при условии, что $a_i$ ненулевые (действительно, определитель у диагональной матрицы посчитать не сложно). Ну вот мы и покажем, что любая форма представляется в таком виде. 

\begin{remark}
    Стоит вспомнить, что в наших рассуждениях характеристика поля не равна двум.
\end{remark}

\begin{theorem}
    (Теорема Лагранжа). Любая $(V, B)\simeq \langle a_1, \ldots, a_n \rangle$, где $n=\dim V$ для каких-то $a_i$. Если в терминах матриц, то для симметричной матрицы $\Gamma$ мы хотим найти матрицу $C$ такую, что $C^T\Gamma C$ - диагональная.
\end{theorem}

Прежде чем доказывать, нам понадобится лемма:\\ 

\begin{lemma}
    Пусть у нас есть $(V, B)$ и какое-то подпространство $U\leq V$. Предположим, что $B\bigg|_U$ - невырожденная форма, тогда $(V, B)\simeq (U, B\bigg|_U)\perp(W, B')$. 
\end{lemma}

\begin{proof}
    Явно построим $W=U^{\perp}= \{v\in V \: B(u, v)=0 \: \forall u\in U\}$ (все вектора, которые перпендикулярны всем из $U$) (нужно проверить, что это подпространство $V$). $B'=B\bigg|_{U^{\perp}}$. Осталось проверить, что $U$ ортогонально $W$, и что любой вектор может быть представлен как сумма какого-то вектора из $U$ и какого-то из ортогонального дополнения. \ 

    Проверим первое. Пусть у нас есть какое-то $u$, которое лежит в обоих множествах. Если он лежит в ортогональном дополнении, то $B(u, v)=0$ $\forall v\in U$, то есть $u\in \Rad(B\bigg|_U)=0$, противоречие. \ 

    Теперь проверяем второе. Пусть у нас есть $v\in V$, посмотрим на линейную форму $B(v, u)$ (первый элемент будет фиксирован), $u\in U$. Это - линейная форма на $U$. Иначе говоря, можно сказать, что мы смотрим на $B(v)\in V^*$, его можно ограничить на $U$, а $B(v)\bigg|_U\in U^*$, её мы и рассматриваем. Раз форма из условия невырождена, то она заадёт биекцию между $U$ и $U^*$. Поэтому $\exists u \in U$ такой, что $B(v)\bigg|_U=B(u)\bigg|_U$. Это означает, что $B(v, u)\bigg|_U=0$, то есть $v-u\in U^{\perp}$, теперь обзовём $v-u=w$, и получим, что $v=w+u$, где $w\in U^\perp$, а $u\in U$.
\end{proof}

\begin{proof}
    (Доказательство т. Лагранжа). Доказывать будем по индукции по размерности $V$. Первый случай - $B\equiv 0$, тогда $(V, B)=\langle 0, 0, \ldots, 0 \rangle$. \ 

    Второй случай. Пусть $B\neq 0$. Тогда $\exists v\in V$ такой, что $B(v, v)\neq 0$. Если бы это всегда были нулики, то применив разложение $B(u, v)$ в симметричные из прошлой лекции, получим, что она тождественный нуль. Посмотрим тогда на подпространство, натянутое на $v$ - $\langle v \rangle$ (одномерное). $B\bigg|_{\langle v \rangle}\simeq \langle B(v, v)\rangle$, причём невырождена. Применяем лемму, получаем, что $B\simeq B\bigg|_{\langle v\rangle}\perp B'$, а у $B'$ размерность уже меньше и можно применить индукционное предположение. 
\end{proof}

Вот мы и узнали, что все формы диагонализируемы. Сложность остаётся в проверке, изометричны ли две формы, которые, казалось бы, очень даже красиво выглядят. Сами $a_i$ не инвариантны в записи $\langle a_1, \ldots, a_n \rangle$. Зато их произведение инвариантно по модулю $(F^*)^2$, но это не очень полезно. Рассмотрим несколько частных случаев, начнём с $F=\CC$. \ 

Привели мы форму к диагональному виду, есть диагональная матрица. Однако мы всё ещё её можем сопрячь с какой-то матрицей $C$. А если $C$ будет диагональной, то если мы так сопряжём, то получим, что элементы диагонали матрицы домножились на квадраты, то есть, $a_i$ определены с точностью до квадратов. Тогда мы можем привести $\langle a_1, \ldots, a_n \rangle$ к сточке из нулей и единиц (если форма невырождена, то только из единиц и определяется только размерностью (исх. пр.) и рангом матрицы Грама). \ 

Посмотрим теперь на вещественные числа. Мы можем привести форму к строчке из $p$ -1, $q$ 1 и сколько-то нулей ($x_1^2+\ldots+x_p^2-x_{p+1}^2-\ldots - x_{p+q}^2$). \\

\begin{theorem}
    (Закон инерции Сильвестра). $p$ и $q$ - инварианты вещественной квадратичной формы. 
\end{theorem}

\begin{proof}

\begin{theorem}
    (Теорема Витта о сокращении). Допустим, у нас есть пространства $U$, $V$ и $V'$, и $V\perp U \simeq V'\perp U$, тогда $V\simeq V'$. 
\end{theorem}

\begin{proof}

    \begin{theorem}
        (Теорема Витта о продолжении). Пусть у нас есть $V$ и подпространство его $U$ такое, что $B\bigg|_U$ - невырожденное. Допустим, есть ещё какое-то подпространство $V'$ и $B\bigg|_U\simeq B_{U'}$, притом задана изометрия $\varphi$. Тогда существует $\tilde{\varphi}:V\rightarrow V$ изометрия такая, что $\tilde{\varphi}|_{U}=\varphi$. 
    \end{theorem}

    \begin{proof}
        Я запутался в рассказе В. Петрова и том, что из чего следует, короче, эта теорема следует из теоремы о сокращении. \ 

        У нас была лемма, что $V=U\perp W$ так как $B\bigg|_U$ - невырожденное. С другой стороны, можно рассмотреть $V$ как $U'\perp W'$ (по аналогичным причинам, $B\bigg|_{U'}\simeq B\bigg|_U$). Причём, $U\simeq U'$, изометрия $\varphi$. По теореме Витте о сокращении, $W\simeq W'$. Допустим, эта изометрия задаётся каким-то $\psi$. Определим $\tilde{\varphi}((u, w))= (\varphi(u), \psi(w))$, тогда понятно, что мы получили изометрию, так как это изометрия на слагаемых, а слагаемые ортогональны. А также она продолжает $\varphi$, потому что мы так построили ($\tilde{\varphi((u, 0))=(\varphi(u), 0)}$).
    \end{proof}

    \begin{defn}
        $O(V, B)$ - группа всех изометрий на себя (группа относительно композиции). В более конкретный терминах матрицы Грама, $O(V, B)=\{C\in GL(V)|C^T\Gamma C=\Gamma\}\leq GL(V)$. 
    \end{defn}

\end{proof}

Количество нулей - размерность радикала, поэтому инвариант. На нули можно сокращать, так как мы можем отфакторизовать по факториалу. Тогда если у нас есть 
\[
\langle a_1, \ldots, a_n, 0, \ldots, 0 \rangle
\]
и аналогичная форма, только с $b_i$, изометричная первой, то если мы выкосим все нули, то результаты также будут изометричны (тут можно не пользоваться теоремой о сокращении, но можно и воспользоваться). \ 

Предположим, $p$ и $q$ не инвариантны, рассмотрим две аозможные строки, тогда сократим столько -1, сколько сможем. Какая-то форма станет состоящей только из 1, тогда она положительно определена. Если мы возьмём ненулевой вектор, и значение $B$ от него два раза, то получим что-то, большее нуля. В случае другого представления можно взять какую-то $x^2-y^2+...$ (что-то, что нас не касается). Но если мы возьмём $x=y$, то вектор в этом случае обнулился, а другой - нет. Так нам и помогла теорема Витте.

\end{proof}

\end{document}
\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[unicode, pdftex]{hyperref}
\usepackage{cmap}
\usepackage{mathtext}
\usepackage{multicol}
\setlength{\columnsep}{1cm}
\usepackage[T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{icomma}
\usepackage{euscript}
\usepackage{mathrsfs}
\usepackage{geometry}
\usepackage{graphicx}
\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[usenames]{color}
\hypersetup{
     colorlinks=true,
     linkcolor=magenta,
     filecolor=magenta,
     citecolor=black,      
     urlcolor=magenta,
     }
\usepackage{fancyhdr}
\pagestyle{fancy} 
\fancyhead{} 
\fancyhead[LE,RO]{\thepage} 
\fancyhead[CO]{\hyperlink{t2}{к списку объектов}}
\fancyhead[LO]{\hyperlink{t1}{к содержанию}} 
\fancyfoot{}
\newtheoremstyle{indented}{0 pt}{0 pt}{\itshape}{}{\bfseries}{. }{0 em}{ }

\renewcommand\thesection{}
\renewcommand\thesubsection{}

%\geometry{verbose,a4paper,tmargin=2cm,bmargin=2cm,lmargin=2.5cm,rmargin=1.5cm}

\title{Матосновы алгоритмов}
\author{Кабашный Иван (@keba4ok) \\ на основе лекций А. С. Охотина и А. В. Тискина}
\date{22 января 2020 г.}

\theoremstyle{indented}
\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}
\newtheorem{alg}{Алгоритм}

\theoremstyle{definition} 
\newtheorem{defn}{Определение}
\newtheorem{exl}{Пример(ы)}
\newtheorem{prob}{Задача}

\theoremstyle{remark} 
\newtheorem{remark}{Примечание}
\newtheorem{cons}{Следствие}
\newtheorem{exer}{Упражнение}
\newtheorem{stat}{Утверждение}

\DeclareMathOperator{\la}{\leftarrow}
\DeclareMathOperator{\ra}{\rightarrow}
\DeclareMathOperator{\lra}{\leftrightarrow}
\DeclareMathOperator{\La}{\Leftarrow}
\DeclareMathOperator{\Ra}{\Rightarrow}
\DeclareMathOperator{\Lra}{\Leftrightarrow}
\DeclareMathOperator{\Llra}{\Longleftrightarrow}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Frac}{Frac}
\DeclareMathOperator{\Imf}{Im}
\DeclareMathOperator{\cont}{cont}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\ev}{ev}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\chard}{char}
\DeclareMathOperator{\CC}{\mathbb{C}}
\DeclareMathOperator{\ZZ}{\mathbb{Z}}
\DeclareMathOperator{\RR}{\mathbb{R}}
\DeclareMathOperator{\NN}{\mathbb{N}}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\Prop}{Prop}
\DeclareMathOperator{\LL}{\mathscr{L}}
\DeclareMathOperator{\KK}{\mathscr{K}}
\DeclareMathOperator{\form}{Form}
\DeclareMathOperator{\Pred}{Pred}
\DeclareMathOperator{\Func}{Func}
\DeclareMathOperator{\Const}{Const}
\DeclareMathOperator{\arity}{arity}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Term}{Term}
\DeclareMathOperator{\sub}{sub}
\DeclareMathOperator{\Sub}{Sub}
\DeclareMathOperator{\Atom}{Atom}
\DeclareMathOperator{\FV}{FV}
\DeclareMathOperator{\Sent}{Sent}
\DeclareMathOperator{\Th}{Th}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Eq}{Eq}
\DeclareMathOperator{\GA}{\mathfrak{A}}
\DeclareMathOperator{\GB}{\mathfrak{B}}
\DeclareMathOperator{\GC}{\mathfrak{C}}
\DeclareMathOperator{\GD}{\mathfrak{D}}

\begin{document}

\newcommand{\resetexlcounters}{%
  \setcounter{exl}{0}%
} 

\newcommand{\resetremarkcounters}{%
  \setcounter{remark}{0}%
} 

\newcommand{\reseconscounters}{%
  \setcounter{cons}{0}%
} 

\newcommand{\resetall}{%
    \resetexlcounters
    \resetremarkcounters
    \reseconscounters%
}

\maketitle 

\newpage

\url{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/} - источник с конспектами А. С. Охотина, где всё в тысячу раз подробнее и грамотнее. \\

\hypertarget{t1}{Основные моменты}. 
\tableofcontents

\newpage

\section{Билеты.}

На каждом билте можно нажимать на то, куда вам надо (кроме того, что с первой и восьмой лекции, пришлите мне, пожалуйста, кто-нибудь, записи с них). К билету относится то, что от метки, по которой вы перешли, и до слова \textbf{подробнее}, ссылка рядом с которым перекинет вас на подробный конспект Охотина, который этот билет содержит (иногда билет на соседних лекциях, тогда два \textbf{подробнее}, соответственно). \\

1. \hyperlink{d-1}{Понятие алгоритма. Псевдокод. Хранение переменных при рекурсии. Машина с произвольным доступом к памяти.} \\

2. \hyperlink{t-1}{Метод «разделяй и властвуй». Быстрое умножение Карацубы, анализ времени работы.} \\ 

3. \hyperlink{t-3}{Сортировка вставкой, сортировка слиянием.} \hyperlink{d1.5}{Куча, действия над нею, сортировка кучей.} \\ 

4. \hyperlink{r1}{«Быстрая сортировка», среднее время работы.} \\ 

5. \hyperlink{t2.5}{Нижняя оценка числа сравнений при сортировке. Сортировка подсчётом. Поразрядная сортировка.} \\ 

6. \hyperlink{t4}{Нахождение i-го по величине элемента массива.} \\ 

7. \hyperlink{e1}{Метод динамического программирования. Задача о разделении стержня. Задача о порядке умножения матриц.} \\ 

8. \hyperlink{d2}{Нахождение наибольшей общей подпоследовательности: «народный» алгоритм, алгоритм Хиршберга.} \\ 

9. \hyperlink{t10}{Поиск в ориентированном графе: поиск в ширину, поиск в глубину, топологическая сортировка, нахождение компонентов сильной связности.} \\ 

10. \hyperlink{t14}{Кратчайшие пути в графе с весами: алгоритм Беллмана–Форда, алгоритм Дейкстры. Очередь с приоритетами и её реализация.} \\ 

11. \hyperlink{t17}{Нахождение минимального остовного дерева графа: алгоритм Прима, алгоритм Крускала.} \\

12. \hyperlink{t20}{Лес непересекающихся множеств, нахождение компонентов связности в неориентированном графе.} \\ 

13. \hyperlink{t22}{Кратчайшие пути в графе между всеми парами вершин. Транзитивное замыкание матриц. Алгоритм Варшалла. Кратчайшие пути с весами, алгоритм Флойда–Варшалла. Использование умножения матриц.}  \\ 

14. Быстрое умножение матриц, алгоритм Штрассена. (это я не стал переписывать, сразу смотрите в \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекции на странице 5})\\ 

15. \hyperlink{t25}{Основная теорема о времени работы рекурсивных алгоритмов.} \\ 

16. \hyperlink{e2}{Быстрое умножение булевых матриц через числовые. Метод четырёх русских.} \\ 

17. \hyperlink{d13}{Структуры данных для представления множеств: вектор, список, двоичное дерево поиска. Основные операции (поиск, вставка, удаление, следующий, предыдущий, наибольший, наименьший), сложность их реализации. АВЛ-деревья. Реализация операций над ними, их сложность.} \\ 

18. \hyperlink{t29}{B-деревья. Реализация операций над ними, их сложность.} \\ 

19. \hyperlink{t30}{Полиномиальное хэширование строк. Алгоритм Рабина–Карпа. Нахождение наибольшей общей подстроки. Нахождение самого длинного палиндрома.}  \\ 

20. \hyperlink{e3}{Алгоритм Кнута–Морриса–Пратта. Конечные автоматы, реализация алгоритма Кнута–Морриса–Пратта на конечном автомате.} \\ 

21. \hyperlink{t32}{Префиксное дерево. Алгоритм Ахо–Корасик. Его реализация на конечном автомате.} \\ 

22. Префиксное дерево для множества строк. Построение упрощённого суффиксного дерева для строки. Суффиксное дерево и его применение. \\ 

23. Суффиксное дерево для строки, алгоритм Укконена.  \\ 

24. Сжатие данных методом Хаффмана. Арифметическое кодирование. \\ 

\texttt{Билеты 22-24 см. в \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l8.pdf}{8 лекции}} \\

25. \hyperlink{t35}{Сжатие данных со словарём, метод Лемпеля–Зива (LZ77, LZ78).} \\ 

26. \hyperlink{t37}{Преобразование Берроуза–Вилера, его реализация.}

\newpage

\section{Лекция 1.} 

\subsection{Билет 1.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d-1}{\textit{Алгоритм}}} - понятие, которое мы формально не определили, но согласно википедии, это - конечная совокупность точно заданных правил решения некоторого класса задач или набор инструкций, описывающих порядок действий исполнителя для решения определённой задачи. 
\end{defn}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d-2}{\textit{Псевдокод}}} - удобный способ записи алгоритма, который, конечно, невполне формален.
\end{defn}

\begin{stat}
    Рассмотрим машину с произвольным доступом памяти (RAM). Это - абстрактная модель вычисления, которая состоит из ячеек с памятью (рисуем их в столбик и нумеруем целыми числами). Программа состоит из команд, и каждая команда имеет номер (скажем вообще, что они называются $x_k$). Пусть первая команда у нас пересылать что-то куда-то (\textit{оператор присваивания} $A=B$, где $B$ может быть константой, прямой адресацией (какая-то ячейка), или косвенной адресацией $x_{x_n}$, а $A$ может быть всем этим же, кроме константы). Следующее, что нам нужно арифметические действия $A=B*C$, где $*\in\{+, -, \cdot, /, \div\}$. \ 

    Добавим также безусловный переход GOTO $n$ - переход к строке программы, аналогично можно задать GOTO $x_n$, которая может пересылать к строчке команды, которая имеет номер, хранящийся в этой ячейке. Также нам нужен условный оператор {IF; THEN}, и в конец добавим оператор HALT, которая будет говорить остановиться, и в целом, всё, что можно, мы уже умеем делать с помощью такой тривиальной вещи. \ 
\end{stat}

\begin{alg}
    Рассмотрим \textcolor{magenta}{\hypertarget{d-3}{\textit{рекурсию}}}, а конкретнее, как мы храним значения. \textcolor{magenta}{\hypertarget{d-4}{\textit{Стэк}}} - абстрактная структура данных, у которой есть дно, вершина, и мы всегда можем положить элемент наверх, или вынести его. Для рекурсии стэк состоит из нескольких stack-frame-ов, которые соответствуют каждому вызову рекурсивной программы, которые могут содержать ячейки со значениями, номерами строк или номерами вызова.
\end{alg}

\subsection{Билет 2.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-1}{\textbf{Быстрое умножение Карацубы}}}. Система идей, которая позволяет значительно ускорить умножение (скажем, двоичных) чисел. \ 

    Идея первая: при умножени двузначных чисел, вместо того, чтобы совершать 4 умножения и нескольких сумм, совершим 3 умножения и несколько сумм и разностей. Пусть у нас есть числа $a_1a_0$ и $b_1b_0$. Тогда рассмотрим $(a_1+a_0)(b_1+b_0)$ и вычтем $(a_1b_1+a_0b_0)$, в итоге получим, что из результатов у нас выйдет собрать искомую сумму четырёх изначальных произведений. \ 
     
    Идея вторая: вообще, как мы могли заметить, нигде сильно нас не волновало, в какой системе счисления мы работали. Расмотрим большие числа $a_{n-1}\ldots a_{\frac{n}{2}}a_{\frac{n}{2}-1}\ldots a_0$ и $b_{n-1}\ldots b_{\frac{n}{2}}b_{\frac{n}{2}-1}\ldots b_0$ (двоичные), и разделим каждое из них пополам. Будем считать половины цифрами, тогда вот мы и свели к первой идее. \ 

    Наконец, идея третья: для умножения $\frac{n}{2}$-значных чисел мы используем тот же самый алгоритм, вызывая его рекурсивно. 
\end{alg}

\begin{proof}
    Пусть у нас есть 2 $n$-значных числа, которые алгоритм $K$ принимает на вход. Тогда у нас есть $C_1$ - $K$ от первых половин, $C_2$ - $K$ от вторых половин, и $C_3$ - $K$ от сумм цифр пары чисел. В итоге мы получаем $D=C_3-C_1-C_2$, и ответ $C_2 D C_1$. Нам нужно понять что-то про $T(n)$ - время на $n$-значных числах, $T(n)=3T(\frac{n}{2})+O(n)$. $3^i$ раз мы вычисляем $K$ от $\frac{n}{2^i}$-значных чисел, суммарно по этажам мы используем $\sum_{n=0}^{\log_2 n}C 3^i \frac{n}{2^i}$, тчо можон допреобразовать к $Cn\frac{(\frac{3}{2})^{\log_2 n+1}-1}{\frac{3}{2}-1}=Cn^{\log_2 3}$.
\end{proof}

\begin{alg}
    В ходе рассуждений мы коснулись идеи \textcolor{magenta}{\hypertarget{t-2}{\textbf{метода ''разделяй и властвуй''}}}, который состоит в том, что мы разбиваем большую задачу на несколько подзадач, которые ещё и можно распараллелить, если так можно, и таким образом, решать меньшие подзадачи вместо решения одной объёмной.
\end{alg}

\subsection{Билет 3.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-3}{\textbf{Сортировка вставкой}}}. Пусть у нас есть массив $a_i$. Рассмотрим первый элемент, он отсортирован. Рассмотрим теперь два элемента; они либо отсортированы, либо нет, тогда мы их меняем. И теперь работаем, фактически, по индукции. 
\end{alg}

\begin{stat}
    Работает алгоритм за время $n^2$, причём зависит он от входных данных довольно сильно.
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t-4}{\textbf{Сортировка слиянием}}}. Разбиваем массив на две части, после чего отсортируем каждую половину и сольём. Делается это с помощью дополнительной памяти, ещё одного массива такого же размера, в который мы будем заносить итоговый результат слияния. Для самого слияния нам также нужны будут нужны указатели, которые двигаются и показывают на элементы, которые мы сравниваем. Тот, который получился меньше, заносится в новый массив, и указатель над ним сдвигается.
\end{alg}

\begin{stat}
    Работает алгоритм за $n\log n$, раз уж $\sum_{i=0}^{\log_2 n} C 2^i \frac{n}{2^i}=c n \log_2 n$. 
\end{stat}

\section{Лекция 2.}

\subsection{Быстрая сортировка.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{r1}{\textbf{Быстрая сортировка}}}. Выбираем \textcolor{magenta}{\hypertarget{d1}{\textit{опорный элемент}}}, с которым сравниваем все остальные элементы (на это уходит линейное время). Затем рекурсивно работаем с тем, что справа от него и слева от него.
\end{alg} \ 

\begin{theorem}
    Если все элементы массива различны и опопрный элемент выбирается случайно, то среднее время работы алгоритма - $\Theta(n\log n)$.
\end{theorem}

\begin{proof}
    Время работы пропорционально числу сравнениий между элементами. Расматриваем два элемента $y_i$ и $y_j$, $i<j$, тогда они сравниваются только, если выбран один из них в качестве опорного. Если будет выбран какой-то $y_k$, $i<k<j$, то они никогда больше не будут сравнены, если что-то на отрезке не между ними - плевать, относительно отрезка между ними ничего не поменялось. Тогда среднее количество сравнений между этими элементами: 
    \[
        \frac{2}{j-i+1}. 
    \]
    Тогда всего среднее количество сравнений: 
    \[
        \sum_{i=1}^{n-1}\sum_{j=i+1}^n \frac{2}{j-i+1} = \sum_{i=1}^{n-1}\sum_{k=1}^{n-1} \frac{2}{k+1}= O(n\log n).
    \]
    Последняя оценка получается из
    \[
        \sum_{k=1}^n\frac{1}{k} \approx \int_1^n \frac{1}{x}dx = \ln n. 
    \]
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 1}

\subsection{Сортировка кучей.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d1.5}{\textit{Куча}}} - двоичное дерево, представленное в виде массива. В этом дереве различаются левый и правый потомок каждой вершины, и потому вершины на каждом уровне упорядочены. Дерево почти сбалансировано: на каждом $i$-м уровне, кроме, может быть, последнего, есть все $2^i$ вершин.
\end{defn}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t2}{\textbf{Сортировка кучей}}}. Для начала, мы строим дерево: записываем по порядочку все вершины так, что у вершины $x_i$ потомки - $x_{2i}$ и $x_{2i+1}$. Затем начинаем на все вершины, кроме висячих смотреть и делать вот что: если она меньше потомка, то поменяем с ним (если меньше обоих, то с меньшим). Так доведём её до куда сможем, и продолжим рпссмотрение для оставшихся невисячих вершин (в изначальном дереве). Так сверху окажется наименьшая вершина, вынесем её, затем - по индукции.
\end{alg}

\begin{stat}
    Работает за $O(n\log n)$ (построение дерева - $O(\log n)$, вынесение вершин - $O(n)$), можно разогнать оценку до $2n$. 
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 3}

\subsection{Скорость сортировки.} 
 
\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t2.5}{\textbf{Нижняя оценка скорости}}}).Всякий алгоритм сортировки, основанный на сравнении, требует $\Omega (n\log n)$ операций сравнения.
\end{theorem}

\begin{proof}
    Построим дерево того, как мы спускаемся к определению последовательности, его высота ограничивается $\log_2n!$, оценим факториал $\biggl(\frac{n}{e}\biggr)^n<n!<n^n$, а после - оценим через это логарифм $n\log_2n - O(n)$. 
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t3}{\textbf{Сортировка подсчётом}}}. Если у нас есть массив из конечного обозримого количества типов элементов, можно сначала посчитать количество первого, затем количество второго, и так далее. Время работы - $O(n+k)$, где $k$ - количество типов, $n$ - количество переменных.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t4}{\textbf{Поразрядная сортировка}}}. Сортируем числа сначала по первому разряду, затем по второму, и так далее\dots Время работы: $O(l(n+k))$, где сравниваются строки длины $l$, алфавит из $k$ символов.
\end{alg}  \ 

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 4}

\subsection{Нахождение $i$-го по величине элемента массива.} 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t4}{\textbf{Нахождение $i$-го элемента}}}. Делим массив на пятёрки подряд идущих элементов (возможно, последняя пятёрка будет неполной). Теперь в каждой пятёрке выделяем медианы, и смотрим на медиану медиан. Сделаем её опорным элементом и как в быстрой сортировке, раскидаем всё по сторонам. Если этот элемент под номером $i$, то мы его нашли, иначе - действуем рекурсивно с одной из сторон. Время работы - линейное.
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 8}

\subsection{Метод динамического программирования.}

\hypertarget{e1}{технические трудности}

\begin{prob}
    Имеется стержень длины $n$. Продав стержень длины $i$, можно выручить $p_i$ денежных единиц. Как выгоднее всего распилить имеющийся стержень?
\end{prob}

\begin{alg}
    Начинаем с первого, и делаем полный перебор. Говнище
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t6}{\textbf{Жадный алгоритм}}}. Отпиливаем самый дорогой кусок, затем опять самый дорогой из возможных, и так далее. Не самый оптимальный.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t7}{\textbf{Метод динамического программрования}}}. Суть этого метода такова. Пусть на каждом шаге надо сделать выбор (принять решение). Известно, что какой-то выбор приводит к оптимальному результату. Этому выбору соответствует некий набор подзадач. Тогда сперва находятся ответы для всех подзадач данной задачи, возникающих при различном выборе, после чего, имея все эти ответы перед глазами, можно будет в каждом случае сделать наилучший выбор.
\end{alg}

\begin{exl}
    Пусть стержень длины 0 не стоит нисколько. Для $j$ от 1 до $n$ пока не найдено никаких способов продать стержень, для всякой длины отрезаемого куска, сложим его цену с выручкой за остаток. Если так можно выручить больше известного, то цена стержня длины $j$ улучшается, и так рекурсивно мы дойдём до получения цены за весь стержень. 
\end{exl}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l2.pdf}{2 лекция, страница 11}

\section{Лекция 3.}

\subsection{Продолжение динамического программирования.}

\begin{prob}
    Пусть нужно умножить $n$ матриц $M_1 \times \ldots \times M_n$. В силу ассоциативности, скобки можно расставить как угодно. От их расстановки зависит общее число операций, и, чтобы умножить матрицы быстрее, надо заранее определить наилучший порядок их умножения.
\end{prob}

\begin{exl}
    Строим верхнедиагональную матрицу $T$, в которой $T_{i, j}$ - наименьшее число действий, необходимых для вычисления $M_{i+1} \times \ldots \times M_n$. \ 

    Внешний цикл по длине куска $l=j-i$, второй - по $i$, во внутреннем перебируются все разбиения произведения на два, и вычисляется следующее значение:
    \[
        T_{i, j}=\min_{k=-+1}^{j-1}(T_{i, k}+T_{k, j}+m_im_km_j). 
    \]
    Разбираем так все по порядку и вычисляем наилучший способ. Время раюоты: $O(n^3)$ - строим таблицу, далее - $2n-1$ вызовов процедуры перемножить $(i, j)$, в каждом - $O(n)$ итераций цикла. И ещё само умножение матриц.
\end{exl}

\begin{remark}
    Для простого понимания - простой пример с кузнечиком, который прыгает на 1 или 2, и ему нужно пропрыгать $n$, сколькими способами это можно сделать? Мы заводим массив $dp[i]$ длины $n$ (кол-во способов добраться до $i$), тогда $dp[i]=dp[i-1]+dp[i-2]$, и так насчитываем все значения, находим ответ для $n$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 1}

\subsection{Нахождение наибольшей общей подпоследовательности.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d2}{\textit{Строкой над алфавитом}}} $\Sigma$ называется всякая конечная последовательность $w=a_1\ldots a_l$, где $l\geq 0$, и $a_1, \ldots, a_l \in \Sigma$ - символы.
\end{defn}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t8}{\textbf{Народный алгоритм}}}. Динамический способ нахождения наибольшей общей подпоследовательности. Заводим таблицу $T$ и в ячейке $T_{i, j}$ записываем длину наибольшей общей подпоследовательности на префиксах длины $i$ и $j$ первого и второго слова соответственно. Заполняем таблицу последовательно от более коротких мар до самых длинных, и в итоге получим ответ в задаче. \ 

    Строим таблицу так: берём $T_{i, j}$. Если у них одинаковые последние элементы, то получим $T_{i-1, j-1}+1$. Если они разные, то $\max({T_{i-1, j}, T_{i, j-1}})$. \ 

    Саму последовательность элементов потом восстанавливаем с конца понятно как. Недостаток в том, что чтобы найти подпоследовательность, нужно хранить всю таблицу, а это $O(mn)$, и это много. Однако, если нужна только длина, то можно ограничиться лишь двумя столбцами (или двумя строчками).
\end{alg} \

\begin{alg}
    \textcolor{magenta}{\hypertarget{t9}{\textbf{Алгоритм Хиршенберга}}}. Построение наибольшей общей подпоследовательности за время $O(mn)$, используя память $O(\min(m, n))$. Пусть $u=u'u''$ - некоторое разбиение $u$. Тогда оптимальное совмещение $u$ и $v$ совмещает $u'$ с каким-то начальным куском $v$ - пусть это $v'$, и $u''$ - с остатком $v''$. Нужно найти это разбиение $v=v'v''$, чтобы потом отдельно запустить совмещение двух соответствующиих пар кусков. \ 

    Алгоритм делит $u$ на две подстроки примерно равной длины. Сперва динамическим программированием находится последняя строчка таблицы $T^{u', v}$, как в базовом алгоритме. Её $j$-ый элемент содержит длину наибольшей общей подпоследовательности $u'$ и $u_j$ - префикса длины $j$. Аналогично находится последняя строка таблицы $T^{(u'')^R, v^R}$ ($R$ - reverse). Дальше складываем таблицы поэлементно и ссмотрим, где достигается максимум - это и есть искомое разбиение $v=v'v''$. Для этого вычисления алгоритс использовал $O(|v|)$ ячеек памяти, которые теперь можно освободить. \ 

    Далее алгоритм вызывается рекурсивно, чтобы вычислить лучшее совмещение $u'$ и $v'$, и $u''$ и $v''$. Полученные совмещения последовательно приписываются друг к другу.
\end{alg} \

\begin{theorem}
    Алгоритм Хиршберга работает за время $O(mn)$. 
\end{theorem}

\begin{proof}
    Принимая за единицу времени время, затрачиваемое на вычисление значение одного элемента $T_{i, j}$ в ''народном'' алгоритме, утверждается, что в общей сложности будет выполнено не более, чем $2mn$ шагов. \ 

    Пусть $f(m, n)$ - время работы в наихудшем случае. Тогда индукцией по $m$ и $n$ доказывается неравенство $f(m, n)\leq 2mn$. При запуске на строках $u$ и $v$, где их мощности соответственно равны $m$ и $n$, вычисление таблицы $T_{u', v}$ займёт $\frac12 mn$ шагов, и за столько же шагов будет вычисляться таблица $T^{(u'')^R, v^R}$. После этого проводятся два рекурсивных вызова, один из которых занимает $f(\frac{m}{2}, k)$ шагов, а другой - $f(\frac{m}{2}, n-k)$ шагов, для некоторого $k$. Время раюоты рекурсивных вызовов оценивается по предположению индукции, откуда получается оценка того же вида для $f(m,n)$. 
    \[
        f(m, n)=2\cdot \frac12 mn+\max_k (f(\frac{m}{2}, k)+f(\frac{m}{2}, n-k))\leq mn+\max_k(mk+m(n-k))=2mn
    \]
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 3}

\subsection{Поиск в ориентированном графе.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t10}{\textbf{Поиск в ширину (BFS)}}}. В каждый момент времени вершина графа может быть помечена или не помечена. Если вершина уже помечена, значит алгоритм нашёл путь из корня в неё. Кроме пометок на вершинах, алгоритм хранит очередь, в которой находятся все те помеченные вершины, для которых ещё не обработаны исходящие дуги. Таким образом, в каждый момент времени вершина может быть не помеченнной, помечанной и обработанной, и помечанной и необработанной. Идём из корня и последовательно отмечаем и заносим в очередь тех, к кому пришли.
\end{alg}

\begin{stat}
    В каждый момент времени очередь состоит из некоторых вершин, находящихся на расстоянии $l$ от $s$, вслед за которыми идут некоторые вершины, находящиеся на расстоянии $l+1$ от $s$. При этом все вершины на расстоянии, меньшем, чем $l$, уже обработаны, ровно как и все вершины на расстоянии $l$, не вошедшие в очередь. Из вершин на расстоянии $l+1$ в очереди есть ровно все потомки обработанных вершин.
\end{stat}

\begin{stat} \ 
    \begin{itemize}
        \item алгоритм помечает врешину $v$ тогда и только тогда, когда есть путь тз $s$ в $v$; 
        \item если алгоритм находит $v$ по дуге $(u, v)$, то один из кратчайших путей из $s$ в $v$ идёт через $u$; 
        \item все пройденные дуги $(u, v)$ образуют дерево.
    \end{itemize}
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t11}{\textbf{Поиск в глубину (DFS)}}}. Ижём в глубину до конца, отмечаем вершины в чёрный, если из них начали идти вниз, серым, если они нам просто встретились на пути. После того, как дошли до конца, идём наверх до первой вершины. Время работы: $O(|V|+|E|)$. 
\end{alg}

\begin{prob}
    \textcolor{magenta}{\hypertarget{t12}{\textbf{Топологическая сортировка}}}. Нужно найти остовные деревья в орграфе (естественно, он должен быть ациклическим). Решается через DFS из следующих утверждений. 
\end{prob}

\begin{stat}
    Граф ациклический тогда и только тогда, когда при поиске в глубину никогда не рассматривается дуга, ведущая в вершину, находящуюся в стеке возврата (дуга из серой в серую).
\end{stat}

\begin{stat}
    Если в ациклическом графе есть дуга $(u, v)$, то время завершения $v$ ментше, чем время завершения $u$. 
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l3.pdf}{3 лекция, страница 6}

\section{Лекция 4.}

\subsection{Окончание поисков в орграфе.}

\begin{prob}
    Найти в данном графе его компоненты сильной связности. 
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t13}{\textbf{Алгоритм Косараджу-Шарира}}}. Спервая запускается поиск в глубину для $G$, а затем запускается поиск в глубину для обращённого графа $G^R$, в котором направления всех дуг изменены на обратные (однако, компоненты связности те же). При поиске в глубину в обращённом графе, во внешнем цикле вершины рассматриваются в порядке их завершения при первом поиске в глубину, от конца к началу. После этого оказывается, что каждый запуск процедуры DFS во внешнем цикле будет находить очередной сильно связный компонент исходного графа. Время работы: $O(|V|+|E|)$, корректность обосновывается следующим:
\end{alg}

\begin{stat}
    Пусть в графе $G$ есть сильно связные компоненты $C$ и $D$, и есть дуга $(u, v)\in E$ из $C$ в $D$. Тогда при поиске в глубину в графе $G$ самое позднее время завершения вершины в $C$ превосходит таковое в $D$. 
\end{stat}

\begin{proof}
    Рассматриваются два случая: самое ранне обнаружение в $C$ меньше, чем в $D$, тогда рассматриваем первую обнаруженную вершину $x\in C$, у всех вершин из $D$ время окончания меньше, чем у неё. Если же это не так, то рассмотрим самую раннюю $y\in D$. Все остальные из этой компоненты будут обнаружены на рекурсивных вызовах, а из $C$ на этом этапе не обнаружатся, поэтому время окончания всех вершин из $C$ больше. 
\end{proof}

\begin{theorem}
    Вершины каждого дерева, найденного алгоритмом Косадаржу-Шарира при втором поиске в глубину - это и есть сильно связные компоненты исходного графа. 
\end{theorem}

\begin{proof}
    Индукция по количеству найденных компонент связности. Надо доказать, что если первые $k$ найденных компонент связности действительно таковы, то и следующая также обладает этим свойством.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 2} 

\subsection{Поиск в алгоритме с весами.}

\begin{prob}
    Пусть в орграфе для каждой дуги задан вес. Нужно найти пуи наименьшего веса из данной вершины $s\in V$ во все вершины графа.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t14}{\textbf{Алгоритм Беллмана-Форда}}}. Для каждой вершины вычисляются значения $d_v$ - наименьший вес пути из $s$ в $v$ и $\pi_v$ - предыдущая вершина на пути наименьшего веса из $s$ в $v$. Изначально полагается, что $d_v=\infty$ и $\pi_v=\text{NULL}$ для всех вершин, и $d_s=0$. Далее алгоритм постепенно находит пути меньшего веса в другие вершины, запоминая веса лучших из найденных путей в этих переменных, Значения уменьшаются с помощью элементарной операции улучшения пути, используя некоторую дугу $(u, v)\in E$. Если $d_u+w_{u, v}<d_v$, то $d_v=d_u+w_{u, v}$, а $\pi_v=u$. Эта операция применяется, пока можно что-то улучшить. Как будет показано, для этого достаточно рассмотреть все дуги $|V|-1$ раз.
\end{alg}

\begin{stat}
    После $i$-ой итерации внешнего цикла алгоритм Беллмана-Форда находит все пути наименьшего веса длины не более чем $i$. 
\end{stat}

\begin{proof}
    Индукция по $i$. 
\end{proof}

\begin{theorem}
    Алгоритм Беллмана-Форда за $|V|\cdot|E|$ шагов или правильно вычисляет пути наименьшего веса из вершины $s$ во все вершины, или сообщает о наличии достижимого цикла отрицательного веса. 
\end{theorem}

\begin{proof}
    Рассмотрим систему после $|V|-1$ итераций, и согласно утв. 7, нейдены все пути наименьшего веса, состоящие не более чем из $|V|-1$ дуг. Если какой-то путь ещё можно улучшить, то в этом пути какая-то вершина встретилась дважды, следовательно, найден цикл отрицательного веса. Если же ничего нельзя улучшить, то любой достижимый цикл будет иметь неотрицательный вес, так как для последовательный верщин цикла можно записать не условие улучшения и сложить по все парам.
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t15}{\textbf{Алгоритм Дейкстры}}}. Этот алгоритм решает ту же задачу, однако на каждом шаге находит очередную вершину $u$, путь наименьшего веса в которую уже известен. Тогда плгоритм рассматривает все дуги, исходящие из вершины $u$. Это позволяет ему рассматривать каждую дугу графа лишь однажды.
\end{alg} \

\begin{theorem}
    Алгоритм Дейкстры работает правильно.
\end{theorem}

\begin{proof}
    Индукцией по длине вычисления доказываем, что для всякой вершины $u\notin Q$, путь наименьшего пути уже построен. Для этого рассматриваются две вершины на кратчайшем пути - одна в среди рассмотренных, другая - среди не рассмотренных ($Q$), и для последней проводятся вычисления, связанные с длиной самого короткого пути для неё.
\end{proof}

\begin{remark}
    Для представления множества $Q$ алгоритм использует особую структуру данных: \textcolor{magenta}{\hypertarget{d3}{\textit{очередь с приоритетами}}}. Каждый элемент $Q$ находится там вместе со своим текущим значением $d_v$. Также заданы операции: 

    \begin{itemize}
        \item $insert(x)$ - вставить новый элемент; 
        \item $min()$ - выдать минимальный элемент; 
        \item $extract_{min}()$ - выдать минимальный и удалить; 
        \item $decrease(x, k)$ - изменить значение элемента $x\in Q$ на $k$, если $k$ меньше текущего значения $x_i$. 
    \end{itemize}

    Скорость работы алгоритма: $|V|$ раз $extract_{min}$ и $|E|$ раз $decrease$, поскольку каждая дуга обрабатывается лишь однажды. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 3}

\subsection{Окончание поиска с весами.}

Сложность алгоритма Дейкстры зависит от того, как реализвана очередь с приоритетами. Тупая реализация: хранить массив $x_v$, индекцированный по $v\in V$. Тогда $decrease$ работает за $O(1)$, но $extract_{min}$ требует время $|V|$. Отсюда общее время работы - $|V|^2+|E|=O(|V|^2)$. \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t16}{\textbf{Улучшение Дейкстры кучей}}}. Используется куча, в которой значение в каждой внутренней вершине не больше, чем значение в любом из её потомков (min-heap). Операции снатовятся следующими:

    \begin{itemize}
        \item $insert(x)$ - вставить новый элемент (он становится листом), после чего дать ему всплыть до его законного места; 
        \item $min()$ - выдать минимальный элемент (просто вернуть $x_1$); 
        \item $extract_{min}()$ - переместить $x_n$ в $x_1$, убрав его в конце, а затем запустить исправление кучи из корня, то есть, heapify(1); 
        \item $decrease(i, k)$ - изменить значение элемента $x_i$ на $k$, после чего дать элементу $x_i$ всплыть наверх, пока возможно. 
    \end{itemize}
\end{alg}

\begin{remark}
    \textcolor{magenta}{\hypertarget{d4}{\textit{Дать всплыть}}} - значит, если $x_i$ меньше своего родителя, то он обменивается так до тех пор, пока не займёт положенное место.
\end{remark}

\subsection{Нахождение минимального остовного дерева.}

\begin{prob}
    Дан неориентированный связный граф, рёбрам которого сопоставлены числа - веса. Нужно найти одно из остовных деревьев с наименьшим весом.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t17}{\textbf{Общий принцип действия алгоритмов}}}. Одно за другим присоединяются рёбра к поддереву какого-то минимального, чтобы опять получилось поддерево минимального.
\end{alg}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d5}{\textit{Сечение}}} графа - разбиение множества вершин на два дизъюнктныхмножества. Ребро \textcolor{magenta}{\hypertarget{d6}{\textit{пересекает сечение}}}, если один из его концов лежит в одной части, а другой - во второй. 
\end{defn}

\begin{stat}
    Пусть $T$ - одно из минимальных остовных деревьев графа, а $F$ - его подмножество. Пусть также имеется какое-то сечение, что никакое ребро из $FF$ его не пересекает. Тогда ребро с наименьшим весом, пересекающее это сечение, принадлежит некоторому минимальному остовному дереву $T'$, которое также содержит $F$. 
\end{stat}

\begin{proof}
    Если $(u, v)$ - такое ребро, входит в $T$, то нам подойдёт $T$. Если его там нет, то рассмотрим цикл, который с нима полчуается и поменяем его на другое ребро из цикла, которое пересекает сечение.
\end{proof}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t18}{\textbf{Алгоритм Прима}}}. Начинаем с произвольной вершины и на каждом шаге добавляем ребро из одной из уже имеющихся вершин в некоторую незадействованную (естественно, из всевозможных рёбер выбирается то, у которого минимальный вес). \ 
    
    Время работы: $|E|\log |V|$, так как выполняется $|V|$ операций внешнего цикла. Операции над очередью с приоритерами - $\log|V|$. Внутренний цикл по $v$: за всё время работы алгоритма каждое ребро рассматривается дважды: с одного конца и с другого. Поэтому тело цикла в общей сложности выполняется $2|E|$ раз, и всякий раз выполняется операция над очередью с приоритетами за время $O(\log |V|)$. 
\end{alg}

\begin{remark}
    Структура данных алгоритма - очередь с приотитетеми, в которой хранятся все вершины, ещё не попавшие в дерево. Значение $d_v$ каждойвершины $v$ - это наименьший вес ребра, соединяющий её с деревом. Также для $v$ запоминается вершина $\pi_v$, через которую $v$ соединена с деревом ребром наименьшего веса. \ 

    Всякий раз, когда в дерево добавляется новая вершина $u$, для любой вершины $v$ не из дерева моэет оказаться, что ребро $(u, v)$ легче, чем ранее известное ребро наименьшего веса $(\pi_v, v)$, соединяющее её с деревом, и это тка, если $w_{u, v}<d_v$. В этом случае значения $d_v$ и $\pi_v$ обновляются, переключая $v$ на соединение с деревом через $u$. 
\end{remark}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t19}{\textbf{Алгоритм Крускала}}}. Текущее подмножество остовного дерева - лес, соединяем компоненты самыми лёгкими путями.
\end{alg}

\begin{theorem}
    На каждом шаге работы алгоритма Крускала переменная $T$ содержит подмножество одного из остовных деревьев минимального веса.
\end{theorem}

\begin{proof}
    Индукция по длине вычисления.
\end{proof}

\begin{stat}
    При использовании леса непересекающихся множеств алгоритм работает за время $|E|\log |E|$ - и, стало быть, $|E|\log |V|$. 
\end{stat}

\begin{proof}
    Сортировка займёт время $|E|\log |E|$. Далее алгоритм выполняет $3|E|$ операций над структурой данных, каждая из которых, при реализации через лес непересекающихся множеств, выполняется за время $O(\log n)$, и в общей сложности получается $|E|\log |V|$, что уже быстрее сортировки.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 8} 

\subsection{Структура данных для непересекающихся множеств.}

\begin{prob}
    Абстрактная структура данных для представителя разбиения множества на непересекающиеся подмножества. У каждого множества есть выжеленный элемент - \textcolor{magenta}{\hypertarget{d7}{\textit{представитель}}}. Заданы операции:

    \begin{itemize}
        \item $make\_set(x)$ - создать одноэлементное множество; 
        \item $find\_set(x)$ - найти представитель множества, содержащего данный элемент; 
        \item $set\_union(x, y)$ - объединение двух множеств.
    \end{itemize}

    Перед нами стоит задача: как эффективно реализовать эту абстрактную структуру данных?
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t20}{\textbf{Лес непересекающихся множеств}}}. У нас есть лес, если одноэлементно - просто вершинка, если представитель - корень дерева, если объединяем, то корень одного будет указывать на корень другого.
\end{alg}

\begin{remark}
    Используются следующие улучшения: \ 
    
    В каждой вершине хранится её условная сложность - \textcolor{magenta}{\hypertarget{d8}{\textit{ранг}}}. Процедура $make\_set(x)$ устанавливает ранг в нуль. При объединении двух деревьев корень дерева меньшего ранга станет указывать на корень дерева большего ранга. Если оба корня имеют одинаковый ранг, то их объединение получит ранг, больший на единицу. \ 

    При выполнении каждой операции поиска все встреченные на пути вершины пренаправятся в корень для ускорения последующих операций цикла.
\end{remark}

\begin{stat}
    Пусь всего элементов - $n$, и к ним применяется $m$ операций. Тогда, с применением первого улучшения, они выполняются за время $O(m \log n)$. 
\end{stat}

\begin{proof}
    Смотрим на дерево, через логарифмы вычисляем.
\end{proof}

\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t21}{\textbf{Тарьян}}}). Пусть всего элементов - $n$, и над ними выполняется $m$ операций. Тогда, с применением первого и второго улучшения, эти операции выполняются за совокупное время $m\alpha (n)$, где $\alpha(n)$ - обратная функция к $A_n(n)$, а $A(k, n)=A_k(n)$ - \textcolor{magenta}{\hypertarget{d9}{\textit{функция Аккермана}}}. 
\end{theorem}

\begin{prob}
    Дан неориентированный граф - возможно, несвязный. Надо определить его компоненты связности и научиться быстро отвечать на вопрос о том, лежат ли две данные вершины в одной компоненте.
\end{prob}

\begin{alg}
    Алгоритм пишется через представление компонент связности в виде структуры данных для непересекающихся множеств.
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l4.pdf}{4 лекция, страница 11} 

\section{Лекция 5.}

\subsection{Пути между всеми парами вершин.}

\begin{prob}
    Дан ориентированный граф $G=(V, E)$, где $V = \{1, \ldots, n\}$, а $E\subseteq V \times V$. Стваится задача проверить чуществование пути из каждой вершины в каждую - то есть: для каждой пары вершин $(i, j)$ определить, есть ли путь из $i$ в $j$. После решения мы получим \textcolor{magenta}{\hypertarget{d10}{\textit{транзитивное замыкание}}} графа.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t22}{\textbf{Очевидный алгоритм}}}. Перебираем все пары вершин $(i, j)$ и для каждой пары все промежуточные вершины $k$. Если она соединена с обоими, то добавляем ребро $(i, j)$. Делаем так пока можем.
\end{alg}

\begin{stat}
    После каждой $t$-ой итерации внешнего цикла будут просчитаны все пути длины не более $2^t$, и для каждого из них в граф будет добавлена дуга. Откуда время работы - не более, чем $n^3 \log_2 n$ шагов.
\end{stat}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t23}{\textbf{Алгоритм Варшалла}}}. Можно построить транзитивное замыкание за время $n^3$, обойдясь без внешнего цикла (без пока можем). Для этого достаточно поменять циклы местами и теперь рассматривать каждую вершину как смежную, соединяя все пары, смежные с ней.
\end{alg}

\begin{stat}
    После $k$-ой итерации внешнего цикла найдены все пути, проходящие только через промежуточные вершины из множества $\{1, \ldots, k\}$. 
\end{stat}

\begin{proof}
    Доказательство индукцией по количеству итераций.
\end{proof}

\begin{prob}
    Пусть граф теперь не только ориентированный, но и рёбра имеют вес. Нужно найти кратчайшие пути между вершинами.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t24}{\textbf{Алгоритм Флойда-Варшалла}}}. То же самое, что и алгоритм варшала, но мы ещё и храним вес рёбер.
\end{alg}

\begin{remark}
    О достижимости можно говорит в рамках матриц смежности. Умножение рассматривается как 
    \[
        c_{i, j}=\bigvee_{k=1}^l a_{i, k}\wedge b_{k, l}. 
    \]
    Тогда если $A$ - матрица смежности, то $A^l$ - матрица достижимости по путям длины ровно $l$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 1}

\subsection{Быстрое умножение матриц.}

Ну, какие-то способы усножения - ебанина какая-то, писать не буду.

\begin{theorem}
    (\textcolor{magenta}{\hypertarget{t25}{\textbf{Основная теорема о времени работы рекурсивных алгоритмов}}}). Пусть задача размера $n$ рашается путём разбиения её на $a$ подзадач того же типа, размера $\frac{n}{b}$ каждая, и процесс разбиения, а также соединения результатов занимает время $O(n^c)$. 
    \[
        T(n)=aT(\frac{n}{b})+O(n^c)
    \]
    Тогда время работы алгоритма оценивается как

    \begin{itemize}
        \item Если $c<\log_b a$, то $O(n^{\log_b a})$;
        \item Если $c=\log_b a$, то $O(n^c \log_b n )$; 
        \item Если $c>\log_b a$. то $(n^c)$. 
    \end{itemize}
\end{theorem}

\begin{proof}
    В дереве рекурсии $\lceil \log_b n \rceil$ уровней, на нулевом - одна подзадача размера $n$, рассчёты занимают время $n^c$, на первом - $a$ подзадач размера $\lceil \frac{n}{b} \rceil$, расчёты для каждой занимают врем] $\lceil \frac{n}{b} \rceil^c$, и так далее, на уровне $i$ - а подзадач размера $\lceil \frac{n}{b^i}\rceil$ каждая, расчёты каждой занимают время $\lceil \frac{n}{b^i}\rceil^c$. Общее время работа - сумма по всем уровням рекурсии.
    \[
        T(n)= \sum_{i=0}^{\lceil \log_b n \rceil} a^i\biggl(\frac{n}{b^i}\biggr)^c=n^c \sum_{i=0}^{\lceil \log_b n \rceil} \biggl(\frac{a}{b^c}\biggr)^i
    \] 
    Таким образом, если $a=b^c$, то сумма геометрической прогрессии - $\log_b^n$, и отсюда $O(n^c\log_b n)$. Если $a<b^c$, то сумма ограничена сверху константой, отсюда $O(n^c)$. И если $a>b^c$, то это возрастающая геометрическая прогрессия, и применив формулу для вычисления получим $O(n^{\log_b a})$. 
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 8}

\begin{prob}
    \hypertarget{e2}{Умножение} булевых матриц.
\end{prob}

\begin{alg}
    Можно записать матрицу в виде кучи нулей и единиц, после чего перемножить в кольце вычетов по модулю $n+1$ или по любому удобному модулю, превосходящему $n$. Тогда вместо дизъюнкции конъюнкций будет вычислена сумма произведений по модулю $n+1$ (в точности равная оличества интинных конъюнкций в дизъюнкции конъюнкций, и потому она лежит в диапазоне от 0 до $n$, откуда следует, что по модулю $n+1$ она вычислится точно). Чтобы узнать значение дизъюнкции конъюнкций, достаточно будет проверить вычисленную сумму на равенство нулю.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t26}{\textbf{Метод четырёх русских}}}. Чисто комбинаторный метод умножения булевых матриц за время $o(n^3)$. Пусть $A$ и $B$ - две булевых матрицы размера $n\times n$, цель - вычислить их произведение $C=AB$. Пусть $k$ - число, намного меньшее, чем $\log_2 n$, и пусть $n$ делится на $k$ (если не делится - немного увеличим $n$ до делимости). Каждая строка матрицы $A$ делится на $\frac{n}{k}$ векторов размера $1\times k$, называемых \textcolor{magenta}{\hypertarget{d11}{\textit{кусочками}}}. Кусочки в $i$-ой строке обозначаются через $A_{i, l}, \ldots, A_{i, \frac{n}{k}}\in \mathbb{B}^{1\times k}$. Матрица $B$ разделяется на $\frac{n}{k}$ подматриц размера $k\times n$, называемых \textcolor{magenta}{\hypertarget{d12}{\textit{полосами}}} и обозначаемыми через $B_1, \ldots, B_{\frac{n}{k}}\in \mathbb{B}^{1\times k}$. Тогда всякая $i$-ая строка $C$, обозначаемая через $C_i\in \mathbb{B}^{1\times n}$, представима в виде следующей поэлементной дизъюнкции строк:
    \[
        C_i=\bigvee_{r=1}^{\frac{n}{k}}A_{i, r}B_r. 
    \]
    Произведение кусочка $A_{i, r}$ на соответствующую полосу $B_r$ - это строка размера $1\times n$, и знак дизъюнкуции в формуле для $C_i$ означает поразрядную дизъюнкцию таких строк,
\end{alg}

\begin{remark}
    Казалось бы, никакого принципиального улучшения не получилось, однако с программистсткой точки зрения, это в чём-то удобнее. \ 

    Главная идея метода же состоит в том, что так как $k$ невелико, возможных кусочков всего $2^k$, и кусочки будут часто повторяться. Таким образом, если запомнить произведения таких кусочков на полосы $B$, в плане вычислений это займёт много меньше времени. Общее время работы будет $O\biggl(\frac{n^3}{\log n}\biggr)$. 
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l5.pdf}{5 лекция, страница 10}

\section{Лекция 6.}

\subsection{Двоичные деревья поиска}

Речь сейчас пойдёт о структурах данных для представления \textcolor{magenta}{\hypertarget{d13}{\textit{множества}}} различными способами, с разными временем выполнения операций (строго говоря, мы будем говорить о \textcolor{magenta}{\hypertarget{d14}{\textit{мультимножестве}}}, так как может быть несолько элементов с одинаковым значением). Элементы погут быть любого вида, на них определено отношение порядка $''\leq''$. Операции: найти элемент с данным значением; найти наибольший (наименьший) элемент; найти элемент, предшествующий или следующий за данным; вставить / удалить элемент. \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t27}{\textbf{Бинарное дерево поиска}}}. Представление мнлжества в виде дерева так, что вершина содержит одно значение и три указателя: на предка (если корень, то NULL), на левое поддерево и на правое (если их нет, то NULL). При этом все вершины в её левом поддереве содержат не меньшие значения, а все вершины в правом поддереве - не бóльшие. Понятно, как на таком дереве задаются требуемые операции.
\end{alg}

\begin{remark}
    Сложность каждой операции - не более, чем высота дерева. Если дерево сбалансированное, то есть, все пути примерно одинаковой длины, то все операции выполняются за логарифмическое время.
\end{remark}

\begin{prob}
    Даже если дерево сбалансированное, то операции над ним могут првести его к дизбалансу, надо как-то сложниить структуру, чтобы избежать критических изменений.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t28}{\textbf{АВЛ-деревья}}}. Усложнённая разновидность двоичных деревьев поиска. Такую разновидность называют \textcolor{magenta}{\hypertarget{d15}{\textit{почти сбалансированной}}}, то есть, высота деревьев-потомков одной вершины отличается не более, чем на 1. Этого ограничения достаточно, чтобы дерево мело логарифмическую высоту.
\end{alg}

\begin{stat}
    АВЛ-дерево высоты $h$ содержит не менее, чем $F_{h+3}-1$ вершин, где $F_n$ - $n$-ое число Фибоначчи.
\end{stat}

\begin{proof}
    Индукция по $h$. База очевидна, переход таков. Если дерево имело высоту $h$, то один из его потомков имеет высоту $h-1$, а другой - высоту не менее, чем $h-2$, то есть, не менее чем $F_{h+2}-1$ и $F_{h+1}-1$ вершин соответственно. Всего, с учётом корня и по определению чисел Фибоначчи, как раз и получится то, что нужно.
\end{proof}

\begin{stat}
    Высота АВЛ-дерева с $n$ вершинами не превосходит $\log_\varphi n$, где $\varphi$ - золотое сечение.
\end{stat}

\begin{remark}
    Естественно, просто так АВЛ-дерево не получить, поэтому придумана операция \textcolor{magenta}{\hypertarget{d16}{\textit{вращение}}}, которая состоит в том, что если у нас есть корень $y$, из которого направо идёт поддерево $t_3$, а налево - поддерево с корнем $x$ и поддеревьями $t_1$ и $t_2$ налево и направо соответственно. Тогда мы подвешиваем дерево за $x$ и перебрасываем его поддерево $t_2$ в левое поддерево $y$. При помощи такой операции можно исправить балансировку при различных операциях, подробно расписывать я это пока что не буду.
\end{remark}

\begin{prob}
    Двоичные деревья рассчитаны на то, чтобы храниться в оперативной памяти компьютера, а для того, чтобы хранить деревья во внешней, медленной памяти, требуется некая адаптация.
\end{prob}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l6.pdf}{6 лекция, страница 1}

\subsection{B-деревья.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t29}{\textbf{B-деревья}}}. Адаптация деревьев поиска для хранения во внешней памяти. Основная мысль - использовать вершины большой степени - с тем, чтобы каждая вершина занимала один блок, а высота дерева уменьшилась бы. \ 

    Пусть вершины в двоичном дереве - это 2-вершины, поскольку у каждой из них 2 потомка и 1 значение, по которому эти потомки разделяются. У $m$-вершины - $m$ потомков (деревья $t_1, \ldots, t_m$ - возможно, пустые), и в ней находится $m-1$ значение: $x_1, \ldots, x_{m-1}$, где значения упорядочены по неубыванию. Все значения в каждом поддереве $t_i$, больше или равны $x_i$, и меньше или равны $x_{i+1}$. \ 

    В B-дереве могут одновременно соержаться вершины различных степеней: выбирается некоторое число $k\geq 2$, после чего корень может иметь степень от 0 до $2k$, а все остальные вершины - любые степени от $k$ до $2k$. При этом, дерево сбалансировано.
\end{alg}

\begin{remark}
    То, как производятся требуемые операции - лучше один раз увидеть и немного прочитать в конспекте А. С. Охотина, чем читать мои заметки.
\end{remark}

\begin{alg}
    B-дерево для $k=2$ называется $2-3-4$ деревом, и такое дерево очень удобно хранить в оперативной памяти. 
\end{alg} \ 

\begin{alg}
    Красно-чёрное дерево - это представление 2-3-4 дерева в виде двоичного дерева, в котором каждая вершина представлена в виде одной или несколькиз связанных между собою двоичных вершин, каждая из которых покрашена в один из двух цветом. Каждая 2-вершина остаётся собой и считается чёрной. Каждая 3-вершина разбивается на две двоичных: чёрную - корень поддерева, и красныю вершину - правого или левого потомка чёрной. Далее - к этой паре вершин, так же как и к исходной 3-вершине, присоединяются три поддерева. Наконец, каждая 4-вершина разбивается на три двоичных - чёрную и два красных потомка, к которым присоединены четыре поддерева.
\end{alg}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l6.pdf}{6 лекция, страница 7}

\section{Лекция 7.}

\subsection{Полиномиальное хэширование.}

\begin{prob}
    Дана длинная строка $w=a_1\ldots a_n$ и короткая искомая строка $x=b_1\ldots b_m$. Требуется найти все вхождения $x$ в $w$ в качестве подстроки.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t30}{\textbf{Хэш-функции}}}. Каждой строке ставится в соответствие некоторое число $w\mapsto h(w)$, и это число хранится вместе со строкой. Тогда, если нужно сравнить две строки, то мы сравниваем сперва соответствующие им числа. Если числа разные, то и строки точно разные. Если соответствующие числа одинаковые, то надо сравнить строки и получить итоговый результат. Самое тупое - сложить коды всех символов, но тогда распределение неравномерное, и вообще это не очень кайфовый вариант. Намного лучше \textcolor{magenta}{\hypertarget{d17}{\textit{полиномиальное хэширование}}}. Берём некоторое основание степени $p$, пусть $w=a_1\ldots a_l$ - строка, тогда используется сумма $\sum_{i=1}^l a_i\cdot p^{l-i}$, взятая по некоторому модулю $M$.
\end{alg} \

\begin{alg}
    \textcolor{magenta}{\hypertarget{t31}{\textbf{Алгоритм Рабина-Карпа}}}. Алгоритм поиска подстроки $x=b_1\ldots b_m$ в строке $w-a_1\ldots a_n$, основанный на полиномиальном хэшировании степени $m-1$: сперва вычисляется значение хэщ-функции для искомой строки, а затем для всех $m$-символьных подстрок данного текста последовательно вычисляется значение их хэш-функции. Когда значение для подстроки и искомой строки совпадают, производится прямое сравнение. Значение хэш-функции для искомой строки: $X=\sum_{i=1}^m b_i\cdot p^{m-i}$ по модулю $q$, а значение хэш-функции для подстроки со смещением $s$: $W_s=\sum_{i=1}^m a_{s+i}\cdot p^{m-1}$ по модулю $q$. Алгоритм сперва вычисляет $X$, а затем - последовательно все $W_s$. Из соотношения $W_{s+1}=pW_s-a_{s+1}p^m+a_{s+m+1}(\mod{q})$. Сложность: $\Theta(m)$ - на подготовку, и затем в худшем случае $O(mn)$, если хэ=-функция выдаст одинаковые значения для всех подстрок. В среднем случае получается время работы $\Theta(n)$. 
\end{alg}

\begin{prob}
    (Наибольшая общая подстрока). Даны две строки: $u=a_1\ldots a_m$ и $v=b_1\ldots b_n$. Надо найти самую длинную их общую подстроку $x$. 
\end{prob}

\begin{alg}
    Тупое решение: для каждой пары позиций найти самую длинную подстроку, время $((m+n)^3)$. 
\end{alg} \

\begin{alg}
    Улучшение через полиномиальное хэширование - научиться отвечать на вопрос ''Есть ли общая подстрока длины $l$?'' за время $O((m+n)\log(m+n))$. Для этого находятся значения хэш-функции для всех подстрок $u$ длины $l$, размещаются в двоичном дереве поиска - время $m\log m$. То же самое - для $v$ за время $n\log n$. 
\end{alg} \

\begin{alg}
    Улучшение через двоичный поиск по $l$. Его можно записать в виде рекурсивной процедуры, отвечающей на вопрос ''Найти длину наибольшей общей подстроки, если известно, что её длина не меньше, чем $l_1$, и строго больше, чем $l_2$?''. Процедура $f(l_1, l_2)$: если $l_2-l_1=1$, то возвращаем $l_1$, $k=\lfloor \frac{l_1+l_2}{2}\rfloor$. Теперь, если есть общая подстрока длины $k$, то возвращаем $f(k, l_2)$, иначе возвращаем $f(l_1, k)$. 
\end{alg}

\begin{prob}
    Найти в строке самую длинную подстроку-палиндром.
\end{prob}

\begin{alg}
    Самый тупой алгоритм: искать вокруг каждого симыола на длину вплоть до $\frac{n}{2}$, время $O(n^2)$. 
\end{alg} \

\begin{alg}
    Улучшение основано на решении подзадачи: ''Есть ли подстрока-палиндром данной длины $l$?''. После того, как будет построен алгоритм для решения подзадачи, останется использовать двоичный поиск по $l$. 
\end{alg} \

\begin{alg}
    Решение через небольшую переформулировку: разворачиваем строку и для таковой и оригинальной рассматриваем хэш-ффункции для всех подстрок длины $l$. После чего, естественн, надо выполнить посимвольную проверку. Двоичный поис по $l$ даст время $O(n\log n)$. 
\end{alg} \

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 2}

\subsection{Алгоритм Кнута-Морриса-Пратта.}

\begin{alg}
    \hypertarget{e3}{Строка} $w$ читается слева направо, и после чтения $i$ символов $w$ он помнит длину $j$ самого длинного префикса строки $x$, на которой заканчивается $a_1 \ldots a_i$ - то есть, наибольшее число $j$, для которого верно $a_{i-j+1}\ldots a_i=b_1\ldots b_j$. \ 

    При чтении очередного символа $a_{i+1}$ алгоритм сравнивает его с $b_{j+1}$, и если эти символы равны, это значит, что префикс $x$ длины $j$ продлевается на один символ. Если же $a_{i+1}\neq b_{j+1}$, то самый длинный префикс $x$, на который заканчивается строка $a_1\ldots a_{i+1}$, будет содержать строго меньше, чем $j+1$ символов. Чтобы найти длину этого префикса, нужно рассмотреть следующий по длине префикс $x$ на который заканчивается $a_1\ldots a_i$, и пытаться продлевать уже его. \ 

    Чтобы иметь возможность находить длину такого префикса, алгоритм заранее строит по данной искомой строке $x$ определённую стуктуру данных - \textcolor{magenta}{\hypertarget{d18}{\textit{префиксную сумму}}}.
\end{alg}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d19}{\textit{Префиксная сумма}}} для строки $x=b_1\ldots b_m$ - это функция $\pi\{1, \ldots, m\}\rightarrow \{0, \ldots, m-1\}$, которая для всякого префикса $b_1\ldots b_i$ строки $x$ выдаёт длину наибольшего суффикса подстроки $b_1\ldots b_i$, который также является префиксом $x$. 
    \[
        \pi(i)=\max\{j|j<i, b_1\ldots b_j = b_{i-j+1}\ldots b_i\} 
    \]
\end{defn}

\begin{remark}
    Повторно применяя функцию $\pi$ можно получить все префиксы $x$, являющиеся суффиксами $b_1\ldots b_i$, и алгоритм будет их перебирать, пока не найдёт такой, который можно продолжить следующим символом текста. 
\end{remark}

\begin{stat}
    Имея готовую таблицу значений функции $\pi$, алгоритм KMP работает за время $\Theta(n)$. 
\end{stat}

\begin{remark}
    Осталось научиться быстро строить значение префиксной функции.
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 4}

\subsection{Поиск с помощью конечных автоматов.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d20}{\textit{Детермированный конечный автомат (DFA)}}} - пятёрка $A=(\Sigma, Q, q_0, \delta, F)$, со следующим значением компонентов:
    
    \begin{itemize}
        \item $\Sigma$ - алфавит; 
        \item $Q$ - конечное множество состояний; 
        \item $q_0\in Q$ - начальное состояние; 
        \item $\delta:Q\times \Sigma \rightarrow Q$ - функция переходов. Если автомат находится в состоянии $q\in Q$ и читает символ $a\in \Sigma$, то его следующее состояние - $\delta(q, a)$; 
        \item $F\subseteq Q$ - множество \textcolor{magenta}{\hypertarget{d21}{\textit{принимающих состояний}}}.
    \end{itemize}

    Для всякой входной строки $w=a_1\ldots a_l$, где $l\geq 0$ и $a_1, \ldots, a_l\in \Sigma$, вычисление - последовательность состояний $p_0 \ldots, p_{l-1, p_l}$, где $p_0=q_0$, и всякое следующее состояние $p_i$, где $i\in\{1, \ldots, l\}$, однозначно определено как $p_i=\delta(p_{i-1}, a_i)$. 
    \[
        p_0\rightarrow^{a_1}p_1\rightarrow^{a_1}\ldots \rightarrow^{a_l} p_l
    \]

    Строка \textcolor{magenta}{\hypertarget{d22}{\textit{принимается}}}, если последнее состоение $p_l$ принадлежит множеству $F$ - иначе \textcolor{magenta}{\hypertarget{d23}{\textit{отвергается}}}. Множество строк, \textcolor{magenta}{\hypertarget{d24}{\textit{распознаваемое}}} автоматом, обозначаемое через $L(A)$ - состоит из всех строк, которые он принимает.
\end{defn}

\begin{theorem}
    Для всякой строки $x=b_1\ldots b_m$ над алфавитом $\Sigma$ существует DFA с $m+1$ состояниями, распознающий множество $\Sigma^* x$ всех строк, заканчивающихся на $x$, и этот DFA можно построить за время $\Theta(|\Sigma|\cdot m)$. 
\end{theorem}

\begin{proof}
    Пока что записывать не буду.
\end{proof}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 6}

\subsection{Представления множеств строк.}

\begin{defn}
    \textcolor{magenta}{\hypertarget{d25}{\textit{Лексикографический порядок}}}. Поэлементное сравнение слева направо.
\end{defn}

\begin{prob}
    Как хранить строки?
\end{prob}

\begin{alg}
    В виде списка или двоичного дерева поиска, однако это всё хуйня.
\end{alg} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t32}{\textbf{Префиксное дерево}}} - структура для хранения множеств строк. Корневое дерево с дугами, помеченными символами алфавита. Дуги, исходящие ииз всякой вершины, должны быть помечены различными символами. Каждая вершина соответствует некоторой строке, которая хранится неявно в виде последовательности дуг на пути к ней. Также всякая вершина хранит один бит информации, принадлежит ли эта строка ммножеству; вместе с битом можно хранить любые дополнительные значения, сопоставленный в этой строке. Корень соответствует пустой строке. Если вершина соответствует строке $w$, и исходящая из неё дуга помечена символом $a\in \Sigma$, то эта дуга идёт в вершину, соответствующую строке $wa$. \ 

    Все операции с любой данной строкой длины $l$ (поиск, вставка, удаление) выолняются за время $O(l)$, не зависящее от числа элементов в хранимом множестве. \ 

    \textcolor{magenta}{\hypertarget{d26}{\textit{Компактное префиксное дерево}}} - объединяем несколько дуг в одну, если они идут одинаково параллельно.
\end{alg}

\begin{stat}
    В компактном префиксном дереве для множества из $n$ элементов не более, чем $n$ внутренних вершин.
\end{stat}

\subsection{Поиск нескольких строк одновременно: алгоритм Ахо-Корасик.}

\begin{prob}
    Пусть для данного текста нужно найти все вхождения не одной строки, а всех строк из конечного множества $K=\{x_1, \ldots, x_k\}$. 
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t33}{\textbf{Алгоритм Ахо-Корасик}}}. Данный алгоритм будет помнить самый длинный только что прочитанный префикс одной из строк $x_1, \ldots, x_k$, а точнее сказать, самую длинную такую строку $u$, что $u$ - префикс какой-то строки из $K$, и алгоритм только что прочитал $u$. 
\end{alg}

\begin{remark}
    Для решения обобщается префиксная сумма и построение функции $\pi$ (работающее за линейное время), но пока что упущу подробности.
\end{remark}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l7.pdf}{7 лекция, страница 9}

\subsection{Суффиксные деревья.}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t34}{\textbf{Суффиксное дерево}}} - структура данных, строящаяся по данной строке и обеспечивающая эффективный поиск подстрок в этой строке. Понятно, что это такое.  
\end{alg}

\section{Лекция 9.}

\subsection{Сжатие данных, основанное на повторении строк.}

\begin{prob}
    Нужно научиться использовать часто повторяющиеся подстроки, чтоы хранить их отдельно в памяти и эффективно использовать при хранении целого текста.
\end{prob}

\begin{alg}
    \textcolor{magenta}{\hypertarget{t35}{\textbf{Метод Лемпеля-Зива LZ77}}}. Алгоритм сжатия данных, использующий повторяемость подстрок. Основная идея в том, что каждый раз, когда ранее прочитанная подстрока встречается повторно, алгоритм пишет вместо неё ссылку на предыдущее вхождение. На каждом шаге выводится тройка $(d, l, a)$, что означает: сперва повторяется подстрока длины $l$, ранее встречавшаяся $d$ символов назад, а потом идёт символ $a$. Реализуется самым простым образом через суффиксное дерево, в котором на каждом шаге читаются следующие символы входной строки, пока не находится самая длинная раньше встречавшаяся подстрока. Обычно используется небольшое улучшение - \textcolor{magenta}{\hypertarget{d27}{\textit{''скользящее окно''}}}, то есть, подстроки ищутся (тем же суффиксным деревом, скорее всего) в окне из последних $m$ элементов, а остальные забываются.
\end{alg} \ 

\begin{theorem}
    Жадный LZ77 оптимален.
\end{theorem} \ 

\begin{alg}
    \textcolor{magenta}{\hypertarget{t36}{\textbf{Метод Лемпеля-Зива LZ78}}}. Тут уже по мере чтения строки строится словарь из часто встречающихся строк в виде префиксного дерева. При декодировании строится в точности этот же словарь. В сжатом представлении строки словарь не хранится. \ 

    В начале в словаре содержится только один элемент под номером нуль: $T_0=\varepsilon$, иными словами, префиксное дерево состоит из корня, помеченного номером 0. На каждом шаге читается самая длинная строка $T_j=v$, и уже имеющаяся в словаре, и выводится её код $j$; также читается и выводится следующий символ $a$. При этом в словарь добавляется новая трока $va$ - конкатенация только что прочитанной со следующим входным символом. \ 

    На префиксном дереве это выглядит так: после вывода очередной пары $(j, a)$ алгоритм переходит в корень префиксного дерева, и дальше читает столько входных символов, сколько возможно. Когда очередной символ прочитать нельзя, создаётся новый лист, при этом выводится номер предыдущей вершины и прочитанный символ. \ 

    При декодировании строится то же самое префиксное дерево. Алгоритму при этом потребуется находить строку по номеру в таблице: можно, например, завести для этого отдельный массиы строк, или же читать в префиксном дереве путь из вершины в корень. Прочитав очередную пару $(j, a)$ алгоритм выводит строку $T_j$ и символ $a$, после чего сразу перескакивает в вершину $j$ и присоединяет к ней новый лист, с переходом по символу $a$. 
\end{alg} \ 

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l9.pdf}{9 лекция, страница 1} \\

\begin{alg}
    \textcolor{magenta}{\hypertarget{t37}{\textbf{Преобразование Берроуза-Вилера}}}. Перутся все циклические сдвиги строки $w=a_1\ldots a_n$. Они сортируются лексикографически, получается таблица $T$ размера $n\times n$, где в каждой строчке - один из циклических сдвигов. наконец, берётся последовательность последних символов $b_1\ldots b_n$ в таблице $T$ и номер $t$ строки $w$ в этой таблице. \ 

    Если в исходной строке какая-то подстрока $au$ часто повторялась, то есть много циклических сдвигов, начинающихся с $u$ и заканчивающихся на $a$, и после сортировки они окажутся рядом. Поэтому в преобразованной строке будут длинные последовательности повторяющихся символов. Использование в качестве метода сжатия данных: преобразовать, а потом применять другие методы к преобразованной строке. \ 

    Утверждается, что по последовательности $b_i$, и $t$ восстанавливается исходная $w$. 
\end{alg}

\begin{stat}
    BWT можно вычислить за линейное время.
\end{stat}

\begin{stat}
    Обратное к BWT преобразование можно вычислить за линейное время.
\end{stat}

\textbf{Подробнее}: \href{https://users.math-cs.spbu.ru/~okhotin/teaching/algorithms_2020/okhotin_algorithms_2020_l9.pdf}{9 лекция, страница 4}

\newpage

\begin{center}
\includegraphics{tiskin}

Торжественно начинается часть конспекта, основанная на лекциях \ 

доцента ФМКН СПБГУ, кандидата физико-математических наук \ 

Александра Владимировича ТИСКИНА 
\end{center}

\section{Преобразование Фурье.}

\subsection{Введение в преобразование Фурье.}

\begin{alg}
    \textbf{Дискретное проеобразование Фурье степени $n$ (DFT$_n$)} - $F_{n, \omega}\cdot a = b$, где $n$ - обратим в $R$, коммутативном кольц без делителей нуля, $\omega$ - первообразный корень $\sqrt[n]{1}$, $a=[a_0, \ldots, a_{n-1}]^T\in R^n$, $b=[b_0, \ldots, b_{n-1}]^T\in R^n$, а $F_n, \omega$ - матрица
    
    \begin{equation*}
        [\omega^{ij}]_{i, j=0}^{n-1}=
        \begin{vmatrix}
            1 & 1 & 1 & \dots & 1 \\
            1 & \omega & \omega^2 & \dots & \omega^{n-1} \\
            1 & \omega^2 & \omega^4 & \dots & \omega^{n-2} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            1 & \omega^{n-1} & \omega^{n-2} & \dots & \omega 
        \end{vmatrix}
    \end{equation*}

    Трудоёмкость: наивный алгоритм, время - $O(n^2)$. 
\end{alg} \

\begin{alg}
    \textbf{Обратное DFT}. $\frac{1}{n}F_{n, \omega^{-1}}\cdot b = a$.
\end{alg}

\begin{remark}
    Используется для \textit{вычисления значений} полинома $a(x)=a_0+a_1x+\ldots+a_{n-1}x^{n-1}$ на $C=\{1, \omega, \ldots, \omega^{n-1}\}$. Аналогично для \textit{интерполяции} полинома $a(x)$ по значениям на $С$ - $\frac{1}{n}F_{n, \omega^{-1}}\cdot a$. \ 

    $\frac{1}{\sqrt{n}}F_{n, \omega^{-1}}\cdot a$ - разложение $a$ по \textit{базису Фурье} (строкам $\frac{1}{\sqrt{n}}F_{n, \omega}$). 
\end{remark}

\subsection{Быстрое преобразование Фурье.}

\begin{alg}
    \textbf{Бвстрое преобразование Фурье (FFT)}. Общая идея: исользовать разложение $n$ на множители и структцтц множества корней из единицы для ускорения вычислений. То есть, пусть $n=n'n''$, $1<n'\leq n'' < n$. В алгоритме FFT 

    \begin{itemize}
        \item вектор $a$ записывается в виде $n'\times n''$-матрицы по строкам; 
        \item вектор $b$ записывается в виде $n''\times n'$-матрицы по строкам; 
        \item DFT$_n$ выражается в виде DFT$_{n'}$, DFT$_{n''}$ над столбцами матриц; 
        \item DFT$_{n'}$, DFT$_{n''}$ вычисляются рекурсивно. 
    \end{itemize}
\end{alg} \

\begin{alg}
    Некоторые классические варианты: $n=\frac{n}{2}\cdot 2$ - \textbf{FFT с прореживанием по времени} (FFT-DIT), и $n=2\cdot \frac{n}{2}$ - \textbf{FFT с прореживанием по частоте} (FFT-DIF). 
\end{alg} \ 

\begin{alg}
    Наиболее общий слуяай - \textbf{шастиэтапное FFT}. Пусть $n=n'n''$, запишем матрицы в таком виде:

    \begin{equation*}
        A=
        \begin{vmatrix}
            a_0 & a_1 & \dots & a_{n''-1} \\
            a_{n''} & a_{n''+1} & \dots & a_{2n''-1} \\
            \vdots & \vdots & \ddots & \vdots \\
            a_{n-n''} & a_{n-n''+1} & \dots & a_{n-1} 
        \end{vmatrix}, 
    \end{equation*}

    и аналогично $B$. Тогда получим, что $B=F_{n'', \omega^{n'}}\cdot (G_{n', n'', \omega}\circ (F_{n', \omega^{n''}}\cdot A))^T$, где $G_{n', n'', \omega}=[\omega^{tv}]_{t, v}: n'\times n''$ - матрица \textit{поворотных множителей}, а оператор $\circ$ - \textit{умножение Адамара} (поэлементное умножение матриц). \ 

    И, действительно, данная схема почти шестиэтапная:

    \begin{itemize}
        \item вектор $a$ записывается в матрицу по строкам; 
        \item $n''$ независимых DFT$_{n'}$ над столбцами, вычисляются рекурсивно; 
        \item транспозиция; применение поворотных множителей (2 этапа); 
        \item $n'$ назависимых DFT$_{n''}$ над столбцами, вычисляются рекурсивно; 
        \item вектор $b$ считывается из матрицы по строкам.
    \end{itemize}
\end{alg}

\begin{remark}
    Трудоёмкость FFT-DIT - $O(n \log n)$, трудоёмкость симметричного FFT ($n'=n''$) - $O(n \log n)$. 
\end{remark}

\begin{remark}
    Если вам интересен \textit{граф-бабочка} и схемы FFT, то можете заглянуть в лекции Тискина. Я их, конечно же, перерисовывать не буду.
\end{remark}

\subsection{Задачи и алгоритмы решения.}

\begin{prob}
    Умножение полиномов. Пусть у нас есть два полинома с комплексными коэффициентами. Нас интересует их произведение. Тупейший алгоритм сделает это за время $O(n^2)$. 
\end{prob}

\begin{alg}
    \textbf{Алгоритм Карацубы.} Поделим многочлены $a$ и $b$ (сделаем их одинаковой длины) на две равные части (по $m$), и запишем $a(x)=a'(x)+a''(x)x^m$, аналогично с $b(x)$. Тогда $c(x)=$
    \[
        = a'b'+((a'+a'')(b'+b'')-a'b'-a''b'')x^m+a''b''x^n. 
    \]
    Таким образом, вместо 4 умножений полиномов с $m$ членами получилось 3. Трудоёмкость процесса получилась $O(n^{\log_2 3})$.  
\end{alg}

\begin{prob}
    Быстрая интерполяция.
\end{prob}

\begin{alg}
    Вот если мы посмотрим на ту красивую матрицу $F$, то заметим, что применив обратное преобразование Фурье к столбцу из значений в корнях из единицы, то как раз получим то, что нужно. В этом нетрудно убедится, провернув преобразования не в эту обратную, а в прямую сторону. Трудоёмкость: $O(n\log n)$. 
\end{alg}

\begin{defn}
    \textit{Свёртка} (линейная) $a$ и $b$ - двух последовательностей длины $n$ - $c=a*b$, что находится по формуле $c_i=\sum_{0\leq j\leq i <2n} a_j'b_{i-j}'$, где $a'$ и $b'$ - дополнения изначальных до длины $2n$. Есть ещё циклическая и косоциклическая свёртки, но они противные.
\end{defn}

\begin{alg}
    Суть - то же умножение полиномов. $c=\frac{1}{n} F_{2n, \omega^{-1}}((F_{2n, \omega}a')\circ (F_{2n, \omega}b'))$.
\end{alg}

\begin{remark}
    Немного про количество битов, необходимых для операций прямого и обратного $DFT$ в $\ZZ_n$:

    \begin{itemize}
        \item сумма двух значений: в результате $(\frac{nr}{2}+1)+1=\frac{nr}{2}+2$ бит;
        \item умножение на поворотный множитель $\omega^q=2^{qr}$, $0\leq q< n$: сдвиг на $qr<nr$ бит: в результате - не более $(\frac{nr}{2}+1)+nr-1=\frac{3nr}{2}$ бит; 
        \item приведение значений с записью из $\frac{3nr}{2}=O(nr)$ бит.
    \end{itemize}

    Трудоёкость каждой операции над значениями: $O(nr)$ битовых операций. Что касается трудоёмкости всего DFT$_n$ в $\ZZ_N$: на вход/выход - $n\cdot O(nr)=O(n^2r)$ бит, и $O(n \log n)$ сложений и умножений на поворотные множители, итого $O(n^2r \log n)$ битовых операций.
\end{remark}

\begin{prob}
    Эффективное умножение многозначных чисел.
\end{prob}

\begin{alg}
    \textbf{Алгоритм Карацубы.} Как и умножение многочленов, просто сделаем из чисел многочлены поразрядно. В качестве значения на лекции рассматривалась двойка.
\end{alg} \

\begin{alg}
    \textbf{Алгоритм Шенхаге-Штрассена.} Основная мысль состоит в том, что мы разбиваем $a$ и $b$ на разряды правильно выбранной длины $l$ и рассматриваем каждый разряд как коэффициент полинома, всего $n/l$ коэффициентов в каждом полиноме. Используем быстрое преобразование Фурье для их умножения. Попарное умножение значений полиномов выполняется рекурсивно, затем учитываются переносы. Сам алгоритм, ну там пиздец намешали и КТО, и косоциклическую свёртку, и Карацубу. В сухом итоге, трудоёмкость - $O(n\log n \log \log n)$.
\end{alg}

\section{Параллельные алгоритмы.}

\subsection{Базовые определения и понятия.}

\begin{defn}
    \textit{Схема} (вычислительная) - ориентированный ациклический граф (dag), с фиксированным количеством входов и выходов. Вычисления неадаптивны (oblivious): последовательность операций не зависит от входа. \ 

    Вычисления над переменным количеством входов: (бесконечное) \textit{семейство схем}. Семейство схем с конечным описанием - \textit{алгоритм}.
\end{defn}

\begin{defn}
    \textit{Размер} схемы - количество узлов, \textit{глубина} - максимальная длина пути от входа до выхода.
\end{defn}

\begin{defn}
    \textit{Схема сравнения} - схема, которой узлы - \textit{элементы сравнения}. Сама схема выглядит как куча вертикальных полосок и перпендикулярных им отрезочков, которые соединяют некоторые пары длинных линий. Вертикальные линии - элементы сравнения, а отрезочки - сами операции сравнения.
\end{defn}

\begin{defn}
    \textit{Схема слияния} - схема сравнения, которая берёт на вход две отсортированные последовательности и на выходе даёт отсортированную последовательность из всех их элементов.
\end{defn}

\begin{defn}
    \textit{Схема сортировки} - схема сравнения, у которой на входе произвольная последовательность, а на выходе - отсортированная последовательность.
\end{defn}

\begin{remark}
    Семейство схем с конечным описанием - алгоритм сортировки. Их размер и глубина - последовательная и параллельная сложность алгоритма. 
\end{remark}

\subsection{Больше о сортировках и слияниях.}

\begin{theorem}
    \textbf{Принцип нулей-единиц.} Схема сравнения является сортирующей тогда и только тогда, когда она сортирует все последовательности нулей и единиц.
\end{theorem}

\begin{proof}
    В одну сторону очевидно, в обратную пойдём от противного, ничего сложного.
\end{proof}

\begin{remark}
    Между прочим, очень применимая хуйня. Ей можно проверять сеть сортировки путём проверки только $2^n$ входных последовательностей, а не $n!$. А также, сеть слияния путём проверки $(n'+1)(n''+1)$ вместо много.
\end{remark}

\begin{alg}
    \textbf{Чётно-нечётное слияние (OEM).} Если $n'=n''=1$, то сравниваем $(x_1, y_1)$, иначе рекурсивно

    \begin{itemize}
        \item слияние $\langle x_1, x_3, \ldots \rangle , \langle y_1, y_3, \ldots \rangle$; 
        \item слияние $\langle x_2, x_4, \ldots \rangle , \langle y_2, y_4, \ldots \rangle$;
        \item попарное сравнение: $(u_2, v_1), (u_3, v_2), \ldots$
    \end{itemize}

    Размер - $O(n\log n)$, глубина - $O(\log n)$. 
\end{alg}

\begin{proof}
    Доказательство корректности приводится посредством индукции.
\end{proof}

\begin{alg}
    \textbf{Сортировка нечётно-чётным слиянием.} Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item сортировка $\langle x_1, \ldots, x_{\lceil n/2 \rceil} \rangle$; 
        \item сортировка второй половины; 
        \item слияние результатов при помощи OEM.
    \end{itemize}

    Размер - $O(n(\log n)^2)$, глубина - $O((\log n)^2)$.
\end{alg}

\begin{defn}
    \textit{Битонная последовательность}: $\langle x_1 \geq \ldots \geq x_m \leq \ldots \leq x_n \rangle$, $1\leq m \leq n$.
\end{defn}

\begin{alg}
    \textbf{Битонное слияние (BM)}: сортировка битонной последовательности. Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item битонное слияние $\langle x_1, x_3, \ldots \rangle $ в упорядоченную последовательность; 
        \item аналогично по чётным; 
        \item попарное сравнение $(u_1, v_1), (u_2, v_2), \ldots$. 
    \end{itemize}

    Размер - $O(n \log n)$, глубина $O(\log n)$. 
\end{alg} \ 

\begin{alg}
    \textbf{Сортировка битонным слиянием.} Если $n=1$ - останавливаемся, иначе рекурсивно

    \begin{itemize}
        \item сортируем первую половину в обратном порядке; 
        \item сортируем вторую половину в прямом порядке; 
        \item битонное слияние.
    \end{itemize}

    Размер - $O(n(\log n)^2)$, глубина $O((\log n)^2)$.
\end{alg}

\begin{prob}
    Возможна ли неадаптивная сортировка схемой размера $o(n(\log n)^2)$? $O(n \log n)$?
\end{prob}

\begin{alg}
    \textbf{Схема AKS.} Размер - $O(n \log n)$, глубина - $O(\log n)$. Использует глубокие понятия теории графив (\textit{экспандеры}). Асимптотически оптимальна, но имеет огромный константный множитель.
\end{alg}

\subsection{Модели параллельных вычислений.}

\begin{alg}
    \textbf{Parallel Random Access Machine (PRAM).} Простая, идеализированная модель общих параллельных вычислений, включающая неограниченное количество \textit{процессоров} (1 операция за единицу времени), и глобальную общую память. Вычисления полностью синхронны. Само вычисление $PRAM$ - последовательность параллельных \textit{шагов}, коммуникауия и синхронизация считаются ''бесплантыми'', однако эта хрень очень трудно реализуется на практике. \ 

    Варианты PRAM: 

    \begin{itemize}
        \item concurrent/exclusive read;
        \item concurent/exclusive write.
    \end{itemize}
\end{alg}

\begin{alg}
    \textbf{Bulk-Synchronous Parallel (BSP) computer.} Простая, реалистичная модель общих параллельных вычислений - масштабируемая, переносимая, предсказуемая. Включает $p$ процессоров, каждый с \textit{локальной памятью} (1 операция за единицу времени), \textit{коммуникационную среду}, состоящую из сети и (возможно) \textit{внешней памяти} (1 единица данных за $g$ единиц времени), и маханизм \textit{барьерной синхронизации} (не чаще 1 раза за $l$ единиц времени). \ 

    Примеры коммуникационной среды:

    \begin{itemize}
        \item $g$ - \textit{communication gap} (inverse bandwidth), время (в худшем случае) для единицы данных войти в сеть или покинуть сеть; 
        \item $l$ - \textit{latency}, время (в худшем случае) для единицы данных быть переданной внутрь сети.
    \end{itemize}

    Вычисление BSP - последовательность параллельных \textit{супершагов}. Сначала происходят вычисления / коммуникация внутри шага (коммуникация включает обмен данными с внешней памятью), а затем происходит синхронизация между супершагами. \ 

    Альтернативная модель - CSP, взаимодействующие последовательные процессы.
\end{alg}

\begin{remark}
    Рассмотрим композиционную модель стоимости вычислений. Для конкретного процессора $proc$ на супершаге $sstep$:

    \begin{itemize}
        \item $comp(sstep, proc)$ - объём локальных вычислений и операций над локальной памятью процессора $proc$ на супершаге $sstep$; 
        \item $comm(sstep, proc)$ - объём данных, отправленных и полученных процессором $proc$ на супершаге $sstep$.
    \end{itemize}

    Для компьютера BSP в целом на одном супершаге $sstep$:

    \begin{itemize}
        \item $comp(sstep)=\max_{0\leq proc < p}comp(sstep, proc)$; 
        \item $comm(sstep)=\max_{0\leq proc <p}comm(sstep, proc)$; 
        \item $cost(sstep)=comp(sstep)+comm(sstep)\cdot g+l$.
    \end{itemize}

    Для вычисления BSP, состоящего из sync супершагов:

    \begin{itemize}
        \item $comp-\sum_{0\leq sstep <sync}comp(sstep)$; 
        \item $comm=\sum_{0\leq sstep < sync}comm(sstep)$; 
        \item $cost = comp+comm\cdot g+sync\cdot l$. 
    \end{itemize}

    Входные или выходные данные хранятся во внешней памяти, стоимость ввода / вывода включена в $comm$.
\end{remark}

\begin{remark}
    Что касается разработки алгоритмов для BSP. Нас интересует минимизация $comp$, $comm$, $sync$ как функций от $n$, $p$. \\
    
    Соглашения: 

    \begin{itemize}
        \item размер задачи $n \gg p$ (\textit{допуск}); 
        \item входные или выходные во внейшней памяти, ввод или вывод - односторонние коммуникации.
    \end{itemize}

    Баланс вычислений:

    \begin{itemize}
        \item \textit{work-optimal} $comp=O(\frac{seq \: work}{p})$.
    \end{itemize}

    Баланс коммуникации:

    \begin{itemize}
        \item цель \textit{scalable} $comm = O(\frac{iinput+output}{p^c})$, $o<c\leq 1$; 
        \item в идеале $fully-scalable$, $c=1$.
    \end{itemize}

    Крупоблочность:

    \begin{itemize}
        \item цель - $sync$, не зависящая от $n$ (может зависеть от $p$); 
        \item ещё лучше - \textit{quasi-flat} $sync=O((\log p)^{O(1)})$; 
        \item в идеале - \textit{flat} $sync=O(1)$. 
    \end{itemize}
\end{remark}

\subsection{Всякая хуйня.}

\begin{alg}
    Схема на основе \textit{сбалансированного двоичного дерева} - tree(n), 1 вход, $n$ выходов (или наоборот). Размер - $n-1$, глубина - $\log n$. Каждый узел вычисляет произвольно заданную операцию за $O(1)$. Последовательная сложность - $O(n)$, сложность на PRAM - $O(\log n)$. Строим равномерно распределяя одинаковые количества операций на процессоры ($n/p$). \ 
    
    Описанный на лекциях BSP-алгоритм вычисления сбалансированнного дерева полностью оптимален:

    \begin{itemize}
        \item оптимальное $comp=O(n/p)=O(\frac{sequential \: work}{p})$; 
        \item оптимальное $comm = O(n/p)=O(\frac{input/output \: size}{p})$; 
        \item оптимальное $sync=O(1)$. 
    \end{itemize}
\end{alg}

\begin{prob}
    Задача \textit{префиксного накомпления}. Пусть нам дан массив $a=[a_0, \ldots, a_{n-1}]$. Нам нужно вычислить $b_{-1}=0$, а точнее $b_i=a_i+b_{i-1}$ для $0\leq i < n$. В более общем виде нуль заменяется на какой-то элемент, а сложение - на любой ассоциативный оператор.
\end{prob}

\begin{alg}
    Схема \textit{префиксного накопления} - prefix($n$) - красивая вещь, которая позволяет справиться с поставленной задачей за размер $2n-2$, глубину $2 \log n$ и сложность на PRAM - $O(\log n)$. \ 

    Рассматривая префиксное накопление на BSP, граф prefix($n$) состоит Из

    \begin{itemize}
        \item верхнего подерева, вычисляемого от листьев к корню tree($n$); 
        \item переноса значений от узлов верхнего поддерева к узлам нижнего; 
        \item нижнего поддерева, вычисляемого от корня к листьям tree($n$). 
    \end{itemize}

    Оба поддерева можно вычислить предыдущим алгоритмом. Перенос значений задаёт $comm=O(n/p)$, $comp=O(n/p)$ и $sync=O(1)$, предполагая допуск $n\geq p^2$. 
\end{alg}

\begin{prob}
    Даны массивы $a=[a_0, \ldots, a_{n-1}]$, $b=[b_0, \ldots, b_{n-1}]$. Нам нудно вычислить $c_{-1}=0$, а ещё лучше $c_i=a_i+b_i\cdot c_{i-1}$ для $i\leq 0 < n$. Ещё это можно записать в виде произведений матриц, но это так, чуть более красивая форма. Пусть $A_i$ - матрица $2 \times 2$, в верхней строке которой 1, 0, а во второй - $a_i$, $b_i$. $C_i$ - матрица $2 \times 1$, сверху - 1, снизу - $c_i$. Тогда суть рассматриваются рекурренты $C_k=A_k\ldots A_1A_0\cdot C_{-1}$. 
\end{prob}

\begin{alg}
    Эта задача - суть предыдущая с префиксными накоплениями. С итоговым размером $O(n)$, глубиной $O(\log n)$. 
\end{alg}

\begin{prob}
    Побитово складываем массивы. И переносим, а потом ещё и параллелим.
\end{prob}

\end{document}